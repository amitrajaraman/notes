\section{Introduction to Logic}

\subsection{Lecture 1}

For any computer scientist, logic is an extremely basic tool. Consider the statement
\begin{quote}
	This sentence is false.
\end{quote}
A little bit of thought shows that the above sentence has no definite truth value -- it is \textit{paradoxical}. Indeed, it is often known as the ``liar's paradox". This sort of self-referential sentence will come back to haunt us many more times in the future.\\

Propositional logic (or \textit{zeroth-order logic}) is basically the form of logic that deals with propositions which can be true or false as well as relations between them.\\
A more useful tool is that of \textit{first-order logic}, that also deals with non-logical objects, predicates about them, and quantifiers ($\forall$ and $\exists$). That is, we are allowed to quantify over elements of the set, but not something like subsets of the set. A lot of mathematical statements cannot be written when we are restricted to first-order logic.\\

We study theories with basic assumptions or \textit{axioms}. Using these axioms, we aim to prove more non-trivial results within the theory. A natural question to ask is: is it possible to have some set of axioms that allow us to concretely determine the truth value of any consequential statement? Once more, the self-referential statement returns.

\begin{theorem}[G\"odel's Incompleteness Theorem]
There are theories whose assumptions cannot be listed.
\end{theorem}
\begin{proof}
Suppose that there exists such a list for the number theory. Consider the \textbf{true} statement
\begin{quote}
	This sentence cannot be proven by the list.
\end{quote}
The list cannot imply the sentence. This yields a contradiction.
\end{proof}

This theorem shows that logic has all but failed as a tool to do math. From this failure, rose computer science. We discuss G\"odel's Incompleteness more concretely later.

\subsection{Lecture 2}

\subsubsection{Propositional Logic: Syntax and Parsing}

We need an efficient method to identify if some group of symbols is a logical argument. We usually define a syntax for this (similar to grammar in English).\\

The logic we consider is over some list of propositions. We give each proposition a symbol. So say there is some set $\mathsf{Vars}$ of \textit{countably many} propositional variables.
% Say
% \[ \mathsf{Vars} = \{p_1,p_2,\ldots\}. \]
These propositional variables are also called \textit{Boolean variables}.\\
Propositions are connected by \textit{logical arguments}. How can we connect propositions?
\begin{itemize}
	\item A statement that is always true/false.
	\item Negation. A statement that is the negation of another.
	\item Conjunction. Two statements being true simultaneously.
	\item Disjunction. At least one of two statements being true.
	\item Implication. If a statement is true, then some other statement is true as well.
	\item Equivalence. Two statements always have the same truth value.
	\item Disequality or exclusive or. Two statements always have different truth values.
\end{itemize}

\begin{center}
\begin{tabular}{c|c|c}
	 true & $\top$ & top  \\
	 false & $\perp$ & bot \\
	 negation & $\neg$ & not \\
	 conjunction & $\wedge$ & and \\
	 disjunction & $\vee$ & or \\
	 implication & $\implies$ & implies \\
	 equivalence & $\iff$ & iff \\
	 exclusive or & $\oplus$ & xor \\
	 opening parenthesis & ( & \\
	 closing parenthesis & ) &
\end{tabular}
\end{center}

We assume that the above \textit{logical connectives} are not in $\mathsf{Vars}$.\\
A \textit{propositional formula} is a finite string containing symbols in $\mathsf{Vars}$ and logical connectives.

\begin{definition}
The set of propositional formulas is the smallest set $P$ such that
\begin{itemize}
	\item $\top,\perp$ are in $P$,
	\item $\mathsf{Vars}\subseteq P$,
	\item if $F\in P$, then $\neg F\in P$, and
	\item if $\circ$ is a binary symbol and $F,G\in P$, then $(F\circ G)\in P$.
\end{itemize}
\end{definition}

Alternatively, this can succinctly be written as ``$F \in P$ if
\[F \coloneqq p\mid\top\mid\perp\mid \neg F \mid (F\vee F) \mid (F\wedge F) \mid (F\implies F) \mid (F\iff F) \mid (F\oplus F) \]
where $p\in\mathsf{Vars}$."

\begin{definition}
$\top$, $\perp$, and any $p\in\mathsf{Vars}$ are known as \textit{atomic formulas}.
\end{definition}

\begin{definition}
For each $F\in P$, $\mathsf{Vars}(F)$ is the set of variables appearing in $F$.
\end{definition}

It is important to note that parentheses are needed (only) between binary operations. So as of now, $(\perp\imp\top)$ is a formula but $\perp\imp\top$ isn't.\\
Not all strings over $\mathsf{Vars}$ and logical connectives are in $P$.

\subsubsection{Examples Encoding Arguments into Logic}

Consider the following argument.
\begin{quote}
	If $c$ then if $s$ then $f$. not $f$. Therefore, if $s$ then not $c$.
\end{quote}
This can be written as
\[ (((c\imp (s\imp f)) \wedge \neg f) \imp (s \imp \neg c)). \]
Another example, say we know that good people always tell the truth and not good people always tell a lie. If there are two people $A$ and $B$ and $A$ says ``I am not good or $B$ is good", then what are $A$ and $B$?\\
Suppose the variables $p_A$ and $p_B$ denote whether $A$ and $B$ are truthful or not. Then the above is basically
\[ ((\neg p_A \vee p_B) \iff p_A). \]
How do we determine whether there are some $p_A,p_B$ that satisfies this?

\subsubsection{Parsing Formulas}

$F\in P$ iff it can be obtained by unfolding one of these generation rules.

\begin{definition}
A \textit{parse tree} of a formula $F\in P$ is a tree such that
\begin{itemize}
	\item the root is $F$,
	\item the leaves are atomic formulas, and
	\item each internal node is formed by applying some formulation rule on its children.
\end{itemize}
\end{definition}

We have the following 

\begin{theorem}
$F\in P$ iff there is a parse tree of $F$. Further, if $F\in P$, it has a \textit{unique} parse tree.
\end{theorem}
The reverse direction follows by definition.\\

A parse tree is a directed acyclic graph (DAG). The parsing produces a parse DAG. This is done by not writing repeated symbols twice, ensuring that all arrows go from higher levels of the DAG to the lower ones.

\begin{definition}
A formula $G$ is a \textit{subformula} of a formula $F$ if $G$ occurs within $F$. Further, $G$ is a proper subformula of $F$ if $F\neq G$. Denote by $\mathsf{sub}(F)$ the set of subformulas of $F$.
\end{definition}

Observe that the nodes of the parse tree of $F$ form $\mathsf{sub}(F)$.

Immediate subformulas are the children of a formula in its parse tree. The corresponding \textit{leading connective} is the connective that joins it to the children. So for example,

\[ \mathsf{sub}((\neg p_2 \iff (p_1 \wedge p_3)) = \{((\neg p_2 \iff (p_1 \wedge p_3)), \neg p_2, (p_1 \wedge p_3), p_1, p_2, p_3\}. \]

\subsubsection{Shorthands}

The reader might have noticed by now that we need to write far more parentheses than required which don't really feel necessary most of the time. If we use some sort of precedence order over logical connectives, we may be able to drop some parentheses without losing the unique parsing property.\\
For example, we may drop outermost parentheses without any confusion. An example of this is writing $((p \wedge q)\imp (r \vee p))$ as $(p \wedge q)\imp (r \vee p)$.\\
Further, in the above example, if we give $\vee$ and $\wedge$ higher precedence then $\imp$ during parentheses, then we can drop all the parentheses! The usual precedence order we use is
\[ \neg > \vee = \wedge = \oplus > \imp = \iff. \]
So how do we go about parsing a formula then? Suppose we have $F_0\circ_1 F_2\circ_2 \cdots\circ_n F_n$, where each $F_i$ is either atomic, enclosed by parentheses, or their negation. We transform it as follows.
\begin{itemize}
	\item Find a $\circ_i$ such that $\circ_{i-1}$ and $\circ_{i+1}$ have lower precedence (if they exist).
	\item Introduce parentheses around $F_{i-1}\circ F_i$ and call it $F_i'\coloneqq (F_{i-1}\circ_i F_i)$ so we now have
	\[ F_0 \circ_1 \cdots \circ_{i-2} F_{i-2} \circ_{i-1} F_i' \circ_{i+1} F_{i+1} \circ_{i+2} \cdots \circ_n F_n. \]
\end{itemize}
Repeat the above until only one term remains ($n$ becomes $1$). We can then parse it normally. For example,
\[ p \wedge q \imp r \vee p \text{ to } (p\wedge q)\imp r\vee p \text{ to } (p\wedge q)\imp (r\vee p) \text{ to } ((p\wedge q)\imp (r\vee p)). \]
Some formulas cannot be unambiguously parsed, for example $p\vee q\wedge r$, $p \vee q \vee r$, or $p\imp q\imp r$. But can we salvage any of them?\\
Associativity preference may further reduce the need of parentheses. Let's make all our operators right associative (first group the rightmost occurrence). So for example, unless mentioned otherwise, we take $p\imp q\imp r$ as $(p\imp (q\imp r))$.

\begin{definition}
For $F\in P$ and $p_1,\ldots,p_k\in\Vars$, we denote by $F[G_1/p_1,\ldots,G_k/p_k]$ the formula obtained by \textit{simultaneously} replacing all occurrences of $p_i$ by the formula $G_i$ for each $i\in[k]$. 
\end{definition}

So for example,
\[ (p\imp (r\imp p))[(r\otimes p)/p] = ((r\otimes p)\imp (r\imp (r\otimes p))). \]
Sometimes, we may also write a formula $F$ as $F(p_1,\ldots,p_k)$. Then, by $F(G_1,\ldots,G_n)$, we mean $F[G_1/p_1,\ldots,G_k/p_k]$.

\subsection{Lecture 3}

\subsubsection{Semantics}

Semantics is giving meaning to formulas. We denote the set of truth values as $\mathcal{B}\coloneqq\{0,1\}$. We may view $0$ as ``false" and $1$ as ``true", but the only important thing is that they are distinct.

\begin{definition}[Model]
A \textit{model} is an function from $\Vars\to\mathcal{B}$.
\end{definition}

For example, $\{p_1\mapsto 1, p_2\mapsto 0, p_3\mapsto 0,\ldots\}$ is a model. \\
It is quite natural to extend this further to formulas in general. That is, a given model $m$ may or may not satisfy a formula $F$. More concretely.

\begin{definition}
The \textit{satisfaction relation} $\vDash$ between models and formulas is the smallest relation that satisfies the following.
\begin{itemize}
	\item $m\vDash\top$,
	\item if $m(p)=1$, then $m\vDash p$,
	\item if $m\nvDash F$, then $m\vDash\neg F$,
	\item if $m\vDash F_1$ or $m\vDash F_2$, then $m\vDash (F_1\vee F_2)$,
	\item if $m\vDash F_1$ and $m\vDash F_2$, then $m\vDash (F_1\wedge F_2)$,
	\item if $m\vDash F_1$ and $m\vDash F_2$ but not both, then $m\vDash (F_1\oplus F_2)$,
	\item if if $m\vDash F_1$ then $m\vDash F_2$, then $m\vDash (F_1\imp F_2$, and
	\item if $m\vDash F_1$ iff $m\vDash F_2$, then $m\vDash (F_1\Leftrightarrow F_2)$.
\end{itemize}
\end{definition}

Observe that $\perp$ is not explicitly mentioned in the above definition since it follows from it being the \textit{smallest} relation.\\

If $m\vDash F$, we say that $m$ \textit{satisfies} $F$.\\
$F$ is \textit{satisfiable} if there is a model $m$ such that $m\vDash F$. This is often abbreviated as \textit{sat}.\\
$F$ is \textit{valid} (written $\vDash F$) if for each model $m$, $m\vDash F$. A valid formula is also called a \textit{tautology}.\\
$F$ is \textit{unsatisfiable} (written $\nvDash F$) if there is no model $m$ such that $m\vDash F$. This is often abbreviated as \textit{unsat}.\\

We can check if a certain formula satisfies a model by moving bottom-up in the parse tree.\\

We overload the $\vDash$ operator in several natural ways.

\begin{definition}
Let $M$ be a set of models. We write $M\vDash F$ if for every $m\in M$, $m\vDash F$.
\end{definition}

\begin{definition}
Let $\Sigma$ be a set of formulas. We write $\Sigma\vDash F$ if for every $m$ that satisfies every formula in $\Sigma$, $m\vDash F$.
\end{definition}

This is read ``$\Sigma$ implies $F$''. If $\Sigma=\{G\}$, we write $G\vDash F$.

\begin{definition}
We write $F\equiv G$ if for each model $m$,
\[ m\vDash F \iff m\vDash G. \]
\end{definition}

\begin{definition}
Formulas $F$ and $G$ are \textit{equisatisfiable} if
\[ F \text{ is sat} \iff G \text{ is sat.}\]
\end{definition}

\begin{definition}
Formulas $F$ and $G$ are \textit{equivalid} if $\vDash F\iff\vDash G$.
\end{definition}

\subsubsection{Decidability of SAT}

\begin{fdef}
A problem is \textit{decidable} if there is an algorithm to solve the problem.
\end{fdef}

This is required since G\"odel's Incompleteness implies the existence of undecidable problems.

The problem we consider here, known as the \textit{propositional satisfiability problem} is:
\begin{quote}
For a given $F\in P$, is $F$ satisfiable?
\end{quote}

\begin{theorem}
The propositional satisfiability problem is decidable.
\end{theorem}
\begin{proof}
We enumerate the $2^{|\Vars(F)|}$ elements of $\Vars(F)\to\mathcal{B}$. If any of the models satisfy the formula, $F$ is sat. Otherwise, it is unsat.
\end{proof}

The cost is obviously exponential and we would want to do better. Indeed, there are several tricks that make satisfiability checking more feasible for real-world formulas.

\subsubsection{Truth Tables}

We wish to assign a truth value to every formula $F$.

Given a model $m:\Vars\to\B$, we can naturally extend it to $m:P\to \B$ as
\[
	m(F) = 
	\begin{cases}
	1, & m\vDash F, \\
	0, & \text{otherwise.}
	\end{cases}
\]

This extended $m$ is known as the \textit{truth function}. We do not introduce new notation for this, keeping the meanings from the definitions for models unchanged.\\

For a formula $F$, a truth table consists of $2^{|\Vars(F)|}$ rows, where each row considers one of the models and computes the corresponding truth value of $F$.

Truth tables are sometimes useful to prove that formulae are equivalent. For example, show that $p\vee q \equiv \neg(\neg p \wedge \neg q)$ and $p\wedge q \equiv \neg(\neg p \vee \neg q)$, also known as De Morgan's laws.\\
It is also easily shown that $p\imp q \equiv (\neg p \vee q)$.

Truth tables are tedious because we need to write $2^n$ rows even if a simple observation could easily show (un)satisfiability.\\
For example, $a \vee (c \wedge b)$ being sat is very clearly true. If there are no $\neg$s (of any form, $\oplus$ in particular) in general, one can just set everything as true. Another example is that $(a \vee (c \neg a))\wedge\neg (a \vee (c \neg a))$ is obviously unsat.\\
How do we take such shortcuts?

\subsubsection{Expressive power of propositional logic}

A finite boolean function is one from $\B^n\to\B$.\\
A formula $F$ with $\Vars(F)=\{p_1,\ldots,p_n\}$ can be viewed as a boolean function $f$ such that for each model $m$, $m(F)=f(m(p_1),\ldots,m(p_n))$. This is just an alternate way of writing a truth table (as a function instead of a table).

\begin{theorem}
For each finite boolean function $f$, there is a formula $F$ that represents $f$.
\end{theorem}
\begin{proof}
Let $f:\B^n\to\B$. Let $p_i^0\coloneqq\neg p_i$ and $p_i^1\coloneqq p_i$. For every $(b_1,\ldots,b_n)\in\B^n$, let
\[
	F_{(b_1,\ldots,b_n)} \coloneqq
	\begin{cases}
		(p_1^{b_1}\wedge\cdots\wedge p_n^{b_n}), & f(b_1,\ldots,b_n)=1 \\
		\perp, & \text{otherwise.}
	\end{cases}
\]
We can then define the required formula $F$ by taking the conjunction over all boolean combinations,
\[ F \coloneqq F_{(0,\ldots,0)} \vee \cdots \vee F_{(1,\ldots,1)}. \]
\end{proof}

Observe that we have only used three logical connectives.

What if we do not have all logical connectives? Then we may not be able to represent all boolean functions. This is known as ``insufficient expressive power".\\

For example, $\wedge$ alone cannot express all boolean functions. Consider the function $f=\{0\mapsto 1, 1\mapsto 1\}$. We show that this cannot be achieved by any $\wedge$s by taking induction on the size of formulas containing the variable $p$ and $\wedge$. For the base case, our only choice of formula is $p$. Now, suppose that formulas $F$ and $G$ of size less than $n-1$ do not represent $f$. We can construct a longer formula by $(F\wedge G)$. This formula does not represent $f$ because we can always pick a model where $F$ or $G$ produce $0$.

We originally used $8$ connectives. This is not the minimal set required for maximum expressivity, however. For example, $\neg$ and $\vee$ can define the whole propositional logic. Indeed,
\begin{itemize}
	\item $\top \equiv p \vee \neg p$,
	\item $\perp \equiv \neg \top$,
	\item $(p \wedge q) \equiv \neg(p \vee q)$,
	\item $(p\otimes q) \equiv (p \wedge \neg q) \vee (\neg p \wedge q)$,
	\item $(p\imp q) \equiv (\neg p \vee q)$, and
	\item $(p\Leftrightarrow q) \equiv (p\imp q) \wedge (q\imp p)$.
\end{itemize}

\subsection{Lecture 4}

\subsubsection{Formal Proofs}

Suppose that for a set of formulas $\Sigma$ and a formula $F$, $\Sigma\vDash F$. Can we infer that $\Sigma\vDash F$ without writing out the truth tables? This syntactic inference is called \textit{derivation}. This is written $\Sigma\vdash F$ and is read ``$\Sigma$ proves $F$''. In this case, $F$ is said to be a ``consequence'' of $\Sigma$.\\

If $F$ occurs on the left hand side $F\in\Sigma$, then $F$ is clearly a consequence.\\

A \textit{proof rule} provides us a means to derive new statements from old statements. They are written as
\begin{prooftree}
	\LeftLabel{\rul{RuleName}}
	\AxiomC{Stuff already there}
	\RightLabel{\rul{Conditions to be met}}
	\UnaryInfC{Stuff to be added}
\end{prooftree}
A derivation proceeds by applying these proof rules.
So for instance, the rule we mentioned earlier can be written as
\begin{prooftree}
	\LeftLabel{\rul{Assumption}}
	\AxiomC{}
	\RightLabel{$F\in\Sigma.$}
	\UnaryInfC{$\Sigma\vdash F$}
\end{prooftree}
Another obvious example is
\begin{prooftree}
	\LeftLabel{\rul{Monotonic}}
	\AxiomC{$\Sigma\vdash F$}
	\RightLabel{$\Sigma\subseteq\Sigma'.$}
	\UnaryInfC{$\Sigma'\vdash F$}
\end{prooftree}

\begin{definition}
A \textit{derivation} is a list of statements that are derived from earlier statements.
\end{definition}

An example of a derivation using the above rules is:
\begin{enumerate}
	\item $\{p\vee q,\neg\neg q\}\vdash\neg\neg q$ \hfill (\rul{Assumption})
	\item $\{p\vee q, \neg\neg q, r\}\vdash\neg\neg q$ \hfill (\rul{Monotonic} applied to 1)
\end{enumerate}
It is important to note that we need to explicitly point out which earlier step we are using (if any).

Let us try to establish some proof rules on our logical connectives.

\paragraph{Negation.}

\begin{prooftree}
	\LeftLabel{\rul{DoubleNeg}}
	\AxiomC{$\Sigma\vdash F$}
	% \RightLabel{$\Sigma\subseteq\Sigma'.$}
	\UnaryInfC{$\Sigma\vdash\neg\neg F$}
\end{prooftree}

For example,
\begin{enumerate}
	\item $\{p\vee q,r\}\vdash r$ \hfill (\rul{Assumption})
	\item $\{p\vee q, r, \neg\neg q\}\vdash r$ \hfill (\rul{Monotonic} applied to 1)
	\item $\{p\vee q, r, \neg\neg q\}\vdash\neg\neg r$ \hfill (\rul{DoubleNeg} applied to 2)
\end{enumerate}

\paragraph{Conjunction.}

We have the following proof rules for the conjunction.

\begin{prooftree}
	\LeftLabel{$\wedge$-\rul{Intro}}
	\AxiomC{$\Sigma\vdash F$}
	\AxiomC{$\Sigma\vdash G$}
	% \RightLabel{$\Sigma\subseteq\Sigma'.$}
	\BinaryInfC{$\Sigma\vdash F\wedge G$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\wedge$-\rul{Elim}}
	\AxiomC{$\Sigma\vdash F\wedge G$}
	% \RightLabel{$\Sigma\subseteq\Sigma'.$}
	\UnaryInfC{$\Sigma\vdash F$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\wedge$-\rul{Symm}}
	\AxiomC{$\Sigma\vdash F\wedge G$}
	\UnaryInfC{$\Sigma\vdash G\wedge F$}
\end{prooftree}

An example using these is
\begin{enumerate}
	\item $\{p\wedge q, \neg\neg q, r\}\vdash p\wedge q$ \hfill (\rul{Assumption})
	\item $\{p\wedge q, \neg\neg q, r\}\vdash p$ \hfill (\rul{$\wedge$-Elim} applied to 1)
	\item $\{p\wedge q, \neg\neg q, r\}\vdash q\wedge p$ \hfill (\rul{$\wedge$-Symm} applied to 1)
\end{enumerate}

\paragraph{Disjunction.}

Except for the last two (which are like De Morgan's law), the rules for the disjunction are similar.

\begin{prooftree}
	\LeftLabel{$\vee$-\rul{Intro}}
	\AxiomC{$\Sigma\vdash F$}
	\UnaryInfC{$\Sigma\vdash F\vee G$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\vee$-\rul{Elim}}
	\AxiomC{$\Sigma\vdash F\vee G$}
	\AxiomC{$\Sigma\cup\{F\}\vdash H$}
	\AxiomC{$\Sigma\cup\{G\}\vdash H$}
	\TrinaryInfC{$\Sigma\vdash H$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\vee$-\rul{Symm}}
	\AxiomC{$\Sigma\vdash F\vee G$}
	\UnaryInfC{$\Sigma\vdash G\vee F$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\vee$-\rul{Def}}
	\AxiomC{$\Sigma\vdash F\vee G$}
	\UnaryInfC{$\Sigma\vdash \neg(\neg F \wedge \neg G)$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\vee$-\rul{Def}}
	\AxiomC{$\Sigma\vdash \neg(\neg F \wedge \neg G)$}
	\UnaryInfC{$\Sigma\vdash F\vee G$}
\end{prooftree}

Let us give another example, which is basically equivalent to the distributive law. Suppose we have $\Sigma\vdash (F\wedge G)\vee(F\wedge H)$, we want to show that we can derive $\Sigma\vdash F\wedge(G\vee H)$. Indeed,
\begin{enumerate}
	\item $\Sigma\vdash (F\wedge G)\vee(F\wedge H)$ \hfill (\rul{Premise})%1
	
	\item $\Sigma\cup\{F\wedge G\} \vdash F\wedge G$ \hfill (\rul{Assumption})%2
	\item $\Sigma\cup\{F\wedge G\} \vdash F$ \hfill (\rul{$\wedge$-Elim} applied to 2)%3
	\item $\Sigma\cup\{F\wedge G\} \vdash G\wedge F$ \hfill (\rul{$\wedge$-Symm} applied to 2)%4
	\item $\Sigma\cup\{F\wedge G\} \vdash G$ \hfill (\rul{$\wedge$-Elim} applied to 4)%5
	\item $\Sigma\cup\{F\wedge G\}\vdash G\vee H$ \hfill (\rul{$\vee$-Intro} applied to 5)%6
	\item $\Sigma\cup\{F\wedge G\}\vdash F\wedge(G\vee H)$ \hfill (\rul{$\wedge$-Intro} applied to 3,6)%7

	\item $\Sigma\cup\{F\wedge H\} \vdash F\wedge H$ \hfill (\rul{Assumption})%8
	\item $\Sigma\cup\{F\wedge H\} \vdash F$ \hfill (\rul{$\wedge$-Elim} applied to 8)%9
	\item $\Sigma\cup\{F\wedge H\} \vdash H\wedge F$ \hfill (\rul{$\wedge$-Symm} applied to 8)%10
	\item $\Sigma\cup\{F\wedge H\} \vdash H$ \hfill (\rul{$\wedge$-Elim} applied to 10)%11
	\item $\Sigma\cup\{F\wedge H\}\vdash H\vee G$ \hfill (\rul{$\vee$-Intro} applied to 11)%12
	\item $\Sigma\cup\{F\wedge H\}\vdash G\vee H$ \hfill (\rul{$\vee$-Symm} applied to 11)%13
	\item $\Sigma\cup\{F\wedge G\}\vdash F\wedge(G\vee H)$ \hfill (\rul{$\wedge$-Intro} applied to 9,13)%14

	\item $\Sigma\vdash F\wedge(G\vee H)$ \hfill (\rul{$\vee$-Elim} applied to 1,7,14)%15
\end{enumerate}

\paragraph{Implication.}

The rules for the implication are:

\begin{prooftree}
	\LeftLabel{$\imp$-\rul{Intro}}
	\AxiomC{$\Sigma\cup \{F\}\vdash G$}
	\UnaryInfC{$\Sigma\vdash F \imp G$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\imp$-\rul{Elim}}
	\AxiomC{$\Sigma\vdash F\imp G$}
	\AxiomC{$\Sigma\vdash F$}
	\BinaryInfC{$\Sigma\vdash G$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\imp$-\rul{Def}}
	\AxiomC{$\Sigma\vdash F\imp G$}
	\UnaryInfC{$\Sigma\vdash \neg F\vee G$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\imp$-\rul{Def}}
	\AxiomC{$\Sigma\vdash \neg F\vee G$}
	\UnaryInfC{$\Sigma\vdash F\imp G$}
\end{prooftree}

For example, let us show that $\{\neg p\vee q,p\}\vdash q$.
\begin{enumerate}
	\item $\{\neg p\vee q, p\}\vdash p$ \hfill (\rul{Assumption})
	\item $\{\neg p\vee q, p\}\vdash \neg p\vee q$ \hfill (\rul{Assumption})
	\item $\{\neg p\vee q, p\}\vdash p\imp q$ \hfill (\rul{$\imp$-Def} applied to 2)
	\item $\{\neg p\vee q, p\}\vdash q$ \hfill (\rul{$\imp$-Elim} applied to 1,3)
\end{enumerate}

As another example, let us show that $\emptyset\vdash(p\imp q)\vee p$.
\begin{enumerate}
	\item $\{\neg p\}\vdash\neg p$ \hfill (\rul{Assumption})
	\item $\{\neg p\}\vdash \neg p \vee q$ \hfill (\rul{$\vee$-Intro} applied to 1)
	\item $\{\neg p\}\vdash p \imp q$ \hfill (\rul{$\imp$-Def} applied to 2)
	\item $\{\neg p\}\vdash (p \imp q)\vee p$ \hfill (\rul{$\vee$-Intro} applied to 3)

	\item $\{p\}\vdash p$ \hfill (\rul{Assumption})
	\item $\{p\}\vdash p\vee(p\imp q)$ \hfill (\rul{$\vee$-Intro} applied to 5)
	\item $\{p\}\vdash (p\imp q)\vee p$ \hfill (\rul{$\vee$-Symm} applied to 6)

	\item $\emptyset\vdash (p\imp p)$ \hfill (\rul{$\imp$-Intro} applied to 5)
	\item $\emptyset\vdash (\neg p\vee p)$ \hfill (\rul{$\imp$-Def} applied to 8)
	
	\item $\emptyset\vdash (p\imp q)\vee p$ \hfill (\rul{$\vee$-Elim} applied to 4,7,9)
	% \item $\emptyset\vdash (\neg p)\imp (p\imp q)$ \hfill (\rul{$\imp$-Intro} applied to 3)
	% \item $\emptyset\vdash (\neg \neg p) \vee (p\imp q)$ \hfill (\rul{$\imp$-Def} applied to 4)
\end{enumerate}

There are several more proof rules for parentheses, $\oplus$, $\iff$, et cetera.

\subsubsection{Soundness}

How do we know that the above proof rules are correct? What does ``correct'' even mean?

\begin{theorem}
\label{theo: derivation implies modeling}
	If proof rules derive a statement $\Sigma\vdash F$, then $\Sigma\vDash F$.
\end{theorem}
We use an inductive argument. Assume that the theorem holds for the premises of the rules. We shall show that it is also true for the conclusions.\\

The above basically unifies the \textit{syntactic} and \textit{semantic} methods of proof. We shall later see that the converse is true holds as well.

Consider the following rule
\begin{prooftree}
	\LeftLabel{$\wedge$-\rul{Elim}}
	\AxiomC{$\Sigma\vdash F\wedge G$}
	\UnaryInfC{$\Sigma\vdash F$}
\end{prooftree}
Consider a model $m\vDash\Sigma$. By the induction hypothesis, $m\vDash F\wedge G$. It is easy to show (using the truth table) that if $m\vDash F\wedge G$, then $m\vDash F$. Therefore, $\Sigma\vDash F$.\\

As another example, consider
\begin{prooftree}
	\LeftLabel{$\imp$-\rul{Intro}}
	\AxiomC{$\Sigma\cup\{F\}\vdash G$}
	\UnaryInfC{$\Sigma\vdash F\imp G$}
\end{prooftree}
Consider a model $m\vDash\Sigma$. There are two possibilities.
\begin{itemize}
	\item $m\vDash F$. Then $m\vDash \Sigma\cup\{F\}$. By the hypothesis, $m\vDash G$. Therefore, $m\vDash F\imp G$.
	\item $m\nvDash F$. Then $m$ trivially satisfies $m\vDash F\imp G$.
\end{itemize}

\subsection{Lecture 5}

\subsubsection{Derived Rules}

In this section and the next, we give some rules derived from those already given that are quite useful when trying to prove statements.

\paragraph{Modus ponens.} %If we have $\Sigma\vdash\neg F\vee G$ and $\Sigma\vdash F$, then we can derive $\Sigma\vdash G$. That is,
\begin{prooftree}
	\LeftLabel{$\vee$-\rul{ModusPonens}}
	\AxiomC{$\Sigma\vdash \neg F\vee G$}
	\AxiomC{$\Sigma\vdash F$}
	\BinaryInfC{$\Sigma\vdash G$}
\end{prooftree}
The proof is as follows.
\begin{enumerate}
	\item $\Sigma\vdash\neg F\vee G$ \hfill (\rul{Premise})
	\item $\Sigma\vdash (F\imp G)$ \hfill (\rul{$\imp$-Def} applied to 1)
	\item $\Sigma\vdash F$ \hfill (\rul{Premise})
	\item $\Sigma\vdash G$ \hfill (\rul{$\imp$-Elim} applied to 2,3) 
\end{enumerate}

\paragraph{Tautology.} %For any $F$ and set $\Sigma$ of formulas, we can derive $\Sigma\vdash \neg F\vee F$. That is,
\begin{prooftree}
	\LeftLabel{\rul{Tautology}}
	\AxiomC{}
	\UnaryInfC{$\Sigma\vdash \neg F \vee F$}
\end{prooftree}
This can be derived as
\begin{enumerate}
	\item $\Sigma\cup\{F\}\vdash F$ \hfill (\rul{Assumption})
	\item $\Sigma\vdash (F\imp G)$ \hfill (\rul{$\imp$-Intro} applied to 1)
	\item $\Sigma\vdash \neg F\vee F$ \hfill (\rul{$\imp$-Def} applied to 2)
\end{enumerate}

\paragraph{Contradiction.} %If we have $\Sigma\vdash F\land\neg F$, then $\Sigma\vdash G$. That is,
\begin{prooftree}
	\LeftLabel{\rul{Contra}}
	\AxiomC{$\Sigma\vdash F\land\neg F$}
	\UnaryInfC{$\Sigma\vdash G$}
\end{prooftree}
This is proved as
\begin{enumerate}
	\item $\Sigma\vdash (F\land\neg F)$ \hfill (\rul{Premise})
	\item $\Sigma\vdash (\neg F\land G)$ \hfill (\rul{$\land$-Symm} applied to 1)
	\item $\Sigma\vdash \neg F$ \hfill (\rul{$\land$-Elim} applied to 2)
	\item $\Sigma\vdash \neg F\lor G$ \hfill (\rul{$\lor$-Intro} applied to 3)
	\item $\Sigma\vdash F$ \hfill (\rul{$\land$-Elim} applied to 1)
	\item $\Sigma\vdash G$ \hfill (\rul{$\lor$-ModusPonens} applied to 4,5)
\end{enumerate}

\paragraph{Contrapositive.} %If $\Sigma\cup\{F\}\vdash G$, then $\Sigma\cup\{\neg G\}\vdash\neg F$. That is,
\begin{prooftree}
	\LeftLabel{\rul{Contrapositive}}
	\AxiomC{$\Sigma\cup\{F\}\vdash G$}
	\UnaryInfC{$\Sigma\cup\{\neg G\}\vdash \neg F$}
\end{prooftree}
This is proved as
\begin{enumerate}
	\item $\Sigma\cup\{F\}\vdash G$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{F\}\vdash \neg\neg G$ \hfill (\rul{DoubleNeg} applied to 1)
	\item $\Sigma\vdash (F \imp \neg\neg G)$ (\rul{$\imp$-Intro} applied to 2)
	\item $\Sigma\vdash \neg F \lor \neg\neg G$ \hfill (\rul{$\imp$-Def} applied to 3)
	\item $\Sigma\vdash \neg\neg G \lor \neg F$ \hfill (\rul{$\lor$-Symm} applied to 4)
	\item $\Sigma\vdash \neg G\imp \neg F$ \hfill (\rul{$\imp$-Def} applied to 5)
	\item $\Sigma\cup\{\neg G\}\vdash \neg G\imp\neg F$ \hfill (\rul{Monotonic} applied to 6)
	\item $\Sigma\cup\{\neg G\}\vdash \neg G$ \hfill (\rul{Assumption})
	\item $\Sigma\cup\{\neg G\}\vdash \neg F$ \hfill (\rul{$\imp$-Elim} 7,8)
\end{enumerate}

\subsubsection{More Derived Rules}

\paragraph{Proof by cases.} %If we have $\Sigma\cup\{F\}\vdash G$ and $\Sigma\cup\{\neg F\}\vdash G$, then $\Sigma\vdash G$. That is, 
\begin{prooftree}
	\LeftLabel{\rul{ByCases}}
	\AxiomC{$\Sigma\cup\{F\}\vdash G$}
	\AxiomC{$\Sigma\cup\{\neg F\}\vdash G$}
	\BinaryInfC{$\Sigma\vdash G$}
\end{prooftree}
This can be proved as
\begin{enumerate}
	\item $\Sigma\cup\{F\}\vdash G$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{\neg F\}\vdash G$ \hfill (\rul{Premise})
	\item $\Sigma\vdash F\lor\neg F$ \hfill (\rul{Tautology})
	\item $\Sigma\vdash G$ \hfill (\rul{$\lor$-Elim} applied to 1,2,3)
\end{enumerate}

\paragraph{Proof by contradiction.} %If we have $\Sigma\cup\{F\}\vdash G$ and $\Sigma\cup\{F\}\vdash\neg G$, then $\Sigma\vdash\neg F$. That is,
\begin{prooftree}
	\LeftLabel{\rul{ByContra}}
	\AxiomC{$\Sigma\cup\{F\}\vdash G$}
	\AxiomC{$\Sigma\cup\{F\}\vdash \neg G$}
	\BinaryInfC{$\Sigma\vdash \neg F$}
\end{prooftree}
This is proved as
\begin{enumerate}
	\item $\Sigma\cup\{F\}\vdash G$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{F\}\vdash \neg G$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{\neg G\}\vdash \neg F$ \hfill (\rul{Contrapositive} applied to 1)
	\item $\Sigma\cup\{\neg \neg G\}\vdash \neg F$ \hfill (\rul{Contrapositive} applied to 1)
	\item $\Sigma\vdash \neg F$ \hfill (\rul{ByCases} applied to 3,4)
\end{enumerate}

\paragraph{Reverse Double Negation.} %If we have $\Sigma\vdash\neg\neg F$, then $\Sigma\vdash F$. That is,
\begin{prooftree}
	\LeftLabel{\rul{RevDoubleNeg}}
	\AxiomC{$\Sigma\vdash \neg\neg F$}
	\UnaryInfC{$\Sigma\vdash F$}
\end{prooftree}
This can be proved as
\begin{enumerate}
	\item $\Sigma\vdash \neg\neg F$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{\neg F\}\vdash \neg\neg F$ \hfill (\rul{Monotonic} applied to 1)
	\item $\Sigma\cup\{\neg F\}\vdash \neg F$ \hfill (\rul{Assumption})
	\item $\Sigma\cup\{\neg F\}\vdash\neg F\land\neg\neg F$ \hfill (\rul{$\land$-Intro} applied to 2,3)
	\item $\Sigma\cup\{\neg F\}\vdash\neg F$ \hfill (\rul{$\land$-Elim} applied to 4)
	\item $\Sigma\cup\{\neg F\}\vdash\neg\neg F\land\neg F$ \hfill (\rul{$\land$-Symm} applied to 5)
	\item $\Sigma\cup\{\neg F\}\vdash F$ \hfill (\rul{Contra} applied to 6)
	\item $\Sigma\cup\{F\}\vdash F$ \hfill (\rul{Assumption})
	\item $\Sigma\vdash F$ \hfill (\rul{ByCases} applied to 7,8)
\end{enumerate}

\paragraph{Resolution.} %If $\Sigma\vdash\neg F\lor G$ and $\Sigma\vdash F\lor H$, then we can derive $\Sigma\vdash G\lor H$. That is,
\begin{prooftree}
	\LeftLabel{\rul{Resolution}}
	\AxiomC{$\Sigma\vdash \neg F\lor G$}
	\AxiomC{$\Sigma\vdash F\lor H$}
	\BinaryInfC{$\Sigma\vdash G\lor H$}
\end{prooftree}
This is proved as
\begin{enumerate}
	\item $\Sigma\vdash \neg F\lor G$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{F\}\vdash \neg F\lor G$ \hfill (\rul{Monotonic} applied to 1)
	\item $\Sigma\cup\{F\}\vdash F$ \hfill (\rul{Assumption})
	\item $\Sigma\cup\{F\}\vdash G$ \hfill (\rul{$\lor$-ModusPonens} applied to 2,3)
	\item $\Sigma\cup\{F\}\vdash G\lor H$ \hfill (\rul{$\lor$-Intro} applied to 4)
	\item $\Sigma\vdash F\lor H$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{\neg F\}\vdash F\lor H$ \hfill (\rul{Monotonic} applied to 6)
	\item $\Sigma\cup\{\neg F\}\vdash \neg F$ \hfill (\rul{Assumption})
	\item $\Sigma\cup\{F\}\vdash H$ \hfill (\rul{$\lor$-ModusPonens} applied to 7,8)
	\item $\Sigma\cup\{\neg F\}\vdash H\lor G$ \hfill (\rul{$\lor$-Intro} applied to 9)
	\item $\Sigma\cup\{\neg F\}\vdash G\lor H$ \hfill (\rul{$\lor$-Symm} applied to 10)
	\item $\Sigma\vdash G\lor H$ \hfill (\rul{ByCases} applied to 5,11)
\end{enumerate}

\subsubsection{Substitutions}

 Let $F_1(p)$ and $F_2(p)$ be formulas. If $\Sigma\vdash (F_1(G)\siff F_1(H))$, $\Sigma\vdash(F_2(G)\siff F_2(H))$, and $\Sigma\vdash F_1(G)\land F_2(G)$, then $\Sigma\vdash F_1(H)\land F_2(H).$\\
 The proof of the above is

\begin{enumerate}
	\item $\Sigma\vdash (F_1(G)\siff F_1(H))$ \hfill (\rul{Premise})
	\item $\Sigma\vdash (F_2(G)\siff F_2(H))$ \hfill (\rul{Premise})
	\item $\Sigma\vdash (F_1(G)\land F_2(G))$ \hfill (\rul{Premise})
	\item $\Sigma\vdash F_1(G)$ \hfill (\rul{$\land$-Elim} applied to 3)
	\item $\Sigma\vdash (F_1(G)\imp F_1(H))$ \hfill (\rul{$\siff$-Def} applied to 1)
	\item $\Sigma\vdash F_1(H)$ \hfill (\rul{$\imp$-Elim} applied to 4,5)
	\item $\Sigma\vdash F_2(G)\land F_1(G)$ \hfill (\rul{$\land$-Symm} applied to 3)
	\item $\Sigma\vdash F_2(G)$ \hfill (\rul{$\land$-Elim} applied to 7)
	\item $\Sigma\vdash (F_2(G)\imp F_2(H))$ \hfill (\rul{$\siff$-Def} applied to 2)
	\item $\Sigma\vdash F_2(H)$ \hfill (\rul{$\imp$-Elim} applied to 8,9)
	\item $\Sigma\vdash F_1(H)\land F_2(H)$ \hfill (\rul{$\land$-Intro} applied to 6,10)
\end{enumerate}

Let $F(p)$ be a formula. If $\Sigma\vdash (G\siff H)$ and $\Sigma\vdash F(G)$, then $\Sigma\vdash F(H)$.\\
This is easily shown by using arguments similar to the one above for each connective and then using induction.\\

However, we do not introduce substitution as a rule since it causes a lot of overhead for the proof-checker. We want the time cost of the proof-checker to be low, and allowing substitutions may not allow that to happen. So for example, an argument such as
\begin{enumerate}
	\item $\Sigma\vdash \neg(\neg\neg F\lor G)$ \hfill (\rul{Premise})
	\item $\Sigma\vdash \neg(F\lor G)$ \hfill (\rul{DoubleNeg} applied to $\neg\neg F$ in 1)
\end{enumerate}
is disallowed for our purposes.\\

So far, we have seen several rules of reasoning. But how would one determine if these rules are sufficient to derive all true statements? Further, this feels quite inconvenient since there are so many rules. Often, we also don't know where to start applying them to a particular problem. We aim to simplify and algorithmize this process.


\subsection{Lecture 6}

If we want to develop algorithms, we require more structure in our input. We need to figure out methods for simplification and suitable ``normal forms''.

In order to prove theorems, we need to get used to ``structural induction''. That is,

\begin{theorem}[Structural Induction]
	Every formula in $P$ has property $Q$ if
	\begin{itemize}
		\item every atomic formula has $Q$ and
		\item if $F,G\in P$ have $Q$ then so do $\neg F$ and $(F\circ G)$, where $\circ$ is any binary symbol.
	\end{itemize}
\end{theorem}

Substitution is an important part of logic. We should be able to substitute equivalent subformulas without altering the truth values of formulas.\\

\begin{lemma}
\label{lem: 6.2}
	Suppose we have formulas $F(p)$, $G$, and $H$. Suppose for some model $m$, $m\vDash G\siff m\vDash H$. Then $m\vDash F(G) \siff m\vDash F(H)$.
\end{lemma}
This can be shown using structural induction, working it out for each connective.\\

\begin{lemma}
	If $F(p)\equiv G(p)$, then for each formula $H$, $F(H)\equiv G(H)$.
\end{lemma}
We may assume without loss of generality that $p$ does not appear in $H$. Consider a model $m$. Then define the model $m'$ by
\[
	m'\coloneqq
	\begin{cases}
		m[p\mapsto 1], & m\vDash H, \\
		m[p\mapsto 0], & m\vDash H.
	\end{cases}
\]
Observe that by construction, $m'\vDash p$ iff $m'\vDash H$.\\
Further note that since $p$ does not appear in $\Vars(F(H))$, $m\vDash F(H)$ iff $m'\vDash F(H)$. By the earlier lemma, this implies that $m'\vDash F(H)$ iff $m'\vDash F(p)$.  Since $F(p)\equiv G(p)$, $m'\vDash F(p)$ iff $m'\vDash G(p)$.\\
Again going backwards like we did now, we see that $m'\vDash G(p)$ iff $m'\vDash G(H)$ iff $m\vDash G(H)$. Therefore, $m\vDash F(H)$ iff $m\vDash G(H)$.

\begin{theorem}
	Let $G$, $H$, and $F(p)$ be formulas. If $G\equiv H$, then $F(G)\equiv F(H)$.
\end{theorem}
This is just a reformulation of \Cref{lem: 6.2}.\\
This allows us to use known equivalences to modify formulas. This was also proved in the previous lecture using the proof system.

\subsection{Lecture 7}

\subsubsection{Negation Normal Form}

Observe the following equivalences that remove $\xor$, $\imp$, and $\siff$ from formulas.

\begin{align*}
	(p\imp q) &\equiv (\neg p\lor q) \\
	(p\xor q) &\equiv (p\lor q) \land (\neg p\lor \neg q) \\
	(p\siff q) &\equiv \neg (p\xor q)
\end{align*}

Removing either $\xor$ or $\siff$ results in an explosion in the formula size, but we assume that we can remove them on will.\\
Motivated by this, we define

\begin{definition}
	A formula is in \textit{negation normal form} (NNF) if $\neg$ appears only in front of the propositional variables.
\end{definition}

For any formula $F$, there is a formula $F'$ in NNF such that $F\equiv F'$.\\
We can do stuff like $\neg(p\lor q) = (p\land \neg q)$ for every logical connective and push $\neg$ under them.\\

There are also $\neg$s hidden inside $\xor$, $\imp$, and $\siff$. It is advisable to also remove these when producing the NNF of a formula.

\subsubsection{Conjunctive Normal Form}

Propositional variables are also referred to as \textit{atoms} (recall ``atomic formulas''). A \textit{literal} is either an atom or its negation. A \textit{clause} is a disjunction of literals.

Since $\lor$ is associative, commutative, and absorbs multiple occurrences, a clause can be referred to as a set of literals.

\begin{definition}
	A formula is in \textit{conjunctive normal form} (CNF) if it is a conjunction of clauses.
\end{definition}

Since $\land$ is associative, commutative, and absorbs multiple occurrences, a CNF formula can be referred to as a set of clauses.

For any formula $F$, there is a formula $F'$ in NNF such that $F\equiv F'$.\\
First, we remove $\xor$, $\imp$, and $\siff$ using the usual equivalences. We then convert it to NNF. We then ``flatten'' $\lor$ and $\land$ using the associativity of $\lor$ and $\land$. The parse tree of the formula then has $\lor$ and $\land$ at alternating levels with the leaves being literals. We then distribute $\lor$ over $\land$ while flattening at each step to obtain a formula in CNF.\\
But does this complete the argument? No, we also need to show that the final part of the algorithm (distribution) terminates (pushing using distributivity increases the size of the formula). To see why it should terminate, define $\nu(F)$ to be the maximum height of the $\lor$-$\land$ alternations in $F$. Suppose there is a subformula $G$ such that
\[ G = \bigvee_{i=0}^{m} \bigwedge_{j=0}^{n_i} G_{ij} \]
After pushing, we obtain
\[ G' = \bigwedge_{j_1=0}^{n_1} \cdots \bigwedge_{j_m=0}^{n_m} \bigvee_{i=0}^m G_{ij}. \]
Note that the height of the height of the above must be strictly less than $\nu(G)$.\\
Observe that $G'$ is either the top formula or the parent of this connective is $\land$. Also, each $G_{ij}$ is either a literal or a clause.\\
We must keep flattening to keep $F(G')$ in the considered form. Due to K\"{o}nig's lemma, this procedure must terminate.

\begin{lemma}[K\"{o}nig's Lemma]
	For an infinite connected graph $G$, if each node has finite degree, then there is an infinite simple path from each node.
\end{lemma}

This implies the required since it says that if the parse tree is infinite at any step, then the height at that step must be infinite as well.\\

A \textit{unit clause} contains only one literal. A \textit{binary clause} contains exactly two literals. A \textit{ternary clause} contains exactly three literals.\\
We can also extend this to the empty set of literals. We take by convention that $\perp$ is the empty clause.

\subsubsection{Tseitin's Encoding}

CNF is desirable because it has a very single structure, and only involves three distinct connectives ($\lor$, $\land$, and $\neg$).\\ The transformation using distributivity is bad because it leads to an explosion in the size of the formula. How do we avoid this?

By introducing fresh variables, Tseitin's encoding translates any formula into an equisatisfiable formula in CNF without an exponential explosion.

\begin{enumerate}
	\item First, assume the input formula $F$ is in NNF without $\xor$, $\imp$, and $\siff$.
	\item Find a $G_1\land\cdots\land G_n$ in $F$ that is just below an $\lor$. If no such subformula exists, terminate.
	\item Replace $F(G_1\land\cdots\land G_n)$ by $F(p)\land(\neg p\lor G_1)\land\cdots\land(\neg p\lor G_n)$, where $p$ is a newly introduced variable.
	\item Go to step 2.
\end{enumerate}

For example, $(p_1\land\cdots\land p_n)\lor(q_n\land\cdots\land q_m)$ becomes
\[ (x\lor y) \land \bigwedge_{1\leq i\leq n} (\neg x\lor p_i) \land \bigwedge_{1\leq j\leq m} (\neg y \lor q_j), \]
which has only $m+n+1$ clauses, as opposed to the $mn$ clauses returned by the na\"{i}ve algorithm. Here, $x$ and $y$ are the newly introduced variables.\\

Why does this preserve satisfiability?\\
Suppose $m\vDash F(p)\land(\neg p\lor G_1)\land\cdots\land(\neg p\lor G_n)$. We have three cases.
\begin{enumerate}[(i)]
	\item If $m\vDash p$, then $m\vDash G_i$ for every $1\leq i\leq n$. Therefore, $m\vDash G_1\land\cdots\land G_n$. We can then perform a substitution to get that $m\vDash F(G_1\land\cdots\land G_n)$.
	\item If $m\nvDash p$ and $m\nvDash G_1\land\cdots\land G_n$. Since $m\vDash F(p)$, we can directly apply the substitution theorem again.
	\item If $m\nvDash p$ and $m\vDash G_1\land\cdots\land G_n$. Since $F(G_1\land\cdots\land G_n)$ is in NNF, $p$ must occur positively (no $\neg p$) in $F(p)$. Therefore, $m[p\mapsto 1]\vDash F(p)$. Since $p$ does not occur in any $G_i$, we also obviously have $m[p\mapsto 1]\vDash G_1\land\cdots\land G_n$. Therefore, $m\vDash F(G_1\land\cdots\land G_n)$ (we basically ignore the modification).
\end{enumerate}