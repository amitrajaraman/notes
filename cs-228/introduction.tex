\section{Introduction to Logic}

\subsection{Lecture 1}

For any computer scientist, logic is an extremely basic tool. Consider the statement
\begin{quote}
	This sentence is false.
\end{quote}
A little bit of thought shows that the above sentence has no definite truth value -- it is \textit{paradoxical}. Indeed, it is often known as the ``liar's paradox". This sort of self-referential sentence will come back to haunt us many more times in the future.\\

Propositional logic (or \textit{zeroth-order logic}) is basically the form of logic that deals with propositions which can be true or false as well as relations between them.\\
A more useful tool is that of \textit{first-order logic}, that also deals with non-logical objects, predicates about them, and quantifiers ($\forall$ and $\exists$). That is, we are allowed to quantify over elements of the set, but not something like subsets of the set. A lot of mathematical statements cannot be written when we are restricted to first-order logic.\\

We study theories with basic assumptions or \textit{axioms}. Using these axioms, we aim to prove more non-trivial results within the theory. A natural question to ask is: is it possible to have some set of axioms that allow us to concretely determine the truth value of any consequential statement? Once more, the self-referential statement returns.

\begin{theorem}[G\"odel's Incompleteness Theorem]
There are theories whose assumptions cannot be listed.
\end{theorem}
\begin{proof}
Suppose that there exists such a list for the number theory. Consider the \textbf{true} statement
\begin{quote}
	This sentence cannot be proven by the list.
\end{quote}
The list cannot imply the sentence. This yields a contradiction.
\end{proof}

This theorem shows that logic has all but failed as a tool to do math. From this failure, rose computer science. We discuss G\"odel's Incompleteness more concretely later.

\subsection{Lecture 2}

\subsubsection{Propositional Logic: Syntax and Parsing}

We need an efficient method to identify if some group of symbols is a logical argument. We usually define a syntax for this (similar to grammar in English).\\

The logic we consider is over some list of propositions. We give each proposition a symbol. So say there is some set $\mathsf{Vars}$ of \textit{countably many} propositional variables.
% Say
% \[ \mathsf{Vars} = \{p_1,p_2,\ldots\}. \]
These propositional variables are also called \textit{Boolean variables}.\\
Propositions are connected by \textit{logical arguments}. How can we connect propositions?
\begin{itemize}
	\item A statement that is always true/false.
	\item Negation. A statement that is the negation of another.
	\item Conjunction. Two statements being true simultaneously.
	\item Disjunction. At least one of two statements being true.
	\item Implication. If a statement is true, then some other statement is true as well.
	\item Equivalence. Two statements always have the same truth value.
	\item Disequality or exclusive or. Two statements always have different truth values.
\end{itemize}

\begin{center}
\begin{tabular}{c|c|c}
	 true & $\top$ & top  \\
	 false & $\bot$ & bot \\
	 negation & $\neg$ & not \\
	 conjunction & $\wedge$ & and \\
	 disjunction & $\vee$ & or \\
	 implication & $\implies$ & implies \\
	 equivalence & $\iff$ & iff \\
	 exclusive or & $\oplus$ & xor \\
	 opening parenthesis & ( & \\
	 closing parenthesis & ) &
\end{tabular}
\end{center}

We assume that the above \textit{logical connectives} are not in $\mathsf{Vars}$.\\
A \textit{propositional formula} is a finite string containing symbols in $\mathsf{Vars}$ and logical connectives.

\begin{definition}
The set of propositional formulas is the smallest set $P$ such that
\begin{itemize}
	\item $\top,\bot$ are in $P$,
	\item $\mathsf{Vars}\subseteq P$,
	\item if $F\in P$, then $\neg F\in P$, and
	\item if $\circ$ is a binary symbol and $F,G\in P$, then $(F\circ G)\in P$.
\end{itemize}
\end{definition}

Alternatively, this can succinctly be written as ``$F \in P$ if
\[F \coloneqq p\mid\top\mid\bot\mid \neg F \mid (F\vee F) \mid (F\wedge F) \mid (F\imp F) \mid (F\siff F) \mid (F\oplus F) \]
where $p\in\mathsf{Vars}$."

\begin{definition}
$\top$, $\bot$, and any $p\in\mathsf{Vars}$ are known as \textit{atomic formulas}.
\end{definition}

\begin{definition}
For each $F\in P$, $\mathsf{Vars}(F)$ is the set of variables appearing in $F$.
\end{definition}

It is important to note that parentheses are needed (only) between binary operations. So as of now, $(\bot\imp\top)$ is a formula but $\bot\imp\top$ isn't.\\
Not all strings over $\mathsf{Vars}$ and logical connectives are in $P$.

\subsubsection{Examples Encoding Arguments into Logic}

Consider the following argument.
\begin{quote}
	If $c$ then if $s$ then $f$. not $f$. Therefore, if $s$ then not $c$.
\end{quote}
This can be written as
\[ (((c\imp (s\imp f)) \wedge \neg f) \imp (s \imp \neg c)). \]
Another example, say we know that good people always tell the truth and not good people always tell a lie. If there are two people $A$ and $B$ and $A$ says ``I am not good or $B$ is good", then what are $A$ and $B$?\\
Suppose the variables $p_A$ and $p_B$ denote whether $A$ and $B$ are truthful or not. Then the above is basically
\[ ((\neg p_A \vee p_B) \iff p_A). \]
How do we determine whether there are some $p_A,p_B$ that satisfies this?

\subsubsection{Parsing Formulas}

$F\in P$ iff it can be obtained by unfolding one of these generation rules.

\begin{definition}
A \textit{parse tree} of a formula $F\in P$ is a tree such that
\begin{itemize}
	\item the root is $F$,
	\item the leaves are atomic formulas, and
	\item each internal node is formed by applying some formulation rule on its children.
\end{itemize}
\end{definition}

We have the following 

\begin{theorem}
$F\in P$ iff there is a parse tree of $F$. Further, if $F\in P$, it has a \textit{unique} parse tree.
\end{theorem}
The reverse direction follows by definition.\\

A parse tree is a directed acyclic graph (DAG). The parsing produces a parse DAG. This is done by not writing repeated symbols twice, ensuring that all arrows go from higher levels of the DAG to the lower ones.

\begin{definition}
A formula $G$ is a \textit{subformula} of a formula $F$ if $G$ occurs within $F$. Further, $G$ is a proper subformula of $F$ if $F\neq G$. Denote by $\mathsf{sub}(F)$ the set of subformulas of $F$.
\end{definition}

Observe that the nodes of the parse tree of $F$ form $\mathsf{sub}(F)$.

Immediate subformulas are the children of a formula in its parse tree. The corresponding \textit{leading connective} is the connective that joins it to the children. So for example,

\[ \mathsf{sub}((\neg p_2 \iff (p_1 \wedge p_3)) = \{((\neg p_2 \iff (p_1 \wedge p_3)), \neg p_2, (p_1 \wedge p_3), p_1, p_2, p_3\}. \]

\subsubsection{Shorthands}

The reader might have noticed by now that we need to write far more parentheses than required which don't really feel necessary most of the time. If we use some sort of precedence order over logical connectives, we may be able to drop some parentheses without losing the unique parsing property.\\
For example, we may drop outermost parentheses without any confusion. An example of this is writing $((p \wedge q)\imp (r \vee p))$ as $(p \wedge q)\imp (r \vee p)$.\\
Further, in the above example, if we give $\vee$ and $\wedge$ higher precedence then $\imp$ during parentheses, then we can drop all the parentheses! The usual precedence order we use is
\[ \neg > \vee = \wedge = \oplus > \imp = \iff. \]
So how do we go about parsing a formula then? Suppose we have $F_0\circ_1 F_2\circ_2 \cdots\circ_n F_n$, where each $F_i$ is either atomic, enclosed by parentheses, or their negation. We transform it as follows.
\begin{itemize}
	\item Find a $\circ_i$ such that $\circ_{i-1}$ and $\circ_{i+1}$ have lower precedence (if they exist).
	\item Introduce parentheses around $F_{i-1}\circ F_i$ and call it $F_i'\coloneqq (F_{i-1}\circ_i F_i)$ so we now have
	\[ F_0 \circ_1 \cdots \circ_{i-2} F_{i-2} \circ_{i-1} F_i' \circ_{i+1} F_{i+1} \circ_{i+2} \cdots \circ_n F_n. \]
\end{itemize}
Repeat the above until only one term remains. We can then parse it normally. For example,
\[ p \wedge q \imp r \vee p \text{ to } (p\wedge q)\imp r\vee p \text{ to } (p\wedge q)\imp (r\vee p) \text{ to } ((p\wedge q)\imp (r\vee p)). \]
Some formulas cannot be unambiguously parsed, for example $p\vee q\wedge r$, $p \vee q \vee r$, or $p\imp q\imp r$. But can we salvage any of them?\\
Associativity preference may further reduce the need of parentheses. Let's make all our operators right associative (first group the rightmost occurrence). So for example, unless mentioned otherwise, we take $p\imp q\imp r$ as $(p\imp (q\imp r))$.

\begin{definition}
For $F\in P$ and $p_1,\ldots,p_k\in\Vars$, we denote by $F[G_1/p_1,\ldots,G_k/p_k]$ the formula obtained by \textit{simultaneously} replacing all occurrences of $p_i$ by the formula $G_i$ for each $i\in[k]$. 
\end{definition}

So for example,
\[ (p\imp (r\imp p))[(r\otimes p)/p] = ((r\otimes p)\imp (r\imp (r\otimes p))). \]
Sometimes, we may also write a formula $F$ as $F(p_1,\ldots,p_k)$. Then, by $F(G_1,\ldots,G_n)$, we mean $F[G_1/p_1,\ldots,G_k/p_k]$.

\subsection{Lecture 3}

\subsubsection{Semantics}

Semantics is giving meaning to formulas. We denote the set of truth values as $\mathcal{B}\coloneqq\{0,1\}$. We may view $0$ as ``false" and $1$ as ``true", but the only important thing is that they are distinct.

\begin{definition}[Model]
A \textit{model} is an function from $\Vars\to\mathcal{B}$.
\end{definition}

For example, $\{p_1\mapsto 1, p_2\mapsto 0, p_3\mapsto 0,\ldots\}$ is a model. \\
It is quite natural to extend this further to formulas in general. That is, a given model $m$ may or may not satisfy a formula $F$. More concretely.

\begin{definition}
The \textit{satisfaction relation} $\vDash$ between models and formulas is the smallest relation that satisfies the following.
\begin{itemize}
	\item $m\vDash\top$,
	\item if $m(p)=1$, then $m\vDash p$,
	\item if $m\nvDash F$, then $m\vDash\neg F$,
	\item if $m\vDash F_1$ or $m\vDash F_2$, then $m\vDash (F_1\vee F_2)$,
	\item if $m\vDash F_1$ and $m\vDash F_2$, then $m\vDash (F_1\wedge F_2)$,
	\item if $m\vDash F_1$ and $m\vDash F_2$ but not both, then $m\vDash (F_1\oplus F_2)$,
	\item if if $m\vDash F_1$ then $m\vDash F_2$, then $m\vDash (F_1\imp F_2$, and
	\item if $m\vDash F_1$ iff $m\vDash F_2$, then $m\vDash (F_1\Leftrightarrow F_2)$.
\end{itemize}
\end{definition}

Observe that $\bot$ is not explicitly mentioned in the above definition since it follows from it being the \textit{smallest} relation.\\

If $m\vDash F$, we say that $m$ \textit{satisfies} $F$.\\
$F$ is \textit{satisfiable} if there is a model $m$ such that $m\vDash F$. This is often abbreviated as \textit{sat}.\\
$F$ is \textit{valid} (written $\vDash F$) if for each model $m$, $m\vDash F$. A valid formula is also called a \textit{tautology}.\\
$F$ is \textit{unsatisfiable} (written $\nvDash F$) if there is no model $m$ such that $m\vDash F$. This is often abbreviated as \textit{unsat}.\\

A few things to note are:
\begin{itemize}
	\item A formula is valid iff its negation is unsat.
\end{itemize}

We can check if a certain formula satisfies a model by moving bottom-up in the parse tree.\\

We overload the $\vDash$ operator in several natural ways.

\begin{definition}
Let $M$ be a set of models. We write $M\vDash F$ if for every $m\in M$, $m\vDash F$.
\end{definition}

\begin{definition}
Let $\Sigma$ be a set of formulas. We write $\Sigma\vDash F$ if for every $m$ that satisfies every formula in $\Sigma$, $m\vDash F$.
\end{definition}

This is read ``$\Sigma$ implies $F$''. If $\Sigma=\{G\}$, we write $G\vDash F$.

\begin{definition}
We write $F\equiv G$ if for each model $m$,
\[ m\vDash F \iff m\vDash G. \]
\end{definition}

\begin{definition}
Formulas $F$ and $G$ are \textit{equisatisfiable} if
\[ F \text{ is sat} \iff G \text{ is sat.}\]
\end{definition}

\begin{definition}
Formulas $F$ and $G$ are \textit{equivalid} if $\vDash F\iff\vDash G$.
\end{definition}

\subsubsection{Decidability of SAT}

\begin{fdef}
A problem is \textit{decidable} if there is an algorithm to solve the problem.
\end{fdef}

This is required since G\"odel's Incompleteness implies the existence of undecidable problems.

The problem we consider here, known as the \textit{propositional satisfiability problem} is:
\begin{quote}
For a given $F\in P$, is $F$ satisfiable?
\end{quote}

\begin{theorem}
The propositional satisfiability problem is decidable.
\end{theorem}
\begin{proof}
We enumerate the $2^{|\Vars(F)|}$ elements of $\Vars(F)\to\mathcal{B}$. If any of the models satisfy the formula, $F$ is sat. Otherwise, it is unsat.
\end{proof}

The cost is obviously exponential and we would want to do better. Indeed, there are several tricks that make satisfiability checking more feasible for real-world formulas.

\subsubsection{Truth Tables}

We wish to assign a truth value to every formula $F$.

Given a model $m:\Vars\to\B$, we can naturally extend it to $m:P\to \B$ as
\[
	m(F) = 
	\begin{cases}
	1, & m\vDash F, \\
	0, & \text{otherwise.}
	\end{cases}
\]

This extended $m$ is known as the \textit{truth function}. We do not introduce new notation for this, keeping the meanings from the definitions for models unchanged.\\

For a formula $F$, a truth table consists of $2^{|\Vars(F)|}$ rows, where each row considers one of the models and computes the corresponding truth value of $F$.

Truth tables are sometimes useful to prove that formulae are equivalent. For example, show that $p\vee q \equiv \neg(\neg p \wedge \neg q)$ and $p\wedge q \equiv \neg(\neg p \vee \neg q)$, also known as De Morgan's laws.\\
It is also easily shown that $p\imp q \equiv (\neg p \vee q)$.

Truth tables are tedious because we need to write $2^n$ rows even if a simple observation could easily show (un)satisfiability.\\
For example, $a \vee (c \wedge b)$ being sat is very clearly true. If there are no $\neg$s (of any form, $\oplus$ in particular) in general, one can just set everything as true. Another example is that $(a \vee (c \neg a))\wedge\neg (a \vee (c \neg a))$ is obviously unsat.\\
How do we take such shortcuts?

\subsubsection{Expressive power of propositional logic}

A finite boolean function is one from $\B^n\to\B$.\\
A formula $F$ with $\Vars(F)=\{p_1,\ldots,p_n\}$ can be viewed as a boolean function $f$ such that for each model $m$, $m(F)=f(m(p_1),\ldots,m(p_n))$. This is just an alternate way of writing a truth table (as a function instead of a table).

\begin{theorem}
For each finite boolean function $f$, there is a formula $F$ that represents $f$.
\end{theorem}
\begin{proof}
Let $f:\B^n\to\B$. Let $p_i^0\coloneqq\neg p_i$ and $p_i^1\coloneqq p_i$. For every $(b_1,\ldots,b_n)\in\B^n$, let
\[
	F_{(b_1,\ldots,b_n)} \coloneqq
	\begin{cases}
		(p_1^{b_1}\wedge\cdots\wedge p_n^{b_n}), & f(b_1,\ldots,b_n)=1 \\
		\bot, & \text{otherwise.}
	\end{cases}
\]
We can then define the required formula $F$ by taking the conjunction over all boolean combinations,
\[ F \coloneqq F_{(0,\ldots,0)} \vee \cdots \vee F_{(1,\ldots,1)}. \]
\end{proof}

Observe that we have only used three logical connectives.

What if we do not have all logical connectives? Then we may not be able to represent all boolean functions. This is known as ``insufficient expressive power".\\

For example, $\wedge$ alone cannot express all boolean functions. Consider the function $f=\{0\mapsto 1, 1\mapsto 1\}$. We show that this cannot be achieved by any $\wedge$s by taking induction on the size of formulas containing the variable $p$ and $\wedge$. For the base case, our only choice of formula is $p$. Now, suppose that formulas $F$ and $G$ of size less than $n-1$ do not represent $f$. We can construct a longer formula by $(F\wedge G)$. This formula does not represent $f$ because we can always pick a model where $F$ or $G$ produce $0$.

We originally used $8$ connectives. This is not the minimal set required for maximum expressivity, however. For example, $\neg$ and $\vee$ can define the whole propositional logic. Indeed,
\begin{itemize}
	\item $\top \equiv p \vee \neg p$,
	\item $\bot \equiv \neg \top$,
	\item $(p \wedge q) \equiv \neg(p \vee q)$,
	\item $(p\otimes q) \equiv (p \wedge \neg q) \vee (\neg p \wedge q)$,
	\item $(p\imp q) \equiv (\neg p \vee q)$, and
	\item $(p\Leftrightarrow q) \equiv (p\imp q) \wedge (q\imp p)$.
\end{itemize}

\subsection{Lecture 4}

\subsubsection{Formal Proofs}

Suppose that for a set of formulas $\Sigma$ and a formula $F$, $\Sigma\vDash F$. Can we infer that $\Sigma\vDash F$ without writing out the truth tables? This syntactic inference is called \textit{derivation}. This is written $\Sigma\vdash F$ and is read ``$\Sigma$ proves $F$''. In this case, $F$ is said to be a ``consequence'' of $\Sigma$.\\

If $F$ occurs on the left hand side $F\in\Sigma$, then $F$ is clearly a consequence.\\

A \textit{proof rule} provides us a means to derive new statements from old statements. They are written as
\begin{prooftree}
	\LeftLabel{\rul{RuleName}}
	\AxiomC{Stuff already there}
	\RightLabel{\rul{Conditions to be met}}
	\UnaryInfC{Stuff to be added}
\end{prooftree}
A derivation proceeds by applying these proof rules.
So for instance, the rule we mentioned earlier can be written as
\begin{prooftree}
	\LeftLabel{\rul{Assumption}}
	\AxiomC{}
	\RightLabel{$F\in\Sigma.$}
	\UnaryInfC{$\Sigma\vdash F$}
\end{prooftree}
Another obvious example is
\begin{prooftree}
	\LeftLabel{\rul{Monotonic}}
	\AxiomC{$\Sigma\vdash F$}
	\RightLabel{$\Sigma\subseteq\Sigma'.$}
	\UnaryInfC{$\Sigma'\vdash F$}
\end{prooftree}

\begin{definition}
A \textit{derivation} is a list of statements that are derived from earlier statements.
\end{definition}

An example of a derivation using the above rules is:
\begin{enumerate}
	\item $\{p\vee q,\neg\neg q\}\vdash\neg\neg q$ \hfill (\rul{Assumption})
	\item $\{p\vee q, \neg\neg q, r\}\vdash\neg\neg q$ \hfill (\rul{Monotonic} applied to 1)
\end{enumerate}
It is important to note that we need to explicitly point out which earlier step we are using (if any).

Let us try to establish some proof rules on our logical connectives.

\paragraph{Negation.}

\begin{prooftree}
	\LeftLabel{\rul{DoubleNeg}}
	\AxiomC{$\Sigma\vdash F$}
	% \RightLabel{$\Sigma\subseteq\Sigma'.$}
	\UnaryInfC{$\Sigma\vdash\neg\neg F$}
\end{prooftree}

For example,
\begin{enumerate}
	\item $\{p\vee q,r\}\vdash r$ \hfill (\rul{Assumption})
	\item $\{p\vee q, r, \neg\neg q\}\vdash r$ \hfill (\rul{Monotonic} applied to 1)
	\item $\{p\vee q, r, \neg\neg q\}\vdash\neg\neg r$ \hfill (\rul{DoubleNeg} applied to 2)
\end{enumerate}

\paragraph{Conjunction.}

We have the following proof rules for the conjunction.

\begin{prooftree}
	\LeftLabel{$\wedge$-\rul{Intro}}
	\AxiomC{$\Sigma\vdash F$}
	\AxiomC{$\Sigma\vdash G$}
	% \RightLabel{$\Sigma\subseteq\Sigma'.$}
	\BinaryInfC{$\Sigma\vdash F\wedge G$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\wedge$-\rul{Elim}}
	\AxiomC{$\Sigma\vdash F\wedge G$}
	% \RightLabel{$\Sigma\subseteq\Sigma'.$}
	\UnaryInfC{$\Sigma\vdash F$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\wedge$-\rul{Symm}}
	\AxiomC{$\Sigma\vdash F\wedge G$}
	\UnaryInfC{$\Sigma\vdash G\wedge F$}
\end{prooftree}

An example using these is
\begin{enumerate}
	\item $\{p\wedge q, \neg\neg q, r\}\vdash p\wedge q$ \hfill (\rul{Assumption})
	\item $\{p\wedge q, \neg\neg q, r\}\vdash p$ \hfill (\rul{$\wedge$-Elim} applied to 1)
	\item $\{p\wedge q, \neg\neg q, r\}\vdash q\wedge p$ \hfill (\rul{$\wedge$-Symm} applied to 1)
\end{enumerate}

\paragraph{Disjunction.}

Except for the last two (which are like De Morgan's law), the rules for the disjunction are similar.

\begin{prooftree}
	\LeftLabel{$\vee$-\rul{Intro}}
	\AxiomC{$\Sigma\vdash F$}
	\UnaryInfC{$\Sigma\vdash F\vee G$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\vee$-\rul{Elim}}
	\AxiomC{$\Sigma\vdash F\vee G$}
	\AxiomC{$\Sigma\cup\{F\}\vdash H$}
	\AxiomC{$\Sigma\cup\{G\}\vdash H$}
	\TrinaryInfC{$\Sigma\vdash H$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\vee$-\rul{Symm}}
	\AxiomC{$\Sigma\vdash F\vee G$}
	\UnaryInfC{$\Sigma\vdash G\vee F$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\vee$-\rul{Def}}
	\AxiomC{$\Sigma\vdash F\vee G$}
	\UnaryInfC{$\Sigma\vdash \neg(\neg F \wedge \neg G)$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\vee$-\rul{Def}}
	\AxiomC{$\Sigma\vdash \neg(\neg F \wedge \neg G)$}
	\UnaryInfC{$\Sigma\vdash F\vee G$}
\end{prooftree}

Let us give another example, which is basically equivalent to the distributive law. Suppose we have $\Sigma\vdash (F\wedge G)\vee(F\wedge H)$, we want to show that we can derive $\Sigma\vdash F\wedge(G\vee H)$. Indeed,
\begin{enumerate}
	\item $\Sigma\vdash (F\wedge G)\vee(F\wedge H)$ \hfill (\rul{Premise})%1
	
	\item $\Sigma\cup\{F\wedge G\} \vdash F\wedge G$ \hfill (\rul{Assumption})%2
	\item $\Sigma\cup\{F\wedge G\} \vdash F$ \hfill (\rul{$\wedge$-Elim} applied to 2)%3
	\item $\Sigma\cup\{F\wedge G\} \vdash G\wedge F$ \hfill (\rul{$\wedge$-Symm} applied to 2)%4
	\item $\Sigma\cup\{F\wedge G\} \vdash G$ \hfill (\rul{$\wedge$-Elim} applied to 4)%5
	\item $\Sigma\cup\{F\wedge G\}\vdash G\vee H$ \hfill (\rul{$\vee$-Intro} applied to 5)%6
	\item $\Sigma\cup\{F\wedge G\}\vdash F\wedge(G\vee H)$ \hfill (\rul{$\wedge$-Intro} applied to 3,6)%7

	\item $\Sigma\cup\{F\wedge H\} \vdash F\wedge H$ \hfill (\rul{Assumption})%8
	\item $\Sigma\cup\{F\wedge H\} \vdash F$ \hfill (\rul{$\wedge$-Elim} applied to 8)%9
	\item $\Sigma\cup\{F\wedge H\} \vdash H\wedge F$ \hfill (\rul{$\wedge$-Symm} applied to 8)%10
	\item $\Sigma\cup\{F\wedge H\} \vdash H$ \hfill (\rul{$\wedge$-Elim} applied to 10)%11
	\item $\Sigma\cup\{F\wedge H\}\vdash H\vee G$ \hfill (\rul{$\vee$-Intro} applied to 11)%12
	\item $\Sigma\cup\{F\wedge H\}\vdash G\vee H$ \hfill (\rul{$\vee$-Symm} applied to 11)%13
	\item $\Sigma\cup\{F\wedge G\}\vdash F\wedge(G\vee H)$ \hfill (\rul{$\wedge$-Intro} applied to 9,13)%14

	\item $\Sigma\vdash F\wedge(G\vee H)$ \hfill (\rul{$\vee$-Elim} applied to 1,7,14)%15
\end{enumerate}

\paragraph{Implication.}

The rules for the implication are:

\begin{prooftree}
	\LeftLabel{$\imp$-\rul{Intro}}
	\AxiomC{$\Sigma\cup \{F\}\vdash G$}
	\UnaryInfC{$\Sigma\vdash F \imp G$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\imp$-\rul{Elim}}
	\AxiomC{$\Sigma\vdash F\imp G$}
	\AxiomC{$\Sigma\vdash F$}
	\BinaryInfC{$\Sigma\vdash G$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\imp$-\rul{Def}}
	\AxiomC{$\Sigma\vdash F\imp G$}
	\UnaryInfC{$\Sigma\vdash \neg F\vee G$}
\end{prooftree}

\begin{prooftree}
	\LeftLabel{$\imp$-\rul{Def}}
	\AxiomC{$\Sigma\vdash \neg F\vee G$}
	\UnaryInfC{$\Sigma\vdash F\imp G$}
\end{prooftree}

For example, let us show that $\{\neg p\vee q,p\}\vdash q$.
\begin{enumerate}
	\item $\{\neg p\vee q, p\}\vdash p$ \hfill (\rul{Assumption})
	\item $\{\neg p\vee q, p\}\vdash \neg p\vee q$ \hfill (\rul{Assumption})
	\item $\{\neg p\vee q, p\}\vdash p\imp q$ \hfill (\rul{$\imp$-Def} applied to 2)
	\item $\{\neg p\vee q, p\}\vdash q$ \hfill (\rul{$\imp$-Elim} applied to 1,3)
\end{enumerate}

As another example, let us show that $\emptyset\vdash(p\imp q)\vee p$.
\begin{enumerate}
	\item $\{\neg p\}\vdash\neg p$ \hfill (\rul{Assumption})
	\item $\{\neg p\}\vdash \neg p \vee q$ \hfill (\rul{$\vee$-Intro} applied to 1)
	\item $\{\neg p\}\vdash p \imp q$ \hfill (\rul{$\imp$-Def} applied to 2)
	\item $\{\neg p\}\vdash (p \imp q)\vee p$ \hfill (\rul{$\vee$-Intro} applied to 3)

	\item $\{p\}\vdash p$ \hfill (\rul{Assumption})
	\item $\{p\}\vdash p\vee(p\imp q)$ \hfill (\rul{$\vee$-Intro} applied to 5)
	\item $\{p\}\vdash (p\imp q)\vee p$ \hfill (\rul{$\vee$-Symm} applied to 6)

	\item $\emptyset\vdash (p\imp p)$ \hfill (\rul{$\imp$-Intro} applied to 5)
	\item $\emptyset\vdash (\neg p\vee p)$ \hfill (\rul{$\imp$-Def} applied to 8)
	
	\item $\emptyset\vdash (p\imp q)\vee p$ \hfill (\rul{$\vee$-Elim} applied to 4,7,9)
	% \item $\emptyset\vdash (\neg p)\imp (p\imp q)$ \hfill (\rul{$\imp$-Intro} applied to 3)
	% \item $\emptyset\vdash (\neg \neg p) \vee (p\imp q)$ \hfill (\rul{$\imp$-Def} applied to 4)
\end{enumerate}

There are several more proof rules for parentheses, $\oplus$, $\iff$, et cetera.

\subsubsection{Soundness}

How do we know that the above proof rules are correct? What does ``correct'' even mean?

\begin{theorem}
\label{theo: derivation implies modeling}
	If proof rules derive a statement $\Sigma\vdash F$, then $\Sigma\vDash F$.
\end{theorem}
We use an inductive argument. Assume that the theorem holds for the premises of the rules. We shall show that it is also true for the conclusions.\\

The above basically unifies the \textit{syntactic} and \textit{semantic} methods of proof. We shall later see that the converse is true holds as well.

Consider the following rule
\begin{prooftree}
	\LeftLabel{$\wedge$-\rul{Elim}}
	\AxiomC{$\Sigma\vdash F\wedge G$}
	\UnaryInfC{$\Sigma\vdash F$}
\end{prooftree}
Consider a model $m\vDash\Sigma$. By the induction hypothesis, $m\vDash F\wedge G$. It is easy to show (using the truth table) that if $m\vDash F\wedge G$, then $m\vDash F$. Therefore, $\Sigma\vDash F$.\\

As another example, consider
\begin{prooftree}
	\LeftLabel{$\imp$-\rul{Intro}}
	\AxiomC{$\Sigma\cup\{F\}\vdash G$}
	\UnaryInfC{$\Sigma\vdash F\imp G$}
\end{prooftree}
Consider a model $m\vDash\Sigma$. There are two possibilities.
\begin{itemize}
	\item $m\vDash F$. Then $m\vDash \Sigma\cup\{F\}$. By the hypothesis, $m\vDash G$. Therefore, $m\vDash F\imp G$.
	\item $m\nvDash F$. Then $m$ trivially satisfies $m\vDash F\imp G$.
\end{itemize}

\subsection{Lecture 5}

\subsubsection{Derived Rules}

In this section and the next, we give some rules derived from those already given that are quite useful when trying to prove statements.

\paragraph{Modus ponens.} %If we have $\Sigma\vdash\neg F\vee G$ and $\Sigma\vdash F$, then we can derive $\Sigma\vdash G$. That is,
\begin{prooftree}
	\LeftLabel{$\vee$-\rul{ModusPonens}}
	\AxiomC{$\Sigma\vdash \neg F\vee G$}
	\AxiomC{$\Sigma\vdash F$}
	\BinaryInfC{$\Sigma\vdash G$}
\end{prooftree}
The proof is as follows.
\begin{enumerate}
	\item $\Sigma\vdash\neg F\vee G$ \hfill (\rul{Premise})
	\item $\Sigma\vdash (F\imp G)$ \hfill (\rul{$\imp$-Def} applied to 1)
	\item $\Sigma\vdash F$ \hfill (\rul{Premise})
	\item $\Sigma\vdash G$ \hfill (\rul{$\imp$-Elim} applied to 2,3) 
\end{enumerate}

\paragraph{Tautology.} %For any $F$ and set $\Sigma$ of formulas, we can derive $\Sigma\vdash \neg F\vee F$. That is,
\begin{prooftree}
	\LeftLabel{\rul{Tautology}}
	\AxiomC{}
	\UnaryInfC{$\Sigma\vdash \neg F \vee F$}
\end{prooftree}
This can be derived as
\begin{enumerate}
	\item $\Sigma\cup\{F\}\vdash F$ \hfill (\rul{Assumption})
	\item $\Sigma\vdash (F\imp G)$ \hfill (\rul{$\imp$-Intro} applied to 1)
	\item $\Sigma\vdash \neg F\vee F$ \hfill (\rul{$\imp$-Def} applied to 2)
\end{enumerate}

\paragraph{Contradiction.} %If we have $\Sigma\vdash F\land\neg F$, then $\Sigma\vdash G$. That is,
\begin{prooftree}
	\LeftLabel{\rul{Contra}}
	\AxiomC{$\Sigma\vdash F\land\neg F$}
	\UnaryInfC{$\Sigma\vdash G$}
\end{prooftree}
This is proved as
\begin{enumerate}
	\item $\Sigma\vdash (F\land\neg F)$ \hfill (\rul{Premise})
	\item $\Sigma\vdash (\neg F\land G)$ \hfill (\rul{$\land$-Symm} applied to 1)
	\item $\Sigma\vdash \neg F$ \hfill (\rul{$\land$-Elim} applied to 2)
	\item $\Sigma\vdash \neg F\lor G$ \hfill (\rul{$\lor$-Intro} applied to 3)
	\item $\Sigma\vdash F$ \hfill (\rul{$\land$-Elim} applied to 1)
	\item $\Sigma\vdash G$ \hfill (\rul{$\lor$-ModusPonens} applied to 4,5)
\end{enumerate}

\paragraph{Contrapositive.} %If $\Sigma\cup\{F\}\vdash G$, then $\Sigma\cup\{\neg G\}\vdash\neg F$. That is,
\begin{prooftree}
	\LeftLabel{\rul{Contrapositive}}
	\AxiomC{$\Sigma\cup\{F\}\vdash G$}
	\UnaryInfC{$\Sigma\cup\{\neg G\}\vdash \neg F$}
\end{prooftree}
This is proved as
\begin{enumerate}
	\item $\Sigma\cup\{F\}\vdash G$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{F\}\vdash \neg\neg G$ \hfill (\rul{DoubleNeg} applied to 1)
	\item $\Sigma\vdash (F \imp \neg\neg G)$ (\rul{$\imp$-Intro} applied to 2)
	\item $\Sigma\vdash \neg F \lor \neg\neg G$ \hfill (\rul{$\imp$-Def} applied to 3)
	\item $\Sigma\vdash \neg\neg G \lor \neg F$ \hfill (\rul{$\lor$-Symm} applied to 4)
	\item $\Sigma\vdash \neg G\imp \neg F$ \hfill (\rul{$\imp$-Def} applied to 5)
	\item $\Sigma\cup\{\neg G\}\vdash \neg G\imp\neg F$ \hfill (\rul{Monotonic} applied to 6)
	\item $\Sigma\cup\{\neg G\}\vdash \neg G$ \hfill (\rul{Assumption})
	\item $\Sigma\cup\{\neg G\}\vdash \neg F$ \hfill (\rul{$\imp$-Elim} 7,8)
\end{enumerate}

\subsubsection{More Derived Rules}

\paragraph{Proof by cases.} %If we have $\Sigma\cup\{F\}\vdash G$ and $\Sigma\cup\{\neg F\}\vdash G$, then $\Sigma\vdash G$. That is, 
\begin{prooftree}
	\LeftLabel{\rul{ByCases}}
	\AxiomC{$\Sigma\cup\{F\}\vdash G$}
	\AxiomC{$\Sigma\cup\{\neg F\}\vdash G$}
	\BinaryInfC{$\Sigma\vdash G$}
\end{prooftree}
This can be proved as
\begin{enumerate}
	\item $\Sigma\cup\{F\}\vdash G$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{\neg F\}\vdash G$ \hfill (\rul{Premise})
	\item $\Sigma\vdash F\lor\neg F$ \hfill (\rul{Tautology})
	\item $\Sigma\vdash G$ \hfill (\rul{$\lor$-Elim} applied to 1,2,3)
\end{enumerate}

\paragraph{Proof by contradiction.} %If we have $\Sigma\cup\{F\}\vdash G$ and $\Sigma\cup\{F\}\vdash\neg G$, then $\Sigma\vdash\neg F$. That is,
\begin{prooftree}
	\LeftLabel{\rul{ByContra}}
	\AxiomC{$\Sigma\cup\{F\}\vdash G$}
	\AxiomC{$\Sigma\cup\{F\}\vdash \neg G$}
	\BinaryInfC{$\Sigma\vdash \neg F$}
\end{prooftree}
This is proved as
\begin{enumerate}
	\item $\Sigma\cup\{F\}\vdash G$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{F\}\vdash \neg G$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{\neg G\}\vdash \neg F$ \hfill (\rul{Contrapositive} applied to 1)
	\item $\Sigma\cup\{\neg \neg G\}\vdash \neg F$ \hfill (\rul{Contrapositive} applied to 1)
	\item $\Sigma\vdash \neg F$ \hfill (\rul{ByCases} applied to 3,4)
\end{enumerate}

\paragraph{Reverse Double Negation.} %If we have $\Sigma\vdash\neg\neg F$, then $\Sigma\vdash F$. That is,
\begin{prooftree}
	\LeftLabel{\rul{RevDoubleNeg}}
	\AxiomC{$\Sigma\vdash \neg\neg F$}
	\UnaryInfC{$\Sigma\vdash F$}
\end{prooftree}
This can be proved as
\begin{enumerate}
	\item $\Sigma\vdash \neg\neg F$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{\neg F\}\vdash \neg\neg F$ \hfill (\rul{Monotonic} applied to 1)
	\item $\Sigma\cup\{\neg F\}\vdash \neg F$ \hfill (\rul{Assumption})
	\item $\Sigma\cup\{\neg F\}\vdash\neg F\land\neg\neg F$ \hfill (\rul{$\land$-Intro} applied to 2,3)
	\item $\Sigma\cup\{\neg F\}\vdash\neg F$ \hfill (\rul{$\land$-Elim} applied to 4)
	\item $\Sigma\cup\{\neg F\}\vdash\neg\neg F\land\neg F$ \hfill (\rul{$\land$-Symm} applied to 5)
	\item $\Sigma\cup\{\neg F\}\vdash F$ \hfill (\rul{Contra} applied to 6)
	\item $\Sigma\cup\{F\}\vdash F$ \hfill (\rul{Assumption})
	\item $\Sigma\vdash F$ \hfill (\rul{ByCases} applied to 7,8)
\end{enumerate}

\paragraph{Resolution.} %If $\Sigma\vdash\neg F\lor G$ and $\Sigma\vdash F\lor H$, then we can derive $\Sigma\vdash G\lor H$. That is,
\begin{prooftree}
	\LeftLabel{\rul{Resolution}}
	\AxiomC{$\Sigma\vdash \neg F\lor G$}
	\AxiomC{$\Sigma\vdash F\lor H$}
	\BinaryInfC{$\Sigma\vdash G\lor H$}
\end{prooftree}
This is proved as
\begin{enumerate}
	\item $\Sigma\vdash \neg F\lor G$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{F\}\vdash \neg F\lor G$ \hfill (\rul{Monotonic} applied to 1)
	\item $\Sigma\cup\{F\}\vdash F$ \hfill (\rul{Assumption})
	\item $\Sigma\cup\{F\}\vdash G$ \hfill (\rul{$\lor$-ModusPonens} applied to 2,3)
	\item $\Sigma\cup\{F\}\vdash G\lor H$ \hfill (\rul{$\lor$-Intro} applied to 4)
	\item $\Sigma\vdash F\lor H$ \hfill (\rul{Premise})
	\item $\Sigma\cup\{\neg F\}\vdash F\lor H$ \hfill (\rul{Monotonic} applied to 6)
	\item $\Sigma\cup\{\neg F\}\vdash \neg F$ \hfill (\rul{Assumption})
	\item $\Sigma\cup\{F\}\vdash H$ \hfill (\rul{$\lor$-ModusPonens} applied to 7,8)
	\item $\Sigma\cup\{\neg F\}\vdash H\lor G$ \hfill (\rul{$\lor$-Intro} applied to 9)
	\item $\Sigma\cup\{\neg F\}\vdash G\lor H$ \hfill (\rul{$\lor$-Symm} applied to 10)
	\item $\Sigma\vdash G\lor H$ \hfill (\rul{ByCases} applied to 5,11)
\end{enumerate}

\subsubsection{Substitutions}

 Let $F_1(p)$ and $F_2(p)$ be formulas. If $\Sigma\vdash (F_1(G)\siff F_1(H))$, $\Sigma\vdash(F_2(G)\siff F_2(H))$, and $\Sigma\vdash F_1(G)\land F_2(G)$, then $\Sigma\vdash F_1(H)\land F_2(H).$\\
 The proof of the above is

\begin{enumerate}
	\item $\Sigma\vdash (F_1(G)\siff F_1(H))$ \hfill (\rul{Premise})
	\item $\Sigma\vdash (F_2(G)\siff F_2(H))$ \hfill (\rul{Premise})
	\item $\Sigma\vdash (F_1(G)\land F_2(G))$ \hfill (\rul{Premise})
	\item $\Sigma\vdash F_1(G)$ \hfill (\rul{$\land$-Elim} applied to 3)
	\item $\Sigma\vdash (F_1(G)\imp F_1(H))$ \hfill (\rul{$\siff$-Def} applied to 1)
	\item $\Sigma\vdash F_1(H)$ \hfill (\rul{$\imp$-Elim} applied to 4,5)
	\item $\Sigma\vdash F_2(G)\land F_1(G)$ \hfill (\rul{$\land$-Symm} applied to 3)
	\item $\Sigma\vdash F_2(G)$ \hfill (\rul{$\land$-Elim} applied to 7)
	\item $\Sigma\vdash (F_2(G)\imp F_2(H))$ \hfill (\rul{$\siff$-Def} applied to 2)
	\item $\Sigma\vdash F_2(H)$ \hfill (\rul{$\imp$-Elim} applied to 8,9)
	\item $\Sigma\vdash F_1(H)\land F_2(H)$ \hfill (\rul{$\land$-Intro} applied to 6,10)
\end{enumerate}

Let $F(p)$ be a formula. If $\Sigma\vdash (G\siff H)$ and $\Sigma\vdash F(G)$, then $\Sigma\vdash F(H)$.\\
This is easily shown by using arguments similar to the one above for each connective and then using induction.\\

However, we do not introduce substitution as a rule since it causes a lot of overhead for the proof-checker. We want the time cost of the proof-checker to be low, and allowing substitutions may not allow that to happen. So for example, an argument such as
\begin{enumerate}
	\item $\Sigma\vdash \neg(\neg\neg F\lor G)$ \hfill (\rul{Premise})
	\item $\Sigma\vdash \neg(F\lor G)$ \hfill (\rul{DoubleNeg} applied to $\neg\neg F$ in 1)
\end{enumerate}
is disallowed for our purposes.\\

So far, we have seen several rules of reasoning. But how would one determine if these rules are sufficient to derive all true statements? Further, this feels quite inconvenient since there are so many rules. Often, we also don't know where to start applying them to a particular problem. We aim to simplify and algorithmize this process.


\subsection{Lecture 6}

If we want to develop algorithms, we require more structure in our input. We need to figure out methods for simplification and suitable ``normal forms''.

In order to prove theorems, we need to get used to ``structural induction''. That is,

\begin{theorem}[Structural Induction]
	Every formula in $P$ has property $Q$ if
	\begin{itemize}
		\item every atomic formula has $Q$ and
		\item if $F,G\in P$ have $Q$ then so do $\neg F$ and $(F\circ G)$, where $\circ$ is any binary symbol.
	\end{itemize}
\end{theorem}

Substitution is an important part of logic. We should be able to substitute equivalent subformulas without altering the truth values of formulas.\\

\begin{lemma}
\label{lem: 6.2}
	Suppose we have formulas $F(p)$, $G$, and $H$. Suppose for some model $m$, $m\vDash G\siff m\vDash H$. Then $m\vDash F(G) \siff m\vDash F(H)$.
\end{lemma}
This can be shown using structural induction, working it out for each connective.\\
We shall use the above ``substitution theorem'' quite a lot in proofs.

\begin{lemma}
	If $F(p)\equiv G(p)$, then for each formula $H$, $F(H)\equiv G(H)$.
\end{lemma}
We may assume without loss of generality that $p$ does not appear in $H$ (Why?). Consider a model $m$. Then define the model $m'$ by
\[
	m'\coloneqq
	\begin{cases}
		m[p\mapsto 1], & m\vDash H, \\
		m[p\mapsto 0], & m\nvDash H.
	\end{cases}
\]
Observe that by construction, $m'\vDash p$ iff $m'\vDash H$.\\
Further note that since $p$ does not appear in $F(H)$, $m\vDash F(H)$ iff $m'\vDash F(H)$. By the earlier lemma, $m'\vDash F(H)$ iff $m'\vDash F(p)$. Since $F(p)\equiv G(p)$, $m'\vDash F(p)$ iff $m'\vDash G(p)$.\\
Again going backwards like we did now, we see that $m'\vDash G(p)$ iff $m'\vDash G(H)$ iff $m\vDash G(H)$. Therefore, $m\vDash F(H)$ iff $m\vDash G(H)$.

\begin{theorem}
	Let $G$, $H$, and $F(p)$ be formulas. If $G\equiv H$, then $F(G)\equiv F(H)$.
\end{theorem}
This is just a reformulation of \Cref{lem: 6.2}.\\
This allows us to use known equivalences to modify formulas. This was also proved in the previous lecture using the proof system.

\subsection{Lecture 7}

\subsubsection{Negation Normal Form}

Observe the following equivalences that remove $\xor$, $\imp$, and $\siff$ from formulas.

\begin{align*}
	(p\imp q) &\equiv (\neg p\lor q) \\
	(p\xor q) &\equiv (p\lor q) \land (\neg p\lor \neg q) \\
	(p\siff q) &\equiv \neg (p\xor q)
\end{align*}

Removing either $\xor$ or $\siff$ results in an explosion in the formula size, but we assume that we can remove them on will.\\
Motivated by this, we define

\begin{definition}
	A formula is in \textit{negation normal form} (NNF) if $\neg$ appears only in front of the propositional variables.
\end{definition}

For any formula $F$, there is a formula $F'$ in NNF such that $F\equiv F'$.\\
We can do stuff like $\neg(p\lor q) = (p\land \neg q)$ for every logical connective and push $\neg$ under them.\\

There are also $\neg$s hidden inside $\xor$, $\imp$, and $\siff$. It is advisable to also remove these when producing the NNF of a formula.

\subsubsection{Conjunctive Normal Form}

Propositional variables are also referred to as \textit{atoms} (recall ``atomic formulas''). A \textit{literal} is either an atom or its negation. A \textit{clause} is a disjunction of literals.

Since $\lor$ is associative, commutative, and absorbs multiple occurrences, a clause can be referred to as a set of literals.

\begin{definition}
	A formula is in \textit{conjunctive normal form} (CNF) if it is a conjunction of clauses.
\end{definition}

Since $\land$ is associative, commutative, and absorbs multiple occurrences, a CNF formula can be referred to as a set of clauses.

For any formula $F$, there is a formula $F'$ in NNF such that $F\equiv F'$.\\
First, we remove $\xor$, $\imp$, and $\siff$ using the usual equivalences. We then convert it to NNF. We then ``flatten'' $\lor$ and $\land$ using the associativity of $\lor$ and $\land$. The parse tree of the formula then has $\lor$ and $\land$ at alternating levels with the leaves being literals. We then distribute $\lor$ over $\land$ while flattening at each step to obtain a formula in CNF.\\
But does this complete the argument? No, we also need to show that the final part of the algorithm (distribution) terminates (pushing using distributivity increases the size of the formula). To see why it should terminate, define $\nu(F)$ to be the maximum height of the $\lor$-$\land$ alternations in $F$. Suppose there is a subformula $G$ such that
\[ G = \bigvee_{i=0}^{m} \bigwedge_{j=0}^{n_i} G_{ij} \]
After pushing, we obtain
\[ G' = \bigwedge_{j_1=0}^{n_1} \cdots \bigwedge_{j_m=0}^{n_m} \bigvee_{i=0}^m G_{ij}. \]
Note that the height of the height of the above must be strictly less than $\nu(G)$.\\
Observe that $G'$ is either the top formula or the parent of this connective is $\land$. Also, each $G_{ij}$ is either a literal or a clause.\\
We must keep flattening to keep $F(G')$ in the considered form. Due to K\"{o}nig's lemma, this procedure must terminate.

\begin{lemma}[K\"{o}nig's Lemma]
	For an infinite connected graph $G$, if each node has finite degree, then there is an infinite simple path from each node.
\end{lemma}

% This implies the required since it says that if the parse tree is infinite at any step, then the height at that step must be infinite as well.\\

A \textit{unit clause} contains only one literal. A \textit{binary clause} contains exactly two literals. A \textit{ternary clause} contains exactly three literals.\\
We can also extend this to the empty set of literals. We take by convention that $\bot$ is the empty clause.

\subsubsection{Tseitin's Encoding}

CNF is desirable because it has a very single structure, and only involves three distinct connectives ($\lor$, $\land$, and $\neg$).\\ The transformation using distributivity is bad because it leads to an explosion in the size of the formula. How do we avoid this?

By introducing fresh variables, Tseitin's encoding translates any formula into an equisatisfiable formula in CNF without an exponential explosion.

\begin{enumerate}
	\item First, assume the input formula $F$ is in NNF without $\xor$, $\imp$, and $\siff$.
	\item Find a $G_1\land\cdots\land G_n$ in $F$ that is just below an $\lor$. If no such subformula exists, terminate.
	\item Replace $F(G_1\land\cdots\land G_n)$ by $F(p)\land(\neg p\lor G_1)\land\cdots\land(\neg p\lor G_n)$, where $p$ is a newly introduced variable.
	\item Go to step 2.
\end{enumerate}

For example, $(p_1\land\cdots\land p_n)\lor(q_n\land\cdots\land q_m)$ becomes
\[ (x\lor y) \land \bigwedge_{1\leq i\leq n} (\neg x\lor p_i) \land \bigwedge_{1\leq j\leq m} (\neg y \lor q_j), \]
which has only $m+n+1$ clauses, as opposed to the $mn$ clauses returned by the na\"{i}ve algorithm. Here, $x$ and $y$ are the newly introduced variables.\\

Why does this preserve satisfiability?\\
Suppose $m\vDash F(p)\land(\neg p\lor G_1)\land\cdots\land(\neg p\lor G_n)$. We have three cases.
\begin{enumerate}[(i)]
	\item If $m\vDash p$, then $m\vDash G_i$ for every $1\leq i\leq n$. Therefore, $m\vDash G_1\land\cdots\land G_n$. We can then perform a substitution to get that $m\vDash F(G_1\land\cdots\land G_n)$.
	\item If $m\nvDash p$ and $m\nvDash G_1\land\cdots\land G_n$. Since $m\vDash F(p)$, we can directly apply the substitution theorem again.
	\item If $m\nvDash p$ and $m\vDash G_1\land\cdots\land G_n$. Since $F(G_1\land\cdots\land G_n)$ is in NNF, $p$ must occur positively (no $\neg p$) in $F(p)$. Therefore, $m[p\mapsto 1]\vDash F(p)$. Since $p$ does not occur in any $G_i$, we also obviously have $m[p\mapsto 1]\vDash G_1\land\cdots\land G_n$. Therefore, $m\vDash F(G_1\land\cdots\land G_n)$ (we basically ignore the modification).
\end{enumerate}

It is worth noting that going from a model that satisfies the encoding to one that satisfies the original formula is non-trivial (if it was, we might have $P=NP$ - Why?).

\subsubsection{Disjunctive Normal Form}

Similar to CNF, we define DNF as:

\begin{definition}
	A formula is said to be in Disjunctive Normal Form (DNF) if it is a disjunction of conjunctions of literals.
\end{definition}

\subsection{Lecture 8}

\subsubsection{\texorpdfstring{$k$}{k}-sat}

\begin{definition}
	A $k$-sat formula is a CNF formula with at most $k$ literals in each clause.
\end{definition}

For example, $(p\wedge q\wedge \neg r)$ is $1$-sat.

\begin{theorem}
	For any $k$-sat formula $F$, there is a $3$-sat formula $F'$ with linear blow up such that $F$ and $F'$ are equisatisfiable.
\end{theorem}
\begin{proof}
	Let $F$ be $k$-sat with $k\geq 4$. Let $G=(\ell_1\lor\cdots\ell_k)$ be a clause, where each $\ell_i$ is a literal.\\
	Let $x_1,\ldots,x_{k-2}$ be variables that don't appear in $F$ and let $G'$ be the set of clauses
	\[ (\ell_1\lor\ell_2\lor x_2) \land \bigwedge_{2\leq i\leq k-3} (\neg x_i \lor x_{i+1} \lor \ell_{i+1}) \land (\neg x_{k-2} \lor \ell_{k-2} \lor \ell_k). \]
	We claim that $G$ and $G'$ are equisatisfiable. Let $m\vDash G'$. Assume instead that $m(\ell_i)=0$ for all $i$. We have $m(x_2) = 1$, and for each $2\leq i\leq k-3$, $m(x_{i+1}) = 1$ (by induction). This implies that $m(x_{i+2}) = 1$, which results in a contradiction in the final clause. Therefore, $m\vDash G$.\\
	For the other direction, let $m(\ell_i)=1$. Then the model
	\[ m' = \{x_2\mapsto 1, \ldots,x_{i-1}\mapsto 1,x_i\mapsto 0,\ldots,x_{k-2}\mapsto 0\} \]
	satisfies $G$.
\end{proof}

Observe that $G'$ contains $3(k-2)$ literals, so there is a linear (up to $3$ times) blowup.\\
This can be enhanced to just $\log k$ new literals, by splitting the clause in half and introducing a new variable at each step. That is, write $(\ell_1\lor\cdots\lor\ell_k)$ as
\[ (x_1\lor\ell_1\lor\cdots\lor\ell_{\frac{k}{2}})\land(\neg x_1\lor\ell_{1+\frac{k}{2}}\lor\cdots\lor\ell_{k}). \]

\subsubsection{\texorpdfstring{$2$}{2}-sat}

The \textit{implication graph} $(V,E)$ of a $2$-sat formula $F$ with $\Vars(F)=\{p_1,\ldots,p_n\}$ is defined by
\[ V = \{p_1,\ldots,p_n,\neg p_1,\ldots,\neg p_n\}\text{ and }E=\{(\overline{\ell}_1,\ell_2),(\overline{\ell}_2,\ell_1) : (\ell_1\lor\ell_2\in F)\}, \]
where $\overline{p} = \neg p$ and $\overline{\neg p} = p$.\\

Observe that if there is a path from $\ell_1$ to $\ell_2$, there is a path from $\overline{\ell}_2$ to $\overline{\ell}_1$.\\
Suppose that for every strongly connected component (scc) $S\subseteq V$ in $(V,E)$, there is a scc $S^c$, known as the \textit{complementary component}, with exactly the negations of the literals in $S$.

For each $m\vDash F$, if there is a path from $\ell_1$ to $\ell_2$, then if $m(\ell_1)=1$, then $m(\ell_2)=1$. As a consequence, in a strongly connected component, either all $m(\ell)$ are $1$ or all are $0$.\\

The \textit{reduced implication DAG} $(V^R,E^R)$ of an implication graph $(V,E)$ is defined by
\[ V^R = \{S : S\text{ is a scc in }(V,E)\}\text{ and }E^R = \{(S,S') : \exists \ell\in S,\ell'\in S' \text{ such that }(\ell,\ell')\in E \}. \]

\begin{theorem}
	A $2$-sat formula $F$ is unsat iff there is a scc $S$ in the implication graph such that $\{p,\neg p\}\subseteq S$ for some $p$.
\end{theorem}

	Suppose there is no such $S$. Define a model as follows. Assign all the literals of an unassigned scc $1$ if all its children (the vertices it has an edge to in the reduced implication DAG) are assigned $1$. Consequently, assign its complement $0$ (this uses the fact that there is no such $S$). One of the directions of the result follows. There is no violation at any point -- this can easily be proved by induction (or the well-ordering principle).\\
	For the other direction, if there is a path from $p$ to $\neg p$, we get a contradiction quite easily, resulting in unsatisfiability.\\

It is seen from this theorem that a $2$-sat formula can be solved in linear time.

\subsubsection{XOR-sat and Horn Clauses}

A formula is \textit{XOR-sat} if it is a conjunction of xors of literals. Since xors are negations of equalities, we can eliminate via substitution.

For a variable $p$ and formulas $F,G$, $(p\xor G)\land F$ and $F[\neg G/p]$ are equisatisfiable. This allows us to do substitution repeatedly after determine satisfiability in polynomial time.

A \textit{Horn clause} is a cluase of the form $\neg p_1\lor \cdots\lor\neg p_n \lor q$, where $p_1,\ldots,p_n\in\Vars$ (where $n\geq 0$) and $q\in\Vars\cup\{\bot\}$. A \textit{Horn formula} is a conjunction of Horn clauses. The clauses with $\bot$ literals are called \textit{goal clauses} and the others are called \textit{implication clauses}. Note that an horn clause is equivalent to
\[ p_1 \land \cdots \land p_n \implies p. \]
Note that the trivial model that assigns $0$ to everything need not always work because $n$ can be $0$.\\
Start with a model all $0$. Pick an implication clause not satisfied yet. Set the right to true. If it causes a problem at some goal clause, then return unsat.

\subsection{Lecture 9}

\subsubsection{Resolution Proof Systems}

For a clause $C$ and a literal $\ell$, $\ell\cup C$ denotes $\{\ell\}\cup C$.\\
% Observe that the conjunction of CNF formulas is also a CNF formula.\\
If we start with a set of CNF formulas, how many rules do we need? It turns out we only need two - \textsf{Assumption} and \textsf{Resolution}.\\
When writing rules, we usually omit $\Sigma$ if it is clear from context. So for example, we can apply \textsf{Assumption} as
\begin{prooftree}
	\AxiomC{$p\lor C$}
	\AxiomC{$\neg p\lor C$}
	\BinaryInfC{$C\lor D$}
\end{prooftree}
The two on the top are called \textit{antecedent} and the bottom is called the \textit{resolvent}. $p$ is called the \textit{pivot} here.\\

It is possible in some cases that we have multiple choices for the pivot. For example, consider
\begin{prooftree}
	\AxiomC{$p\lor q\lor r$}
	\AxiomC{$\neg p\lor \neg q\lor r$}
	\BinaryInfC{$q\lor\neg q\lor r$}
\end{prooftree}
and
\begin{prooftree}
	\AxiomC{$p\lor q\lor r$}
	\AxiomC{$\neg p\lor \neg q\lor r$}
	\BinaryInfC{$p\lor\neg p\lor r$}
\end{prooftree}
In one case, we take $p$ as the pivot and in the other, we take $q$. However, if we have a choice in the pivot, then it will be like the above, wherein both $p$ and $\neg p$ are in the resolvent. Therefore, any non-trivial resolution will have a unique pivot.

The resolution proof method takes a set of clauses $\Sigma$ and produces a forest of clauses as a proof. The goal of the proof method is to find the empty clause (which signifies an inconsistency).\\

For example, consider the context of the derivation to be $\Sigma=\{p\lor q,\neg p\lor q,\neg q\lor r,\neg r\}$. Then
\begin{prooftree}
	\AxiomC{$p\lor q$}
	\AxiomC{$\neg p\lor q$}
	\BinaryInfC{$q$}
	\AxiomC{$\neg q\lor r$}
	\BinaryInfC{$r$}
	\AxiomC{$\neg r$}
	\BinaryInfC{$\bot$}
\end{prooftree}

The lower we go, the more the \textit{depth} increases.\\
Recall that formal proof systems do not explicitly refer to $\bot$, they encode $\bot$ by $F\land\neg F$ for some formula $F$. Even in the above system for example, we actually get $\Sigma\vdash\neg r$ and $\Sigma\vdash r$.\\
Therefore, if a resolution proof system can derive $\Sigma\vdash\bot$, $\Sigma$ is unsatisfiable (due to the soundness of our system).

Say we want to prove $\Sigma\vdash F$. We convert $\neg F\wedge\bigwedge\Sigma$ into a new set of clauses $\Sigma'$ and apply the resolution proof method on this instead. If we derive $\bot$, $\Sigma\vdash F$ is derivable.

\subsubsection{Optimizations}

How do we find resolution proofs? There are some issues in implementation.\\
A proof method implicitly defines a non-deterministic proof search algorithm. To implement such an algorithm, we should ensure that we are not doing unnecessary work. For now, we only worry about a single rule.\\

\begin{enumerate}
	\item For clauses $C$ and $D$, if $D\subseteq C$ and $\bot$ can be derived using $C$, then it can be derived using $D$.\\
	If $D\subseteq C$, $C$ is said to be \textit{redundant}. For example, $\neg q\lor r$ is redundant in $\{q,\neg q\lor r, r, \neg r\}$ (since $r\subseteq \neg q\lor r$). Shorter clauses help us get to the empty clause faster.

	\item If a clause contains both $p$ and $\neg p$, it is valid.\\
	If a valid clause contributes in deriving $\bot$, the descendants must participate in some resolution step with pivot $p$. This step is counterproductive (the resolvent is a superset of some antecedent). For example,
	\begin{prooftree}
		\AxiomC{$p\lor C$}
		\AxiomC{$\neg p\lor p\lor D$}
		\BinaryInfC{$p\lor C\lor D$}
	\end{prooftree}
	
	\item If a literal occurs in a CNF formula and its negation does not, it is said to be a \textit{pure literal}. The removal of clauses containing pure literals in a CNF preserves satisfiability.

	\item If $\ell$ occurs in a resolution proof, $\neg\ell$ can be removed from every clause.
\end{enumerate}

\subsection{Exercises}

\begin{exercise}
	Suppose we have $n$ variables in a $2$-sat problem. What is the maximal number of clauses in the formula such that the formula is satisfiable?
\end{exercise}
\begin{solution*}
	Observe that it is possible to have $3n^2$ clauses. Indeed, consider the set of clauses
	\[ \{\{p,\neg q\} : p,q\in\Vars\}\cup\{\{p,q\} : p,q\in\Vars\} \]
	which is satisfiable by the model that takes $1$ everywhere.\\
	We claim that this is the maximal number of clauses. Indeed, let us have an implication graph with $2k\geq 2$ strongly connected components. Also, we can use the notion of complementary components and let the sccs have $v_1,\ldots,v_k$ vertices (where there are two sccs corresponding to each $v_i$).\\
	The number of edges is then at most
	\[ 2\sum_{1\leq i\leq k} v_i^2 + \sum_{1\leq i\leq k} v_i^2 + \sum_{1\leq i<j\leq k} 4 v_i v_j. \]
	The first term arises due to complete connectedness within each scc and the second and third terms arise by taking into consideration the maximal number of edges between sccs. The above is equal to
	\[ 3\left(\sum_{1\leq i\leq k} v_i\right)^2 - 2\sum_{1\leq i<j\leq k} v_i v_j = 3n^2 - 2\sum_{1\leq i<j\leq k} v_i v_j \leq 3n^2, \]
	thus proving the claim.
\end{solution*}

\subsection{Lecture 10}

\subsubsection{Completeness of the resolution proof system}

If $\Sigma$ is unsatisfiable, are we guaranteed to derive $\Sigma\vdash\bot$ via resolution?\\

We define the set $\Res^n(\Sigma)$ of clauses that are derivable via resolution proofs of depth $n$ from the set of clauses $\Sigma$. So $\Res^0(\Sigma)\coloneqq\Sigma$ and
\[ \Res^{n+1}(\Sigma)\coloneqq\Res^n(\Sigma)\cup\{C : C\text{ is a resolvent of clauses }C_1,C_2\in\Res^n(\Sigma)\} \]

For example,
\[ \Res^1(\{p\lor q, \neg p\lor q, \neg q\lor r, \neg r\}) = \Sigma\cup\{q, p\lor r, \neg p\lor r,\neg q\}. \]

Since there are only finitely many variables in $\Sigma$, we can only derive finitely many clauses.

\begin{definition}
	Let $\Sigma$ be a finite set of clauses. There exists an $m$ such that
	\[ \Res^{m+1}(\Sigma) = \Res^m(\Sigma). \]
	We define $\Res^*(\Sigma)\coloneqq\Res^m(\Sigma)$.
\end{definition}

\begin{theorem}
	If a finite set of clauses $\Sigma$ is unsatisfiable, $\bot\in\Res^*(\Sigma)$.
\end{theorem}
\begin{proof}
	We prove this by induction over the cardinality of $\Sigma$. We assume there are no tautology classes in $\Sigma$.\\
	Let $\Sigma=\{p\}$. If it is unsat, then $\{p,\neg p\}\subseteq\Sigma$. The derivation to get $\bot$ is obvious.\\
	Suppose it holds for all formulas containing variables $p_1,\ldots,p_n$. Let $\Sigma$ be an unsat set of clauses with variables $p_1,\ldots,p_n,p$. Let $\Sigma_0$ and $\Sigma_1$ be the set of clauses from $\Sigma$ that have $p$ and $\neg p$ respectively. Let $\Sigma_*$ be the remaining clauses. Clearly, $\Sigma=\Sigma_0\land\Sigma_1\land\Sigma_*$. Now, define $\Sigma_0'=\{C\setminus\{p\} : C\in\Sigma_0\}$. Similarly define $\Sigma_1'$ by removing $\neg p$.\\
	It may be shown that $\Sigma_0'\vDash\Sigma_0$ and $\Sigma_1'\vDash\Sigma_1$.\\

	Consider the formula $(\Sigma_0'\land\Sigma_*)\lor(\Sigma_1'\land\Sigma_*)$. Clearly, $p$ is not in this formula.\\
	We claim that if this formula is sat, so is $\Sigma$. Let $m$ satisfy this formula. Then $m\vDash\Sigma_*$. If $m\vDash(\Sigma_1'\land\Sigma_*)$, then since all the clauses of $\Sigma_0$ have $p$, $m[p\mapsto 1]\vDash\Sigma_0$ and since $\Sigma_1'$ and $\Sigma_*$ don't have $p$, $m[p\mapsto 1]\vDash\Sigma_1'$ and $m[p\mapsto 1]\vDash\Sigma_*$. Since $\Sigma_1'\vDash\Sigma_1$, $m[p\mapsto 1]\vDash\Sigma_1$. Therefore, $m[\mapsto 1]\vDash (\Sigma_0\land\Sigma_1\land\Sigma_*)$. Similarly, if $m\vDash(\Sigma_0'\land\Sigma_*)$, then $m[p\mapsto 0]\vDash(\Sigma_0\land\Sigma_1\land\Sigma_*)$.\\
	Now, since $\Sigma$ is unsat, so is $(\Sigma_0'\land\Sigma_*)\lor(\Sigma_1'\land\Sigma_*)$.\\

	Since they don't have $p$, $\bot\in\Res^*(\Sigma_0'\land\Sigma_*)$ and $\bot\in\Res^*(\Sigma_1'\land\Sigma_*)$. Choose a derivation of $\bot$ from both.\\
	If $\bot$ was derived using only clauses from $\Sigma_*$ from either proof. Then we are done.\\
	If $\Sigma_0'$ and $\Sigma_1'$ are involved in each of the derivations, then $p\in\Res^*(\Sigma_0\land\Sigma_*)$ and $\neg p\in\Res^*(\Sigma_1\land\Sigma_*)$ (``add'' $p$ and $\neg p$ to each of the clauses that involve $\Sigma_0'$ and $\Sigma_1'$ respectively). Again, the required follows, thus proving the theorem.
\end{proof}

\subsubsection{Compactness and Implication}

Let $\Sigma$ be a finite set of formulas and $F$ be a formula. The following are equivalent.
\begin{enumerate}[(i)]
	\item $\Sigma\vdash F$.
	\item $\emptyset\in\Res^*(\Sigma')$, where $\Sigma'$ is the CNF of $\bigwedge\Sigma\land\neg F$.
	\item $\Sigma\vDash F$.
\end{enumerate}

(i) to (iii) says that proof rules are sound. (ii) to (i) says that a proof can be generated. (iii) to (ii) is what we showed in the previous theorem.\\

But what do we do in the case where $\Sigma$ is infinite?

Suppose $\Sigma''\vDash F$ and $\Sigma''$ is infinite. We shall show that this implies that this is truee iff there is some finite $\Sigma\subseteq\Sigma''$ such that $\Sigma\vDash F$.\\
% If $\Sigma''\cup\{\neg F\}$ is sat, then every finite subset of $$

\begin{theorem}
	Let $S$ be an infinite set of finite binary string. There exists an infinite string $w$ such that
	\[ |\{w'\in S : w_n \text{ is a prefix of }w'\}|=\infty \]
	for all $n$ where $w_n$ is a prefix of $w$ of length $n$.
\end{theorem}
\begin{proof}
	We inductively consutrct $w$ and keep shrinking $S$. Initially, $w=\epsilon$. Let $S_0=\{u\in S : u\text{ starts with }0\}$, $S_1 = \{u\in S : u \text{ starts with }1\}$, and $S_\epsilon = S \cap \{w\}$. Clearly, $S=S_\epsilon\cup S_0\cup S_1$. At least one of $S_0$ and $S_1$ is infinite. Suppose $S_0$ is infinite. Then ``expand'' $w$ to $0$ and restrict to $S=S_0$.\\
	Suppose we have $w$ of length $n$ and $w$ is a prefix of all string in $S$. Again, let $S_0=\{u\in S : u\text{ has }0\text{ at the $n$th position}\}$, $S_1 = \{u\in S : u \text{ has }1\text{ at the $n$th position}\}$, and $S_\epsilon = S\cap\{w\}$. If $S_0$ is infinite, expand $w$ to $w0$ and $S=S_0$. Otherwise, $w=w1$ and $S=S_1$.\\
	The result follows.
\end{proof}

\begin{theorem}
	A set $\Sigma$ of formulas is satisfiable iff every finite subset of $\Sigma$ is satisfiable.
\end{theorem}

\begin{proof}
	The forward direction is trivial.\\
	Order the formulas of $\Sigma$ as $\Sigma=\{F_1,F_2,\ldots\}$. Let $\{p_1,p_2,\ldots\}$ be an ordered list of variables from $\Vars(\Sigma)$ such that we first write formulas in $\Vars(F_1)$, then $\Vars(F_2)\setminus\Vars(F_1)$, and so on.\\
	We have models $m_n$ such that $m_n\vDash\bigwedge_{i=1}^n F_i$.\\
	We assume $m_n : \Vars\left(\bigwedge_{i=1}^n F_i\right)\to\B$. We can think of the $(m_n)$ as finite binary strings since the variables are ordered $p_1,p_2,\ldots$ and $m_n$ assigns values to the first $k$ variables as a string. let $S=\{m_n\text{ as a string}:n>0\}$. Let $m$ be the infinite string as mentioned in the previous theorem.\\
	If we then interpret $m$ as a model, then $m\vDash\Sigma$. Consider a formula $F_n\in\Sigma$ and suppose there are $k$ variables up to $F_n$. Let $m'$ be the prefix of length $k$ of $m$. There must be $m_j\in S$ such that $m'$ is a prefix of $m_j$ and $j>n$. Since $m_j$ satisfies the conjunction of the $n$ formulas, $m_j\vDash F_n$. Therefore, $m'\vDash F_n$ and $m\vDash F_n$. 
\end{proof}

If we can enumerate a set using some algorithm, it is said to be \textit{effectively enumerable}.\\

A yes/no problem is \textit{semi-decidable} if we have an algorithm for only one side of the problem is decidable (either yes or no).

\begin{theorem}
	If $\Sigma$ is effectively enumerable, $\Sigma\vDash F$ is semi-decidable.
\end{theorem}
\begin{proof}
	If $\Sigma\vDash F$, there is a finite set $\Sigma_0\subseteq\Sigma$ such that $\Sigma_0\vDash F$. Since $\Sigma$ is effectively enumerable, let $G_1,G_2,\ldots$ be an enumeration of it. Let $S_n = \{G_1,\ldots,G_n\}$.\\
	There is then some $k$ such that $S_k\vDash F$. We may enumerate $S_n$ and check if $S_n\vDash F$ (which is decidable by virtue of truth tables). Therefore, if $\Sigma\vDash F$, we will eventually say yes. So it is decidable if the answer is yes, but not if no.
\end{proof}

\subsection{Lecture 11}

\subsubsection{DPLL (Davis-Putnam-Loveland-Logemann) Method}



