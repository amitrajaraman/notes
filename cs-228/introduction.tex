\section{Introduction to Logic}

\subsection{Lecture 1}

For any computer scientist, logic is an extremely basic tool. Consider the statement
\begin{quote}
    This sentence is false.
\end{quote}
A little bit of thought shows that the above sentence has no definite truth value -- it is \textit{paradoxical}. Indeed, it is often known as the ``liar's paradox". This sort of self-referential sentence will come back to haunt us many more times in the future.\\

Propositional logic (or \textit{zeroth-order logic}) is basically the form of logic that deals with propositions which can be true or false as well as relations between them.\\
A more useful tool is that of \textit{first-order logic}, that also deals with non-logical objects, predicates about them, and quantifiers ($\forall$ and $\exists$). That is, we are allowed to quantify over elements of the set, but not something like subsets of the set. A lot of mathematical statements cannot be written when we are restricted to first-order logic.\\

We study theories with basic assumptions or \textit{axioms}. Using these axioms, we aim to prove more non-trivial results within the theory. A natural question to ask is: is it possible to have some set of axioms that allow us to concretely determine the truth value of any consequential statement? Once more, the self-referential statement returns.

\begin{theorem}[G\"odel's Incompleteness Theorem]
There are theories whose assumptions cannot be listed.
\end{theorem}
\begin{proof}
Suppose that there exists such a list for the number theory. Consider the \textbf{true} statement
\begin{quote}
    This sentence cannot be proven by the list.
\end{quote}
The list cannot imply the sentence. This yields a contradiction.
\end{proof}

This theorem shows that logic has all but failed as a tool to do math. From this failure, rose computer science. This is the basic spirit of the proof and might not be completely understood. We discuss it more concretely later.

\subsection{Lecture 2}

\subsubsection{Propositional Logic: Syntax and Parsing}

We need an efficient method to identify if some group of symbols is a logical argument. We usually define a syntax for this (for example, grammar in English).\\

The logic we consider is over some list of propositions. We give each proposition a symbol. So say there is some set $\mathsf{Vars}$ of \textit{countably many} propositional variables.
% Say
% \[ \mathsf{Vars} = \{p_1,p_2,\ldots\}. \]
These propositional variables are also called \textit{Boolean variables}.\\
Propositions are connected by \textit{logical arguments}. How could we connect propositions?
\begin{itemize}
    \item A statement that is always true/false.
    \item Negation. A statement that is the negation of another.
    \item Conjunction. Two statements being true simultaneously.
    \item Disjunction. At least one of two statements being true.
    \item Implication. If a statement is true, then some other statement is true as well.
    \item Equivalence. Two statements always have the same truth value.
    \item Disequality or exclusive or. Two statements always have different truth values.
\end{itemize}

\begin{center}
\begin{tabular}{c|c|c}
     true & $\top$ & top  \\
     false & $\perp$ & bot \\
     negation & $\neg$ & not \\
     conjunction & $\wedge$ & and \\
     disjunction & $\vee$ & or \\
     implication & $\implies$ & implies \\
     equivalence & $\iff$ & iff \\
     exclusive or & $\oplus$ & xor \\
     opening parenthesis & ( & \\
     closing parenthesis & ) &
\end{tabular}
\end{center}

We assume that the above \textit{logical connectives} are not in $\mathsf{Vars}$.\\
A \textit{propositional formula} is a finite string containing symbols in $\mathsf{Vars}$ and logical connectives.

\begin{definition}
The set of propositional formulas is the smallest set $P$ such that
\begin{itemize}
    \item $\top,\perp\in P$,
    \item $\mathsf{Vars}\subseteq P$,
    \item if $f\in P$, then $\neg f\in P$, and
    \item if $\circ$ is a binary symbol and $f,g\in P$, then $(f\circ g)\in P$.
\end{itemize}
\end{definition}

Alternatively, this can succinctly be written as ``$f \in P$ if
\[f \coloneqq p\mid\top\mid\perp\mid \neg f \mid (f\vee f) \mid (f\wedge f) \mid (f\implies f) \mid (f\iff f) \mid (f\oplus f) \]
where $p\in\mathsf{Vars}$."

\begin{definition}
$\top$, $\perp$, and any $p\in\mathsf{Vars}$ are known as \textit{atomic formulas}.
\end{definition}

\begin{definition}
For each $f\in P$, $\mathsf{Vars}(f)$ is the set of variables appearing in $f$.
\end{definition}

It is important to note that parentheses are needed (only) between binary operations. So as of now, $(\perp\implies\top)$ is a formula but $\perp\implies\top$ isn't.\\
Not all strings over $\mathsf{Vars}$ and logical connectives are in $P$.

\subsubsection{Examples Encoding Arguments into Logic}

Consider the following argument.
\begin{quote}
    If $c$ then if $s$ then $f$. not $f$. Therefore, if $s$ then not $c$.
\end{quote}
This can be written as
\[ (((c\implies (s\implies f)) \wedge \neg f) \implies (s \implies \neg c)). \]
Another example, say we know that good people always tell the truth and not good people always tell a lie. If there are two people $A$ and $B$ and $A$ says ``I am not good or $B$ is good", then what are $A$ and $B$?\\
Suppose the variables $p_A$ and $p_B$ denote whether $A$ and $B$ are truthful or not. Then the above is basically
\[ ((\neg p_A \vee p_B) \iff p_A). \]
How do we determine whether there is some $p_A,p_B$ that satisfies this?

\subsubsection{Parsing Formulas}

$F\in P$ iff it can be obtained by unfolding one of these generation rules.

\begin{definition}
A \textit{parse tree} of a formula $F\in P$ is a tree such that
\begin{itemize}
    \item the root is $F$,
    \item the leaves are atomic formulas, and
    \item each internal node is formed by applying some formulation rule on its children.
\end{itemize}
\end{definition}

We have the following 

\begin{theorem}
$F\in P$ iff there is a parse tree of $F$.
\end{theorem}
\begin{proof}
The reverse direction follows by definition.\\
How do we show that any $F\in P$ has a parse tree? In fact, it has a \textit{unique} parse tree.
\end{proof}

A parse tree is a directed acyclic graph (DAG). The parsing produces a parse DAG. This is done by not writing repeated symbols twice, ensuring that all arrows go from higher levels of the DAG to the lower ones.

\begin{definition}
A formula $G$ is a \textit{subformula} of a formula $F$ if $G$ occurs within $F$. Further, $G$ is a proper subformula of $F$ if $F\neq G$. Denote by $\mathsf{sub}(F)$ the set of subformulas of $F$.
\end{definition}

Observe that the nodes of the parse tree of $F$ form $\mathsf{sub}(F)$.

Immediate subformulas are the children of a formula in its parse tree. The corresponding \textit{leading connective} is the connective used to join the children. So for example,

\[ \mathsf{sub}((\neg p_2 \iff (p_1 \wedge p_3)) = \{((\neg p_2 \iff (p_1 \wedge p_3)), \neg p_2, (p_1 \wedge p_3), p_1, p_2, p_3\}. \]

\subsubsection{Shorthands}

The reader might have noticed by now that we need to write so many parentheses, which don't really feel necessary most of the time. If we use some sort of precedence order over logical connectives, we may be able to drop some parentheses without losing the unique parsing property.\\
For example, we may drop outermost parentheses without any confusion. An example of this is writing $((p \wedge q)\implies (r \vee p))$ as $(p \wedge q)\implies (r \vee p)$.\\
Further, in the above example, if we give $\vee$ and $\wedge$ higher precedence then $\implies$ during parentheses, then we can drop all the parentheses! The usual precedence order we use is
\[ \neg > \vee = \wedge = \oplus > \implies = \iff. \]
So how do we go about parsing a formula then? Suppose we have $F_0\circ_1 F_2\circ_2 \cdots\circ_n F_n$, where each $F_i$ is either atomic, enclosed by parentheses, or their negation. We transform it as follows.
\begin{itemize}
    \item Find a $\circ_i$ such that $\circ_{i-1}$ and $\circ_{i+1}$ have lower precedence (if they exist).
    \item Introduce parentheses around $F_{i-1}\circ F_i$ and call it $F_i'\coloneqq (F_{i-1}\circ_i F_i)$ so you now have
    \[ F_0 \circ_1 \cdots \circ_{i-2} F_{i-2} \circ_{i-1} F_i' \circ_{i+1} F_{i+1} \circ_{i+2} \cdots \circ_n F_n. \]
\end{itemize}
Repeat the above until only one expression remains ($n$ becomes $1$). We can then parse it normally. For example,
\[ p \wedge q \implies r \vee p \text{ to } (p\wedge q)\implies r\vee p \text{ to } (p\wedge q)\implies (r\vee p) \text{ to } ((p\wedge q)\implies (r\vee p)). \]
Some formulas cannot be unambiguously parsed, for example $p\vee q\wedge r$, $p \vee q \vee r$, or $p\implies q\implies r$. But can we salvage any of them?\\
Associativity preference may further reduce the need of parentheses. Let's make all our operators right associative (first group the rightmost occurrence). So for example, unless mentioned otherwise, we take $p\implies q\implies r$ as $(p\implies (q\implies r))$.

\begin{definition}
For $F\in P$ and $p_1,\ldots,p_k\in\Vars$, we denote by $F[G_1/p_1,\ldots,G_k/p_k]$ the formula obtained by \textit{simultaneously} replacing all occurrences of $p_i$ by the formula $G_i$ for each $i\in[k]$. 
\end{definition}

So for example,
\[ (p\implies (r\implies p))[(r\otimes p)/p] = ((r\otimes p)\implies (r\implies (r\otimes p))). \]
Sometimes, we may also write a formula $F$ as $F(p_1,\ldots,p_k)$. Then, by $F(G_1,\ldots,G_n)$, we mean $F[G_1/p_1,\ldots,G_k/p_k]$.

\subsection{Lecture 3}

\subsubsection{Semantics}

Semantics is giving meaning to formulas. We denote the set of truth values as $\mathcal{B}\coloneqq\{0,1\}$. We may view $0$ as ``false" and $1$ as ``true", but the only important thing is that they are distinct.

\begin{definition}[Model]
A \textit{model} is an function from $\Vars\to\mathcal{B}$.
\end{definition}

For example, $\{p_1\mapsto 1, p_2\mapsto 0, p_3\mapsto 0,\ldots\}$. Since $\Vars$ is countable, the set of models is non-empty and infinite.\\
A given model $m$ may or may not satisfy a formula $F$. This satisfaction relation is denoted by $m\vDash F$. Let us define this more concretely.

\begin{definition}
The \textit{satisfaction relation} $\vDash$ between models and formulas is the smallest relation that satisfies the following.
\begin{itemize}
    \item $m\vDash\top$,
    \item if $m(p)=1$, then $m\vDash p$,
    \item if $m\nvDash F$, then $m\vDash\neg F$,
    \item if $m\vDash F_1$ or $m\vDash F_2$, then $m\vDash F_1\vee F_2$,
    \item if $m\vDash F_1$ and $m\vDash F_2$, then $m\vDash F_1\wedge F_2$,
    \item if $m\vDash F_1$ and $m\vDash F_2$ but not both, then $m\vDash F_1\oplus F_2$,
    \item if if $m\vDash F_1$ then $m\vDash F_2$, then $m\vDash (F_1\implies F_2)$, and
    \item if $m\vDash F_1$ iff $m\vDash F_2$, then $m\vDash (F_1\iff F_2)$.
\end{itemize}
\end{definition}

Observe that $\perp$ is not explicitly mentioned in the above definition since it follows from it being the \textit{smallest} relation.\\

If $m\vDash F$, we say that $m$ \textit{satisfies} $F$.\\
$F$ is \textit{satisfiable} if there is a model $m$ such that $m\vDash F$. This is often abbreviated as \textit{sat}.\\
$F$ is \textit{valid} (written $\vDash F$) if for each model $m$, $m\vDash F$. A valid formula is also called a \textit{tautology}.\\
$F$ is \textit{unsatisfiable} (written $\nvDash F$) if there is no model $m$ such that $m\vDash F$. This is often abbreviated as \textit{unsat}.\\

We can check if a certain formula satisfies a model by moving bottom-up in the parse tree.\\

We overload the $\vDash$ operator in several natural ways.

\begin{definition}
Let $M$ be a set of models. We write $M\vDash F$ if for every $m\in M$, $m\vDash F$.
\end{definition}

\begin{definition}
Let $\Sigma$ be a set of formulas. We write $\Sigma\vDash F$ if for every $m$ that satisfies every formula in $\Sigma$, $m\vDash F$.
\end{definition}

This is read ``$\Sigma$ implies $F$''. If $\Sigma=\{G\}$, we write $G\vDash F$.

\begin{definition}
We write $F\equiv G$ if for each model $m$,
\[ m\vDash F \iff m\vDash G. \]
\end{definition}

\begin{definition}
Formulas $F$ and $G$ are \textit{equisatisfiable} if
\[ F \text{ is sat} \iff G \text{ is sat.}\]
\end{definition}

\begin{definition}
Formulas $F$ and $G$ are \textit{equivalid} if $\vDash F\iff\vDash G$.
\end{definition}

\subsubsection{Decidability of SAT}

\begin{fdef}
A problem is \textit{decidable} if there is an algorithm to solve the problem.
\end{fdef}

This is required since G\"odel's Incompleteness implies the existence of undecidable problems.

The problem we consider here, known as the \textit{propositional satisfiability problem} is:
\begin{quote}
For a given $F\in P$, is $F$ satisfiable?
\end{quote}

\begin{theorem}
The propositional satisfiability problem is decidable.
\end{theorem}
\begin{proof}
We enumerate the $2^{|\Vars|}$ elements of $\Vars(F)\to\mathcal{B}$. If any of the models satisfy the formula, $F$ is sat. Otherwise, it is unsat.
\end{proof}

The cost is obviously exponential and we would want to do better. There are, however, several tricks that make satisfiability checking more feasible for real-world formulas.

\subsubsection{Truth Tables}

We wish to assign a truth value to every formula $F$.

Given a model $m:\Vars\to\mathcal{M}$, we can naturally extend it to $m:P\to B$ as
\[
    m(F) = 
    \begin{cases}
    1, & m\vDash F, \\
    0, & \text{otherwise.}
    \end{cases}
\]

This extended $m$ is known as the \textit{truth function}. We needn't introduce new symbols since this is a very natural extension.\\

For a formula $F$, a truth table consists f $2^{|\Vars(F)|}$ rows, where each row considers one of the models and computes the corresponding truth value of $F$.

For example, show that $p\vee q \equiv \neg(\neg p \wedge \neg q)$ and $p\wedge q \equiv \neg(\neg p \vee \neg q)$, also known as De Morgan's laws.\\
It is also easily shown that $p\implies q \equiv (\neg p \vee q)$.

Truth tables are tedious because we need to write $2^n$ rows even if a simple observation could easily show (un)satisfiability.\\
For example, $a \vee (c \neg a)$ is very clearly true. If there's no $\neg$s (of any form, $\oplus$ in particular) in general, one can just set everything as true. Another example is that $(a \vee (c \neg a))\wedge\neg (a \vee (c \neg a))$ is obviously unsat.\\
How do we take such shortcuts?

\subsubsection{Expressive power of propositional logic}

A finite boolean function is one from $\B^n\to\B$.\\
A formula $F$ with $\Vars(F)=\{p_1,\ldots,p_n\}$ can be viewed as a boolean function $f$ such that for each model $m$, $m(F)=f(m(p_1),\ldots,m(p_n))$. This is just an alternate way of writing a truth table (as a function instead of a table).

\begin{theorem}
FOr each finite boolean function $f$, there is a formula $F$ that represents $f$.
\end{theorem}
\begin{proof}
Let $f:\B^n\to\B$. Let $p_i^0\coloneqq\neg p_i$ and $p_i^1\coloneqq p_i$. For every $(b_1,\ldots,b_n)\in\B^n$, let
\[
    F_{(b_1,\ldots,b_n)} \coloneqq
    \begin{cases}
        (p_1^{b_1}\wedge\cdots\wedge p_n^{b_n}), & f(b_1,\ldots,b_n)=1 \\
        \perp, & \text{otherwise.}
    \end{cases}
\]
We can then define the required formula $F$ by taking the conjunction over all boolean combinations,
\[ F \coloneqq F_{(0,\ldots,0)} \vee \cdots \vee F_{(1,\ldots,1)}. \]
\end{proof}

Observe that we have only used three logical connectives.

What if we do not have all logical connectives? Then we may not be able to represent all boolean functions. This is known as ``insufficient expressive power".\\

For example, $\wedge$ alone cannot express all boolean functions. Consider the function $f=\{0\mapsto 1, 1\mapsto 1\}$. We show that this cannot be achieved by any $\wedge$s by taking induction on the size of formulas containing the variable $p$ and $\wedge$. For the base case, our only choice of formula is $p$. Now, suppose that formulas $F$ and $G$ of size less than $n-1$ do not represent $f$. We can construct a longer formula by $(F\wedge G)$. This formula does not represent $f$ because we can always pick a model where $F$ and $G$ produce $0$.

We originally used $8$ connectives. This is not the minimal set required for maximum expressivity, however. For example, $\neg$ and $\vee$ can define the whole propositional logic. Indeed,
\begin{itemize}
    \item $\top \equiv p \vee \neg p$,
    \item $\perp \equiv \neg \top$,
    \item $(p \wedge q) \equiv \neg(p \vee q)$,
    \item $(p\otimes q) \equiv (p \wedge \neg q) \vee (\neg p \wedge q)$,
    \item $(p\implies q) \equiv (\neg p \vee q)$, and
    \item $(p\iff q) \equiv (p\implies q) \wedge (q\implies p)$.
\end{itemize}