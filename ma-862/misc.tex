%!TEX root = ./main.tex

\clearpage
\section{Miscellaneous}

\subsection{The K\"{o}nig-Egev\'{a}ry Theorem}

In a bipartite graph $G = (S,T,E)$, we denote a matching by a triple $(A,B,f)$, where $A\subseteq S$, $B\subseteq T$, and $f:A\to B$ is a bijection such that $(a,f(a)) \in E$ for any $a \in A$.

\begin{fdef}
	Given a bipartite graph $G=(S,T,E)$, a \emph{cover} of $G$ is a pair $(J,K)$, where $J \subseteq S$, $K \subseteq T$, and for any $st \in E$, $s \in J$ or $t \in K$. The size of such a cover is $|J|+|K|$.
\end{fdef}

\begin{flem}
	Let $M$ be a matching in $G$ and $(J,K)$ a cover. Then, $|M| \le |J|+|K|$.
\end{flem}
\begin{proof}
	Let $M = (A,B,f)$ be a matching. We have
	\[ |M| = |A| = |J \cap A| + |(S-J)\cap A| \le |J| + |f(A \cap (S-J))| \le |J|+|K|. \]
\end{proof}

\begin{ftheo}[K\"{o}nig]
	Let $G$ be a bipartite graph, $M$ a matching in $G$, and $(J,K)$ a cover. Then, $|M|=|J|+|K|$ iff $M$ is a maximum matching and $(J,K)$ is a minimum cover. 
\end{ftheo}
The forward direction is trivial by the previous lemma. We spend the rest of this subsection proving the backward direction, with a proof due to Edmonds.

Denote by $\F$ the field $\R[\{x_e : e \in E\}]$ of rational functions with real coefficients in $\{x_e : e \in E\}$.

\begin{flem}
	Consider the matrix $X$ over $\F$, with rows indexed by $S$ and columns indexed by $T$, such that $X_{st} = x_{st}$ if $st \in E$ and $0$ otherwise. Then, $\rank(X)$ is equal to the size of a maximum matching in $G$. 
\end{flem}
\begin{proof}
	Let $M = (A,B,f)$ be a largest matching in $G$ of size $k$. Consider the $k \times k$ submatrix $Y$ of $X$, with rows indexed by $A$ and columns indexed by $B$. Note that
	\[ \det(Y) = \sum_{\substack{\pi : A \to B \\ \pi \text{ bijection}}} \sign(\pi) \prod_{s \in A} X_{s \pi(s)} \ne 0, \]
	because $f$ is a bijection for which the corresponding term is nonzero. Therefore, $\rank(X) \ge k$.\\
	On the other hand, let $\rank(X) = r$ and suppose there is a $r \times r$ submatrix $Y$ of $X$ with nonzero determinant, with rows indexed by $A$ and columns indexed by $B$. Then, by the definition of the determinant, there is a bijection $\pi : A \to B$ such that $s \pi(s) \in E$ for all $s \in A$. Therefore, there is a matching of size at least $\rank(X)$.
\end{proof}

\begin{flem}
	Consider the same matrix $X$ as in the previous lemma. Then, $\rank(X)$ is equal to the size of a smallest cover of $G$.
\end{flem}
\begin{proof}
	Suppose that $X$ is such that there is some row that is not linearly dependent on the remaining rows. Removing this row, we obtain a matrix $X_1$. Keep repeating this procedure until a matrix $X_s$ is obtained, such that every row of $X_s$ is linearly dependent on the other rows. Now perform the same procedure on the columns, until the matrix $X_{s+t}$ is obtained. Then, $\rank(X) = s+t+\rank(X_{s+t})$. Set $(J,K)$ to be the sets indexing the removed rows and columns. It is also easy to see that $X_{s+t}$ is the all zeros matrix -- 
\end{proof}

\subsection{The Erd\H{o}s-Ko-Rado Theorem}

\begin{fdef}
	$\mathcal{A} \subseteq B(n,k)$ is said to be an \emph{intersecting family} if for any $X,Y \in \mathcal{A}$, $X \cap Y \ne \emptyset$.
\end{fdef}

\begin{ftheo}[Erd\H{o}s-Ko-Rado]
	\label{ekr}
	Let $k \le n/2$ and $\mathcal{A} \subseteq B(n,k)$ be intersecting. Then,
	\begin{enumerate}[label=(\alph*)]
		\item $|\mathcal{A}| \le \binom{n-1}{k-1}$ and furthermore,
		\item if $|\mathcal{A}| = \binom{n-1}{k-1}$, there is some $i \in [n]$ such that $i \in A$ for all $A \in \mathcal{A}$.
	\end{enumerate}
\end{ftheo}

% other interesting results if t-intersecting; one threshold beyond which (i) is true and another beyond which (ii)
% in fact, the explicit minimum n for which it happens is known!!! remarkable result (wilson's theorem)
% all ``intersecting permutations'' : \exists i such that \pi_i = \tau_i : number here is (n-1)! and the bound is attained iff it is the set of all perms that send i to j

Consider the \emph{Kneser graph} $K(n,k)$ on $B(n,k)$, with $X,Y$ adjacent iff $X \cap Y = \emptyset$. The Erd\H{o}s-Ko-Rado Theorem just characterizes the maximum independent sets in this graph.

\begin{flem}[Ratio bound]
	\label{lem:ratio-bound}
	Let $G$ be a $d$-regular graph on $n$ vertices. Let $\theta$ be the smallest eigenvalue of the adjacency matrix of $G$. Then, setting $\alpha(G)$ as the size of a maximum independent set in $G$, $\alpha(G) \le \frac{n}{1-d/\theta}$.\\
	Moreover, if $S$ is an independent set for which equality is attained above, then $\chi_S - (|S|/n) \mathbf{1}$ is an eigenvector of $G$ with eigenvalue $\theta$.
\end{flem}
\begin{proof}
	Note that $\theta < 0$. Let $s = |S|$, $A$ the adjacency matrix of $G$, and consider
	\[ M = A - \theta \Id - \frac{d-\theta}{n}J, \]
	where $J$ is the all-ones matrix.\\
	Recall that a graph is regular iff its adjacency matrix commutes with $J$. First off, we have
	\[ M \mathbf{1} = A \mathbf{1} - \theta \mathbf{1} - \frac{d-\theta}{n} J \mathbf{1} = 0. \]
	For any $x$ orthogonal to $\mathbf{1}$,
	\[ x^\top M x = x^\top(A-\theta \Id)x \ge 0. \]
	It follows that $M$ is positive semidefinite. Set $v = \chi_S$ for a stable set $S$. Then,
	\begin{align*}
		0 &\le v^\top M v \\
			&= v^\top A v - \theta v^\top v - \frac{d-\theta}{n} v^\top J v \\
			&= -\theta |S| - \frac{d-\theta}{n}|S|^2.
	\end{align*}
	The first part of the result follows. For the second part, we must have that equality is attained above iff $v^\top M v = 0$. Since $M$ is positive semidefinite, $Mv = 0$. Therefore,
	\[ Av = \theta v + \frac{d-\theta}{n} Jv = \theta v + \frac{|S|(d-\theta)}{n} \mathbf{1}. \]
	Consequently,
	\begin{align*}
		A\left(v - \frac{|S|}{n} \mathbf{1}\right) &= \theta v + \frac{|S|(d-\theta)}{n} \mathbf{1} - \frac{|S|d}{n} \mathbf{1} \\
			&= \theta v - \frac{|S|\theta}{n} \mathbf{1}. \qedhere
	\end{align*}
\end{proof}

The proof above works for the following more general result.

\begin{flem}
	Let $A$ be a symmetric matrix with constant row sum $d$ that is compatible with the $d$-regular graph $G$ on $n$ vertices, that is, $A_{uv} = 0$ if $u,v$ are not adjacent. If $\theta$ is the least eigenvalue of $A$, then $\alpha(G) \le n / (1 - d/\theta)$.\\
	Moreover, if equality holds above for a set $S$, then $\chi_S - (|S|/n)\mathbf{1}$ is an eigenvector of $A$ with eigenvalue $\theta$.
\end{flem}
Alternatively, $\chi_S \in \langle \mathbf{1}\rangle \oplus E_\theta$, where $E_\theta$ is the eigenspace of $\theta$.\\

Let us now get to the proof of \nameref{ekr}. To use the ratio bound above, we would like to analyze the spectrum of the Kneser graph (for the first part), and then use the eigenspaces of its adjacency matrix for the second part.\\
The Kneser graph is a $\binom{n-k}{k}$-regular graph on $\binom{n}{k}$ vertices. Recall from Schrijver that the eigenvalues of the Kneser graph are
\[ (-1)^j \binom{n-k-j}{k-j} \]
with multiplicity $\binom{n}{j} - \binom{n}{j-1}$ for $j=0,1,\ldots,k$. It may be checked that the least eigenvalue corresponds to $j=1$, and is equal to $-\binom{n-k-1}{k-1}$. Therefore,
\[ \alpha(K(n,k)) \le \frac{\binom{n}{k}}{1 + \binom{n-k}{k}/\binom{n-k-1}{k-1}} = \frac{\binom{n}{k}}{1 + \frac{n-k}{k}} = \binom{n-1}{k-1} \]
as desired.\\

For the second part, we require the eigenspaces of the Kneser graph. Consider the $\binom{n}{k} \times n$ matrix $N(1,k)$, with rows indexed by $B(n,k)$ and columns indexed by $[n]$, with $B_{X,i} = 1$ if $i \in X$ and $0$ otherwise. It is easy to show that $\rank(N(1,k)) = n$. Each column is the characteristic vector of a maximum stable set achieving the ratio bound, and a simple dimension-counting argument shows that the column space of this matrix is precisely $\langle \mathbf{1}\rangle \oplus E_\theta$.\\
If $S$ is a stable set meeting the ratio bound, then $v = \chi_S$ is in the column space of $N = N(1,k)$. Let $h$ such that $Nh = v$. For $X \in B(n,k)$, denote the row of $X$ in $N$ as $r_X$. If $X \not\in S$, then $r_X$ is orthogonal to $h$. Without loss of generality, assume that $\{1,\ldots,k\} \in S$. Then, $X \not\in S$ if $X \cap [k] = \emptyset$.\\
Consider the $\binom{n-k}{k} \times n$ submatrix $M$ of $N$ indexed by those rows not intersecting $[k]$. Note that the first $k$ columns of $M$ are $0$. It is seen that the rank of $M$ is $n-k$ (Why?). It follows that if $i \not\in [k]$, $h_i = 0$. That is, $\operatorname{supp} h \subseteq [k]$. This is true more generally for any $X \in S$ in place of $[k]$, so $\operatorname{supp} h \subseteq \bigcap_{Y \in S} Y$.

\subsection{Standard Young Tableaux}

Recall the definition of a Standard Young Tableau, and the number $f_\lambda$ of SYTs of shape $\lambda$. Also recall Theorem 2.35 in my \href{https://amitrajaraman.github.io/notes/ma-861/}{Combinatorics I} notes.

\begin{ftheo}
	\label{th7.5}
	\[ \sum_{\lambda \vdash n} f_\lambda^2 = n!. \]
\end{ftheo}

In Combinatorics I, had seen an algebraic proof there using representation theory, as well as a bijective proof using the RSK algorithm. We give an alternate proof here.\\
Consider \emph{Young's poset}, a poset on the set of all partitions, with $\lambda \ple \mu$ if the Ferrer diagram of $\lambda$ is contained in that of $\mu$. The diagram of this poset is called the \emph{Hasse diagram}, with the resulting graph being called the \emph{Hasse graph}. We consider walks on this graph, which are referred to as \emph{Hasse walks}. Given a Hasse walk, each step of the walk takes a step either up ($U$) or down ($D$) the poset. We can then associate to any walk a sequence of $U$s and $D$s, called the walk's \emph{type}. For example, the walk $1^2,21,2,3,31,21$ would be associated to $DUUDU$, which we write more succinctly as $DU^2DU$.\\

How many walks of type $U^n$ are there starting at $\emptyset$? Given any such walk, we can create an SYT by filling an $i$ in the new block obtained in the $i$th step. For example, the walk $\emptyset,1,1^2,1^3,21^3$ would correspond to the SYT
\[ \begin{ytableau} 1 & 4 \\ 2 \\ 3 \end{ytableau} \]
Consequently, $f_\lambda$ is the number of walks from $\emptyset$ to $\lambda$ of type $U^n$. Therefore, the number of walks from $\emptyset$ to $\emptyset$ of type $U^nD^n$ is $\sum_{\lambda \vdash n} f_\lambda^2$.\\

Denote Young's poset by $Y$, and consider the infinite-dimensional vector space $\C[Y]$. Consider the linear map $U : \C[Y] \to \C[Y]$ with
\[ U(\lambda) = \sum_{\substack{\mu \supseteq \lambda \\ |\mu| = |\lambda|+1}} \mu. \]
Similarly, define $D : \C[Y] \to \C[Y]$ by
\[ D(\lambda) = \sum_{\substack{\mu \subseteq \lambda \\ |\mu| = |\lambda|-1}} \mu. \]
Denoting by $Y(i)$ the set of partitions of $i$, define the restriction $U_i : \C[Y(i)] \to \C[Y(i+1)]$ of $U$ and $D_i$ similarly.

\begin{flem}[Weyl's Identity]
	$DU - UD = \Id$.
\end{flem}
Similar identities hold in many places -- for example in the proof of \Cref{schrijver-lem1}, and if we look at the derivative operator $\partial : \C[x] \to \C[x]$, we have $\partial x - x \partial = \Id$.
\begin{proof}[Proof sketch]
	To prove this, split a given Ferrer diagram into ``rectangles'', grouping together parts of the same size. Then, the up operator is obtained by choosing a rectangle, and adding a block to its outer up-right corner, and the down operator is obtained by choosing a rectangle, and removing its inner down-right corner. All such $UD$ and $DU$ operations may be paired, except that which adds a single block to a new row of the partition.
\end{proof}

Due to this identity, this poset is said to be a \emph{differential poset}.
\begin{fcor}
	For a polynomial $p(x)$, $Dp(U) = p'(U) + p(U)D$.
\end{fcor}
\begin{proof}
	We may assume without loss of generality that $p(x) = x^n$ for some $n$. The statement for $n=1$ follows from Weyl's identity. In general, using the inductive hypothesis, we have
	\[ DU^{n+1} = (DU^n)U = (nU^{n-1} + U^nD)U = nU^{n} + U^n(UD+\Id) = (n+1)U^n + U^{n+1}D. \]
\end{proof}

More generally, if $p(x)$ is a polynomial, then
\[ Dp(U) = p'(U) + p(U)D. \]
This is easily proved by assuming without loss of generality that $p(x)$ is of the form $x^n$, then performing induction on $n$.

\begin{proof}[Proof of \Cref{th7.5}]
	By the earlier argument, we have $D^nU^n \emptyset = \left(\sum_{\lambda \vdash n} f_\lambda^2\right) \emptyset$. We prove this by induction. The base case $n=1$ is trivial. In general,
	\[ D^nU^n \emptyset = D^{n-1}(DU^n) \emptyset = D^{n-1}(nU^{n-1} + U^nD)\emptyset = n D^{n-1}U^{n-1} \emptyset = (n!) \emptyset. \qedhere \]
\end{proof}

Consider $Y(j-1,j)$, the bipartite graph on $Y(j-1) \cup Y(j)$, with adjacency being determined by inclusion. Let $A$ be the adjacency matrix of this graph.

\begin{ftheo}
	$A$ has eigenvalue $0$ with multiplicity $p(j) - p(j-1)$, and for $0 \le s \le j-1$, the eigenvalue $\pm \sqrt{j-s}$ with multiplicity $p(s)-p(s-1)$.
\end{ftheo}
The proof is actually vaguely similar to that in the Schrijver bound, where we saw the eigenvalues of $B(n,k)$.
\begin{proof}
	Since $DU-UD = \Id$, each $U_i$ is injective and each $D_i$ is surjective -- this is because $D_{i+1} U_i - U_{i-1} D_i = \Id$.\\
	We have $\dim \ker D_j = p(j) - p(j-1)$, showing the multiplicity of $0$ as an eigenvalue, since $D_j$ is just the adjacency operator for partitions in $Y(j)$.\\
	Let $v \in \ker D_s$, and consider $v^* = \pm \sqrt{j-s} U^{j-1-s}v + U^{j-s}v$. We claim that $v^*$ is an eigenvector of $A$, of eigenvalue $\pm\sqrt{j-s}$. We have
	\begin{align*}
		Av^* &= \pm\sqrt{j-s}AU^{j-1-s}v + AU^{j-s}v \\
			&= \pm\sqrt{j-s}U^{j-s}v + DU^{j-s}v \\
			&= \pm \sqrt{j-s}U^{j-s}v + (j-s)U^{j-s-1}v + U^{j-s}Dv \\
			&= \pm\sqrt{j-s}\left( \pm\sqrt{j-s}U^{j-s-1}v + U^{j-s}v \right) = \pm\sqrt{j-s}v^*.
	\end{align*}
	This gives $\pm \sqrt{j-s}$ as an eigenvalue of multiplicity $\ge \ker D_s = p(s) - p(s-1)$. A simple counting argument over all $s$ shows that this covers all the eigenvalues (with multiplicities) -- we have $p(j) - p(j-1) + 2\sum_{s=0}^{j-1} (p(s) - p(s-1)) = p(j) - p(j-1) + 2p(j-1) = p(j) + p(j-1)$, which is the total number of vertices.
\end{proof}

\begin{fcor}
	Fix $j \ge 1$. The number of ways to choose a partition $\lambda$ of $j$, then repeatedly, $m$ times, deleting an inner corner and adding an outer corner, to get back $\lambda$, that is, the number of closed walks of length $2m$, is 
	\[ \sum_{s=1}^{j} (p(j-s) - p(j-s-1)) s^m. \]
\end{fcor}
This immediately follows by looking at $\Tr(A^{2m})$.\\

Let $w = A_n A_{n-1} \cdots A_1$ be a word, where each $A_i$ is in $\{U,D\}$. $w$ can be interpreted as both a linear transformation as well as the type of a Hasse walk.\\
Let $\lambda$ be a partition. We say that $w$ is \emph{valid} for $\lambda$, if it is possible to reach $\lambda$ from $\emptyset$ with a walk of type $w$.

\begin{fprop}
	$w$ is valid if the number of $U$s in $w$ minus the number of $D$s in $w$ is $|\lambda|$, and any suffix of $w$ has at least as many $U$s as $D$s.
\end{fprop}

If $w$ is valid for partitions of size $n$, then set
\[ w\emptyset = \sum_{\lambda \vdash n} \alpha(w,\lambda) \lambda. \]
As observed earlier, we have $\alpha(U^n,\lambda) f_\lambda$.

\begin{ftheo}
	Let $\lambda \vdash n$ and $w = A_r A_{r-1} \cdots A_1$ be valid for $\lambda$. Define $S_w = \{i \in [n] : A_i = D\}$, and for each $i \in S_w$, $a_i$ as the number of $D$s to the right of $A_i$ and $b_i$ as the number of $U$s to the right of $A_i$. Then,
	\[ \alpha(w,\lambda) = f_\lambda \prod_{i \in S_w} (b_i - a_i).  \]
\end{ftheo}
That is,
\[ w\emptyset = \left(\prod_{i \in S_w} (b_i - a_i)\right) U^n\emptyset. \]
% hook-length formula: to generate a uniformly random SYT of shape \lambda, pick a random cell, pick a random point in its hook, pick a random point in its hook and so on... at the finally landed cell, input n. remove this cell and recurse. this generates a random SYT (Why?). The probability of any SYT is n!/\prod h_{ij} (Why?).
\begin{proof}
	The idea is to pick an arbitrary $DU$ in $w$, and replace it with $UD + \Id$. Eventually, we end up at $w = \sum_{i-j=m} r_{ij} U^i D^j$, where $m$ is the difference between the number of $U$s and $D$s in $w$. We are interested in $r_{i,0}$, since $U^iD^j \emptyset = 0$ for $j > 0$. Clearly, these coefficients $r_j$ do not depend on the order in which we replace the $DU$s -- this is a consequence of the fact that $\{U^iD^j : i-j = m\}$ is linearly independent, since $(U^iD^j)(1^k) = 0$ iff $k < j$.\\
	We have
	\[ w\emptyset = r_{m,0} (w) U^m \emptyset, \]
	so we are done if we show that $r_{m,0} = \prod_{i \in S_w} (b_i - a_i)$.
	Now, observe that
	\[ Uw = \sum_{i-j = m} r_{ij}(w)U^{i+1}D^j = \sum_{i-j=m+1} r_{i+1,j}(Uw) U^{i} D^j, \]
	so $r_{i,j}(Uw) = r_{i-1,j}(w)$ and
	\[ Dw = \sum_{i-j = m} r_{ij}(w) DU^{i}D^j = \sum_{i-j = m} r_{ij}(w) (U^iD + iU^{i-1}) D^j, \]
	so $r_{i,j}(Dw) = r_{i,j-1}(w) + (i+1) r_{i+1,j}(w)$.\\
	So, for $j = 0$, $r_{i,0}(Uw) = r_{i-1,0}(w)$ and $r_{i,0}(Dw) = (i+1) r_{i+1,0}(w)$. Here, $i+1$ is precisely the difference between the number of $U$s and $D$s in $w$. The desideratum follows by an inductive argument.
\end{proof}

Set $\beta(\ell,\lambda)$ as the number of walks of length $\ell$ from $\emptyset$ to $\lambda$. Clearly, $\beta(\ell,\lambda) = 0$ unless $\ell \equiv |\lambda| \pmod{2}$. Also, $\beta(\ell,\lambda)$ is the coefficient of $\lambda$ in $(D+U)^\ell \emptyset$.
\begin{flem}
	Set
	\[ (D+U)^\ell = \sum_{i,j} b_{i,j}(\ell) U^i D^j. \]
	$b_{i,j}(\ell) = 0$ if $\ell = i-j$ is odd, and if $\ell-i-j = 2m$, then
	\[ b_{i,j}(\ell) = \frac{\ell!}{2^m i! j! m!}. \]
\end{flem}
\begin{proof}
	We prove this by induction on $\ell$. The statement is trivial for $\ell = 1$. In general, using the inductive hypothesis,
	\begin{align*}
		\sum_{i,j} b_{i,j}(\ell+1) U^i D^j &= (D+U) (D+U)^{\ell} \\
			&= \sum_{i,j} b_{i,j}(\ell) (U^{i+1} D^j + D U^i D^j) \\
			&= \sum_{i,j} b_{i,j}(\ell) (U^{i+1} D^j + U^i D^{j+1} + i U^{i-1} D^j),
	\end{align*}
	so
	\[ b_{i,j}(\ell+1) = b_{i-1,j}(\ell) + b_{i,j-1}(\ell) + (i+1) b_{i+1,j}(\ell). \]
	A simple calculation completes the proof.
\end{proof}

\begin{fcor}
	Let $\ell \ge n$, $\lambda$ a partition of $n$, and $\ell-n$ be even. Then,
	\[ \beta(\ell,\lambda) = \binom{\ell}{n} (1 \cdot 3 \cdot 5 \cdot \cdots \cdot (\ell-n-1)) f_\lambda. \]
\end{fcor}
\begin{proof}
	Indeed,
	\[ (D+U)^\ell \emptyset = \sum_{i} b_{i,0}(\ell) U^i \emptyset = \sum_{i} \frac{\ell!}{2^{(\ell-i)/2} i! ((\ell-i)/2)!} \sum_{\lambda \vdash i} f_\lambda \lambda, \]
	and simplifying this a bit completes the proof.
\end{proof}