\section{Introduction}

	\subsection{Lecture 1: Matrix Multiplication}

		We begin with a question.

		\begin{problem*}
			Given three $n\times n$ matrices $A,B,C$, decide whether $AB = C$. 
		\end{problem*}

		One na\"{i}ve way to do this is to compute $AB$ and check if it is identical to $C$. The na\"{i}ve implementation of this runs in $O(n^3)$, while the best known implementation (at the time) runs in about $O(n^{2.373\ldots})$. \\
		Can we do the required in $O(n^2)$ time, perhaps in a random fashion (with some failure probability)?\\

		% \paragraph{A simple randomized algorithm that does not do the job}
		Consider the following algorithm to start with. For each row in $C$, choose an entry randomly and verify that it matches the corresponding entry in $AB$. In a similar spirit, a second algorithm is to choose $n$ entries of $C$ randomly and verify.\\

		If $AB = C$, it is clear that no matter how we choose to test, we shall return that the two are indeed equal. The probability we would like to minimize is
		\[ \Pr[\text{the algorithm outputs yes} \mid AB \ne C]. \]
		Of course, this probability depends on $A,B,C$. This probability is over the randomness inherent in the algorithm, not in some choosing of $A,B,C$. \\

		When $AB$ and $C$ differ at only one entry, the earlier proposed algorithm has a success probability of $1/n$ (so the quantity mentioned above is $1-1/n$). This is very bad, as it means that to reduce the failure probability to some constant, we would need to repeat this $n$ times.\\

		An algorithm that does the job is as follows.\\ % r \gets_R \{0,1\}^n
		Randomly choose $r \in \{0,1\}^n$. Compute $ABr$ and $Cr$, and verify that the two are equal. This is an $O(n^2)$ algorithm, since multiplying a matrix with a vector takes $O(n^2)$ and we perform this operation thrice, in addition to an $O(n)$ verification step at the end.\\

		We claim that the failure probability of this algorithm is at most $1/2$.\\
		The failure probability can be rephrased as follows. Let $x,y \in \R^{1 \times n}$. What is $\Pr[xr = yr \mid x \ne y]$? The earlier failure probability is at most equal to this, with equality attained (in a sense) when the two matrices differ at exactly one row. \\
		This in turn is equivalent to the following. Let $z \in \R^{1 \times n}$. What is $\Pr[zr = 0 \mid z \ne 0]$? Suppose that $z_i \ne 0$ for some $i$. For any choice of the remaining $n-1$ bits, at most one of the two options for the $i$th bit can result in $zr = 0$.\\
		Let us do this slightly more formally. Assume wlog that $z_n \ne 0$. Then,
		\begin{align*}
			\Pr\left[z_1r_1 + \cdots + z_nr_n = 0 \mid z_n \ne 0\right] &= \Pr\left[r_n = -\frac{z_1r_1 + \cdots + z_{n-1}r_{n-1}}{z_n} \mid z_n \ne 0\right] \\
				&\le \max_{r_1,\ldots,r_{n-1}} \Pr\left[ r_n = -\frac{z_1r_1 + \cdots + z_{n-1}r_{n-1}}{z_n} \mid z_n \ne 0, r_1,\ldots,r_{n-1} \right]
		\end{align*}
		which is plainly at most $1/2$ -- we cannot have that both $0$ and $1$ are equal to the quantity of interest!

		\begin{remark}
			If we instead choose $r$ from $\{0,1,\ldots,q-1\}^n$ instead, the failure probability now goes down at most $1/q$.\\
			There is a tradeoff at play here between the reduction in the failure probability and the increase in the number of random bits (it goes from $n$ to $O(n \log q)$).
		\end{remark}

		\begin{question*}
			Can we reduce the number of random bits in this algorithm? Can we make it deterministic?
		\end{question*}

		To answer the question of determinism, suppose the algorithm designer chooses $k$ vectors $r^{(1)},\ldots,r^{(k)} \in \R^n$ and tests whether $ABr^{(i)} = Cr^{(i)}$. This will fail if $k < n$. Indeed, an adversarial input is a $z$ that is nonzero but with $zr^{(i)} = 0$ for $1 \le i \le k$.\\
		The determinism here is in the sense that the vectors are chosen before the inputs are provided.\\

		On the other hand, we \emph{can} reduce the number of random bits used. In fact, we can go to about $O(\log n)$ random bits.\\
		The goal of derandomization is to use a smaller number of random bits (perhaps by conditioning together previously independent bits), without losing the power of the earlier independent bits.\\
		Let
		\[ A(x) = a_0 + a_1x + \cdots + a_{d}x^{d} \]
		be a nonzero polynomial of degree $d$. Choose $x$ randomly from $\{0,1,\ldots,q-1\}$. It is not difficult to see that
		\[ \Pr_{x \sim \{0,1,\ldots,q-1\}}\left[ A(x) = 0 \right] \le \frac{d}{q}. \]
		Inspired by this, we can reduce randomness as follows. Choose $x$ randomly from $\{0,1,\ldots,2n-1\}$, and set $r = (1,x,x^2,\ldots,x^{n-1})$. Then,
		\[ \Pr[ z_1r_1 + z_2r_2 + \cdots + z_nr_n = 0 ] = \Pr\left[ z_1 + z_2x + z_2x^2 + \cdots + z_nx^n \right] \le \frac{n-1}{2n-1} \le \frac{1}{2}. \]
		There are some other issues that enter the picture here, namely the bit complexity now that $x^{n-1}$ has $O(n)$ bits. One easy fix for this is to perform all operations modulo some prime.

		% next class, pairwise independence.

		% MISSED LECTURE 2

		% LECTURE 3
	\subsection{Lecture 3: Pairwise Independence}

		Let $X_1,\ldots,X_n$ be random variables such that for any distinct $i,j$, $X_i,X_j$ are independent:
		\[ \Pr[X_i = \alpha, X_j = \beta] = \Pr[X_i = \alpha] \Pr[X_j = \beta]. \]
		This is referred to as \emph{pairwise independence}. Analogously, we can define \emph{$k$-wise independence}, which requires that any subset of at most $k$ random variables is independent.

		\begin{fex}
			Let random variables $X_1,X_2$ take values in $\{0,1\}$ uniformly, and let $X_3 = X_1 \oplus X_2$. This set of random variables is pairwise independent, but not completely independent!
		\end{fex}

		% independently put each vertex in S with pr 1/2 - 1/2-approx algo for max-cut

		Given a cut $(S,\overline{S})$ of a graph, denote
		\[ \partial S = \{ (u,v) : u \in S, v \not\in S \}. \]
		% prof denotes it \delta(S)
		Consider an algorithm that chooses a uniformly random cut $S$ of the vertex set $V$ (which corresponds to independently choosing each vertex with probability $1/2$). Then,
		\[ \E[|\partial S|] = \sum_{e \in E} \Pr[e \in \partial S] = \sum_{\{u,v\} \in E} \Pr[u \in S, v \not\in S] + \Pr[u \not\in S, v \in S] = |E|/2. \]
		In particular, this gives (in expectation) a $1/2$-approximation of a max-cut.\footnote{Using Markov's inequality, it gives a $1/2$-approximation with probability at least $1/2$.}\\

		Now, note that this algorithm does not require independence of all the $|V|$ vertex-choosings, it suffices to have pairwise independence! This begs the question, how do we generate $n$ pairwise independent while using a small number of actual random bits?\\
		Bouncing off the idea in the previous example, we can take $k$ random bits $X_1,\ldots,X_k$, and generate $2^k-1$ pairwise independent random bits by considering $\bigoplus_{i \in S} X_i$ for each non-empty $S \subseteq [k]$ (why are these pairwise independent?).\\
		Consequently, we can generate $n$ pairwise random bits using just $O(\log(n))$ random bits.

		\begin{remark}
			Since we have just $\log n$ random bits, we can cycle through all the possible choices for the bits, since there are only $n$ choices! This gives a deterministic polynomial time $1/2$-approximation algorithm for the max-cut problem. Instead of looking at all the $O(2^n)$ cuts, it is enough to look at $O(n)$ cuts. \\
			Interestingly, this does not even look at the structure of the graph!
		\end{remark}

		\begin{fprop}
			To generate $n$ pairwise independent random bits, we require $\Omega(\log n)$ independent random bits.
		\end{fprop}
		\begin{proof}
			Suppose that given $k$ independent random bits $Y_1,\ldots,Y_k$, we can come up with $n$ pairwise independent random bits $X_1,\ldots,X_n$. Let $f_i : \{0,1\}^k \to \{0,1\}$ for $1 \le i \le n$ be defined by $X_i = f_i(Y_1,\ldots,Y_k)$. Also, denote $f_i^{-1}(1) = \{ x \in \{0,1\}^k : f_i(x) = 1 \}$.\\
			The basic constraint that $\Pr[X_i = 1] = 1/2$ means that $|f_i^{-1}(1)| = 2^{k-1}$ and the pairwise independence constraint gives that for distinct $i,j$, $|f_i^{-1}(1) \cap f_j^{-1}(1)| = 2^{k-2}$. Let $M$ be the $n \times 2^k$ matrix such that $M_{ij} = f_i(j)$ (in the sense of the binary expansion of $j$).\\
			The previous constraints then just say that $M M^\top = 2^{k-2} (I+J)$, where $J$ is the all ones matrix.\\
			Note that the $n \times n$ matrix $2^{k-2}(I+J)$ is of rank $n$. It follows that $\rank(M) = \rank(MM^\top) = n$, so $2^k \ge n$ and we are done!
		\end{proof}

		% \{0,1\}^k \to \{0,1\}^n -- (2^{n})^{2^k} -- any bit is set for exactly 1/2 the input bits.
		% f()

		% come up with n size (2^{n}/2) subsets that are ``pairwise independent'' -- S_i \cap S_j = n/4

		Now, what happens if we want to generate pairwise independent functions instead of just bits? Can we do better?\\
		In particular, can we generate pairwise independent random variables $X_1,\ldots,X_n$ that uniformly take values in $\F_q$, where $q$ is a prime power?\\

		One simple construction is similar to the earlier one -- take $k \coloneqq \log n$ random values $y_1,\ldots,y_k$ from $\F_p$, and consider $\sum_{i \in S} y_i$ for each non-empty $S \subseteq [k]$. This takes $\log n \cdot \log |\F|$ random bits. \\

		% x^ry^s

		A better constructio for $n = q$ is as follows -- randomly choose $a,b \in \F$, and let the required random variables be $\{ az+b : z \in \F_q \}$. This takes just $\log n + \log |\F|$ bits!\\
		This idea can further be generalized to $k$-wise independence as well, taking a degree-$(k-1)$ polynomial instead. 

		% next lecture: more pairwise independence!