\section{Randomized algorithms and derandomization}

	\subsection{Lecture 1: Matrix multiplication}

		We begin with a question.

		\begin{problem*}
			Given three $n\times n$ matrices $A,B,C$, decide whether $AB = C$. 
		\end{problem*}

		One na\"{i}ve way to do this is to compute $AB$ and check if it is identical to $C$. The na\"{i}ve implementation of this runs in $O(n^3)$, while the best known implementation at the time runs in about $O(n^{2.373\ldots})$. \\
		Can we do the required in $O(n^2)$ time, perhaps in a random fashion (with some probability of failure)?\\

		% \paragraph{A simple randomized algorithm that does not do the job}
		Consider the following algorithm to start with. For each row in $C$, choose an entry randomly and verify that it matches the corresponding entry in $AB$. In a similar spirit, a second algorithm is to choose $n$ entries of $C$ randomly and verify.\\

		If $AB = C$, it is clear that no matter how we choose to test, we shall return that the two are indeed equal. The probability we would like to minimize is
		\[ \Pr[\text{the algorithm outputs yes} \mid AB \ne C]. \]
		Of course, this probability depends on $A,B,C$. This probability is over the randomness inherent in the algorithm, not in some choosing of $A,B,C$. \\

		When $AB$ and $C$ differ at only one entry, the earlier proposed algorithm has a success probability of $1/n$ (so the quantity mentioned above is $1-1/n$). This is very bad, as it means that to reduce the failure probability to some constant, we would need to repeat this $n$ times.\\

		An algorithm that does the job is as follows.\\ % r \gets_R \{0,1\}^n
		Randomly choose $r \in \{0,1\}^n$. Compute $ABr$ and $Cr$, and verify that the two are equal. This is an $O(n^2)$ algorithm, since multiplying a matrix with a vector takes $O(n^2)$ and we perform this operation thrice, in addition to an $O(n)$ verification step at the end.\\

		We claim that the failure probability of this algorithm is at most $1/2$.\\
		The failure probability can be rephrased as follows. Let $x,y \in \R^{1 \times n}$. What is $\Pr[xr = yr \mid x \ne y]$? The earlier failure probability is at most equal to this, with equality attained (in a sense) when the two matrices differ at exactly one row. \\
		This in turn is equivalent to the following. Let $z \in \R^{1 \times n}$. What is $\Pr[zr = 0 \mid z \ne 0]$? Suppose that $z_i \ne 0$ for some $i$. For any choice of the remaining $n-1$ bits, at most one of the two options for the $i$th bit can result in $zr = 0$.\\
		Let us do this slightly more formally. Assume wlog that $z_n \ne 0$. Then,
		\begin{align*}
			\Pr\left[z_1r_1 + \cdots + z_nr_n = 0 \mid z_n \ne 0\right] &= \Pr\left[r_n = -\frac{z_1r_1 + \cdots + z_{n-1}r_{n-1}}{z_n} \mid z_n \ne 0\right] \\
				&\le \max_{r_1,\ldots,r_{n-1}} \Pr\left[ r_n = -\frac{z_1r_1 + \cdots + z_{n-1}r_{n-1}}{z_n} \mid z_n \ne 0, r_1,\ldots,r_{n-1} \right]
		\end{align*}
		which is plainly at most $1/2$ -- we cannot have that both $0$ and $1$ are equal to the quantity of interest!

		\begin{remark}
			If we instead choose $r$ from $\{0,1,\ldots,q-1\}^n$ instead, the failure probability now goes down at most $1/q$.\\
			There is a tradeoff at play here between the reduction in the failure probability and the increase in the number of random bits (it goes from $n$ to $O(n \log q)$).
		\end{remark}

		\begin{question*}
			Can we reduce the number of random bits in this algorithm? Can we make it deterministic?
		\end{question*}

		To answer the question of determinism, suppose the algorithm designer chooses $k$ vectors $r^{(1)},\ldots,r^{(k)} \in \R^n$ and tests whether $ABr^{(i)} = Cr^{(i)}$. This will fail if $k < n$. Indeed, an adversarial input is a $z$ that is nonzero but with $zr^{(i)} = 0$ for $1 \le i \le k$.\\
		The determinism here is in the sense that the vectors are chosen before the inputs are provided.\\

		On the other hand, we \emph{can} reduce the number of random bits used. In fact, we can go to about $O(\log n)$ random bits.\\
		The goal of derandomization is to use a smaller number of random bits (perhaps by conditioning together previously independent bits), without losing the power of the earlier independent bits.\\
		Let
		\[ A(x) = a_0 + a_1x + \cdots + a_{d}x^{d} \]
		be a nonzero polynomial of degree $d$. Choose $x$ randomly from $\{0,1,\ldots,q-1\}$. It is not difficult to see that
		\[ \Pr_{x \sim \{0,1,\ldots,q-1\}}\left[ A(x) = 0 \right] \le \frac{d}{q}. \]
		Inspired by this, we can reduce randomness as follows. Choose $x$ randomly from $\{0,1,\ldots,2n-1\}$, and set $r = (1,x,x^2,\ldots,x^{n-1})$. Then,
		\[ \Pr[ z_1r_1 + z_2r_2 + \cdots + z_nr_n = 0 ] = \Pr\left[ z_1 + z_2x + z_2x^2 + \cdots + z_nx^n \right] \le \frac{n-1}{2n-1} \le \frac{1}{2}. \]
		There are some other issues that enter the picture here, namely the bit complexity now that $x^{n-1}$ has $O(n)$ bits. One easy fix for this is to perform all operations modulo some prime.

		% next class, pairwise independence.

		% MISSED LECTURE 2

	\subsection{Lectures 3--4: Pairwise independence}

		\subsubsection{Lecture 3}

			Let $X_1,\ldots,X_n$ be random variables such that for any distinct $i,j$, $X_i,X_j$ are independent:
			\[ \Pr[X_i = \alpha, X_j = \beta] = \Pr[X_i = \alpha] \Pr[X_j = \beta]. \]
			This is referred to as \emph{pairwise independence}. Analogously, we can define \emph{$k$-wise independence}, which requires that any subset of at most $k$ random variables is independent.

			\begin{fex}
				Let random variables $X_1,X_2$ take values in $\{0,1\}$ uniformly, and let $X_3 = X_1 \oplus X_2$. This set of random variables is pairwise independent, but not completely independent!
			\end{fex}

			% independently put each vertex in S with pr 1/2 - 1/2-approx algo for max-cut

			Given a cut $(S,\overline{S})$ of a graph, denote
			\[ \partial S = \{ (u,v) : u \in S, v \not\in S \}. \]
			% prof denotes it \delta(S)
			Consider an algorithm that chooses a uniformly random cut $S$ of the vertex set $V$ (which corresponds to independently choosing each vertex with probability $1/2$). Then,
			\[ \E[|\partial S|] = \sum_{e \in E} \Pr[e \in \partial S] = \sum_{\{u,v\} \in E} \Pr[u \in S, v \not\in S] + \Pr[u \not\in S, v \in S] = |E|/2. \]
			In particular, this gives (in expectation) a $1/2$-approximation of a max-cut.\footnote{Using Markov's inequality, it gives a $1/2$-approximation with probability at least $1/2$.}\\

			Now, note that this algorithm does not require independence of all the $|V|$ vertex-choosings, it suffices to have pairwise independence! This begs the question, how do we generate $n$ pairwise independent while using a small number of actual random bits?\\
			Bouncing off the idea in the previous example, we can take $k$ random bits $X_1,\ldots,X_k$, and generate $2^k-1$ pairwise independent random bits by considering $\bigoplus_{i \in S} X_i$ for each non-empty $S \subseteq [k]$ (why are these pairwise independent?).\\
			Consequently, we can generate $n$ pairwise random bits using just $O(\log(n))$ random bits.

			\begin{remark}
				Since we have just $\log n$ random bits, we can cycle through all the possible choices for the bits, since there are only $n$ choices! This gives a deterministic polynomial time $1/2$-approximation algorithm for the max-cut problem. Instead of looking at all the $O(2^n)$ cuts, it is enough to look at $O(n)$ cuts. \\
				Interestingly, this does not even look at the structure of the graph!
			\end{remark}

			\begin{fprop}
				\label{ind random bits using pairwise ind random bits}
				To generate $n$ pairwise independent random bits, we require $\Omega(\log n)$ independent random bits.
			\end{fprop}
			\begin{proof}
				% credit: amit and amit
				Suppose that given $k$ independent random bits $Y_1,\ldots,Y_k$, we can come up with $n$ pairwise independent random bits $X_1,\ldots,X_n$. Let $f_i : \{0,1\}^k \to \{0,1\}$ for $1 \le i \le n$ be defined by $X_i = f_i(Y_1,\ldots,Y_k)$. Also, denote $f_i^{-1}(1) = \{ x \in \{0,1\}^k : f_i(x) = 1 \}$.\\
				The basic constraint that $\Pr[X_i = 1] = 1/2$ means that $|f_i^{-1}(1)| = 2^{k-1}$ and the pairwise independence constraint gives that for distinct $i,j$, $|f_i^{-1}(1) \cap f_j^{-1}(1)| = 2^{k-2}$. Let $M$ be the $n \times 2^k$ matrix such that $M_{ij} = f_i(j)$ (in the sense of the binary expansion of $j$).\\
				The previous constraints then just say that $M M^\top = 2^{k-2} (I+J)$, where $J$ is the all ones matrix.\\
				Note that the $n \times n$ matrix $2^{k-2}(I+J)$ is of rank $n$. It follows that $\rank(M) = \rank(MM^\top) = n$, so $2^k \ge n$ and we are done!
			\end{proof}

			Alternatively, after getting $M$, one may observe that if we replace $0$ with $-1$, then the rows of $M$ are orthogonal, which again gives the required.

			% \{0,1\}^k \to \{0,1\}^n -- (2^{n})^{2^k} -- any bit is set for exactly 1/2 the input bits.
			% f()

			% come up with n size (2^{n}/2) subsets that are ``pairwise independent'' -- S_i \cap S_j = n/4

			Now, what happens if we want to generate pairwise independent functions instead of just bits? Can we do better?\\
			In particular, can we generate pairwise independent random variables $X_1,\ldots,X_n$ that uniformly take values in $\F_q$, where $q$ is a prime power?\\

			One simple construction is similar to the earlier one -- take $k \coloneqq \log n$ random values $y_1,\ldots,y_k$ from $\F_p$, and consider $\sum_{i \in S} y_i$ for each non-empty $S \subseteq [k]$. This takes $\log n \cdot \log |\F|$ random bits. \\

			A better construction for $n = q$ is as follows -- randomly choose $a_0,a_1 \in \F$, and let the required random variables be $\{ a_1z+a_0 : z \in \F_q \}$. This takes just $\log n + \log |\F|$ bits! We leave the details of checking this to the reader.\\

			% next lecture: more pairwise independence!
		
		\subsubsection{Lecture 4}

			% LECTURE 4

			In the above construction for generating $q$ pairwise independent random variables uniform in $\F_q$, if we set $q = 2^r$, then this in fact generates $q$ pairwise independent random bits $\log q$ times, using only $2 \log q$ independent random bits!\\
			The na\"{i}ve method to do this would involve generating $q$ pairwise independent random bits $\log q$ times, which takes $(\log q)^2$ bits.\\

			Further, we can generalize the construction to $n$ of the form $q^r$ by considering $\{ a_0 + \sum_{i=1}^r a_i x_i : x_i \in \F_q \}$, where the $a_i$ are iid drawn from $\F_q$.\\
			This idea can further be generalized to $k$-wise independence as well, taking a degree-$(k-1)$ polynomial $\{ \sum_{i=0}^{k-1} a_i x^i : x \in \F_q \}$ instead. Why are these $k$-wise independent? Fix distinct $x_1,x_2,\ldots,x_k \in \F_q$ and $\alpha_1,\ldots,\alpha_k \in \F$. Is it true that
			\[ \Pr\left[\sum_{j} a_j x_i^j = \alpha_i \text{ for all $i$}\right] = \frac{1}{q^k}? \]
			Indeed, there is a unique solution $(a_0,\ldots,a_{k-1})$ to this since the matrix corresponding to the system of equations is a Vandermonde matrix, which has nonzero determinant (even over $\F_q$).

			\begin{exercise}
				Show that a Vandermonde matrix is invertible.
			\end{exercise}
			\begin{solution*}
				Suppose instead that there is a nonzero vector $v$ such that $Mv = 0$, where $M$ is our $k\times k$ Vandermonde matrix of interest. This gives a nonzero polynomial of degree at most $k-1$ with $k$ roots, which is not possible.
			\end{solution*}

	\subsection{Lectures 4--5: Counting distinct elements in a stream}

		\subsubsection{Lecture 4 (continued)}

			Pseudorandomness has various applications in streaming algorithms. We generally have storage space that is far smaller than the input. We also have only one ``pass'' at the input and cannot look at older input. We can however run multiple copies of the same algorithm as we get the input, and in this case this can give better results.

			\begin{problem*}
				Suppose we are getting a stream of items $a_1,\ldots,a_m$ in $[n]$. Count the number of distinct elements that appear.
			\end{problem*}
			A realistic example of the above is trying to find the number of unique visitors to a website.

			One trivial way to do this is to store an array of size $n$ of all the elements seen so far (or perhaps marking the elements which have been seen). This requires $O(n)$ space.\\
			Can we go to $O(\log n)$ space, perhaps slightly giving up precision?\\

			Let $h$ be a function that maps each element in $[n]$ to $[0,1]$ (the continuous interval) uniformly randomly. That is, each $h(i)$ is independently uniformly randomly distributed in $[0,1]$. We start with a variable $m$ set at $\infty$. For a new $a$ in the stream, we set $m \gets \min(m,h(a))$. Finally, output $1/m - 1$.\\
			The random variable $m$ is essentially the minimum of $k$ random variables iid drawn from $[0,1]$, where $k$ is the number of unique elements. Then, $\E[m] = 1/(k+1)$.

			% next lecture: we need to store the values of all n outputs of h! can we introduce pseudorandomness to alleviate this issue?

		\subsubsection{Lecture 5}

			Before moving on, let us verify that $\E[m] = 1/(k+1)$? We have that for $x \in [0,1]$,
			\[ \Pr[m \ge x] = \Pr[h(i) \ge x \text{ for all $i$}] = (1-x)^k. \]
			Therefore,
			\[ \E[m] = \int_0^1 x \cdot k (1-x)^{k-1} \dif x = \int_0^1 n(x^{k-1} - x^k) \dif x = \frac{1}{k+1}. \]

			Now, we still have to store all $n$ outputs of $h$, so this has not really introduced any lower storage space. Choose a field $\F$ with $|\F| = N \ge n$. We shall choose $h(i)$ from $\F$ (or rather, $[N]$) such that they are pairwise independent. Recall that we had seen how to do this in Lectures 3 and 4. This construction only requires us to store the $a$ and $b$ from the algorithm, and we can compute $h(i) = ai+b$ whenever needed. This also lowers the space requirement to $O(\log n)$.  We shall now output $(N/m)-1$ instead of $(1/m)-1$.\\
			We want to show that $(N/m)-1$ is ``close'' to $k$ with high probability. That is, let us try to bound
			\[ \Pr\left[ (1-\epsilon)\frac{N}{k} \le m \le (1+\epsilon)\frac{N}{k} \right] \]
			from below.\\
			Define
			\[ Y_{i,\lambda} = \indic_{h(i) > \lambda} = \begin{cases} 1, & \text{if $h(i) > \lambda$} \\ 0, & \text{otherwise.} \end{cases} \]
			Also define
			\[ Y_\lambda = \sum_{i \in S} Y_{i,\lambda}, \]
			where $S$ is the set of the $k$ distinct elements that are seen. Then, we want to find
			\[ \Pr\left[ Y_{(1-\epsilon)\frac{N}{k+1}} = 0 \text{ and } Y_{(1+\epsilon)\frac{N}{k+1}} \ne 0 \right]. \]
			Indeed, $m$ is at least the lower bound iff no element in the stream is mapped to something less than it, and at most the upper bound iff at least one element is mapped to something less than it. Now,
			\[ \E[Y_\lambda] = \sum_{i \in S} \E[Y_{i,\lambda}] \approx k\lambda/N. \]
			Then, using Markov's inequality,
			\[ \Pr[Y_\lambda \ge 1] \le \E[Y_i] = \frac{k\lambda}{N} \]
			and as a result,
			\[ \Pr[Y_{(1-\epsilon)\frac{N}{k}} = 0] = \Pr\left[ m \ge (1-\epsilon)\frac{N}{k} \right] \ge \epsilon. \]
			Observe that thus far, we have not used any sort of independence.

			\begin{flem}
				If $X_1,\ldots,X_n$ are pairwise independent real-valued random variables,
				\[ \Var\left[\sum_i X_i\right] = \sum_i \Var[X_i]. \]
			\end{flem}
			\begin{proof}
				We have
				\begin{align*}
					\Var\left[\sum_i X_i\right] &= \E\left[ \left(\sum_i X_i - \E[X_i]\right)^2 \right] \\
						&= \E\left[ \sum_i (X_i - \E[X_i])^2 + 2\sum_{i < j} (X_i - \E[X_i])(X_j - \E[X_j]) \right] \\
						&= \sum_i \E\left[ (X_i-\E[X_i])^2 \right] + 2\sum_{i<j} \E\left[ (X_i-\E[X_i])(X_j-\E[X_j]) \right] \\
						&= \sum_i \Var[X_i] + 2\sum_{i<j} \E[X_i-\E[X_i]]\E[X_j-\E[X_j]] . & \text{($X_i,X_j$ are independent)}
				\end{align*}
			\end{proof}

			Now, set $U = (1+\epsilon)N/k$, so we have $\E[Y_U] = 1+\epsilon$. By the above lemma,
			\[ \Var[Y_U] = k\Var[Y_{i,U}] = k\cdot\frac{U}{N}\left( 1-\frac{U}{N} \right) = (1+\epsilon)\left(1 - \frac{1+\epsilon}{k}\right). \]
			Therefore, using Chebyshev's inequality,
			\begin{align*}
				\Pr[Y_U \ne 0] &\ge 1 - \Pr\left[ |Y_U - (1+\epsilon)| \ge (1+\epsilon) \right] \\
					&\ge 1 - \frac{(1+\epsilon)\left(1-\frac{1+\epsilon}{k}\right)}{(1+\epsilon)^2} \ge 1-\frac{1}{1+\epsilon} = \frac{\epsilon}{1+\epsilon}.
			\end{align*}
			Finally,
			\begin{align*}
				\Pr\left[ Y_{(1-\epsilon)\frac{N}{k}} = 0 \text{ and } Y_{(1+\epsilon)\frac{N}{k}} \ne 0 \right] &\ge 1 - \left( \Pr\left[Y_{(1-\epsilon)\frac{N}{k}} 
				\ne 0 \right] + \Pr\left[ Y_{(1+\epsilon)\frac{N}{k}} \ne 0 \right] \right) \\
					&\ge \epsilon + \frac{\epsilon}{1+\epsilon} - 1.
			\end{align*}

	\subsection{Lectures 20--22: Bipartite matching}

		\subsubsection{Lecture 20}

			The problem we shall study now is that of finding a perfect matching in a bipartite graph $G = (X,X,E)$. That is, we have two copies of a set $X$ with all edges between the two copies.

			This is a problem as old as computer science itself, and quite recently an almost-linear time algorithm for the above has been found \cite. % m polylog(m)

			We shall give an algebraic algorithm due to Mulmuley, Vazirani, Vazirani \cite{matching-mvv}. It is rather simple, and is also parallelizable. Consider the \emph{biadjacency matrix} $A_G$ of $G$, with rows and columns indexed by $X$ and $(A_G)_{uv} = 1$ if $uv$ is an edge and $0$ otherwise. Note that the $X$ used to index the rows and columns are different (choose which one is used for rows arbitrarily).

			Recall the determinant
			\[ \det(M) = \sum_{\sigma \in S_n} \sign(\pi) \prod_{i=1}^n M_{i,\sigma(i)} \]
			and permanent
			\[ \perm(M) = \sum_{\sigma \in S_n} \prod_{i=1}^n M_{i,\sigma(i)}. \]
			Suppose that the vertex sets $X$ in our bipartite graph are $[n]$.\\
			Note that any perfect matching essentially corresponds to a permutation of $[n]$ such that there is an edge between $i$ and $\sigma(i)$ for every $i \in [n]$, that is, $(A_G)_{i,\sigma(i)} = 1$ for all $i$. Due to this, we can also assign a sign to any perfect matching. \\

			Clearly, the number of perfect matchings is then just $\perm(A_G)$. On the other hand,
			\[ \det(A_G) = \sum_{M \text{ is a perfect matching}} \sign(M). \]
			Therefore, if $G$ does not have a perfect matching, $\det(A_G) = 0$. The converse is clearly not true as seen by $K_{2,2}$, which has biadjacency matrix
			\[ \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}. \]
			Similarly, the determinant is $0$ if any two vertices have the same neighbour set.\\
			How do we change something to make the converse hold true (with high probability)? The idea is rather simple, and involves changing by biadjacency matrix by replacing each element with a random integer from $\{1,2,\ldots,2n\}$. Call this new (random) matrix $M_G$. We claim that in this new setting, $\det(M_G) \ne 0$ with probability at least $1/2$.\\
			Indeed, consider the determinant polynomial in $n^2$ variables, which is
			\[ \det\begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} & x_{n2} & \cdots & x_{nn}  \end{pmatrix}. \]
			Note that this is a degree $n$ polynomial. 

			\begin{flem}[Polynomial Identity Lemma]
				Let $p$ be a polynomial in $m$ variables of degree $d$. Then,
				\[  \Pr_{\alpha \sim \{1,2,\ldots,2d\}^{m}} [p(\alpha) \ne 0] \ge \frac{1}{2}. \]
			\end{flem}
			We have already seen a proof of this back in Lecture 18 (in the case where coefficients are rational), where we worked with $\F_p$ instead. Indeed, something being nonzero modulo $p$ implies nonzeroness in $\R$.\\
			An alternative proof is by induction on the number of variables.\\

			Using this lemma in our setting, we see that $\det(M_G) = 0$ with probability at most $1/2$, so we are done. Further, we can use this algorithm to actually find a perfect matching. For $i \in [n]$, assuming that the perfect matching has the edge $1i$, check if the remaining part of the graph has a perfect matching. If yes, find a perfect matching on it (recursively). Otherwise, increment $i$.\\

			Now, can we come up with a \emph{parallel} algorithm for constructing a perfect matching? Assuming we have polynomially many machines that run independently, is it possible to determine a perfect matching rapidly, say in constant or logarithmic time?\\

			In the simple case where we have a \emph{unique} perfect matching, this is quite simple by running $m$ many machines parallelly, each computing a determinant of the graph excepting the vertices in an edge $e$. If for a given $e$ the determinant is nonzero, the edge must be in the matching.\\
			The algorithm we shall see has its idea centered around the above observation.\\

			Suppose that we can assign weights to the edges $w : E \to \Z$ such that the minimum weight perfect matching is unique, where the weight of a matching $M$ is
			\[ w(M) = \sum_{e \in M} w(e). \]
			We then alter the biadjacency matrix $A_G$ so that the edge $e$'s entry is $2^{w(e)}$ instead of $1$. Then,
			\[ \det(A_G) = \sum_{\text{perfect matchings $M$}} \sign(M) 2^{w(M)}. \]
			Note that due to the uniqueness, the above determinant is nonzero! It cannot be cancelled by any sum of higher weight matchings (Why?). After that, for each edge, decrement the weight by one and see if the minimum weight has now decreased. If it has, this edge must be part of the minimum weight perfect matching.\\
			All that remains is to find a weight assignment such that there is a unique minimum weight perfect matching. It turns out that a random weight assignment does the trick. This is not immediately clear, because if we assign weights in $\{1,2,\ldots,n^2\}$, say, then despite there being possibly exponentially many matchings, the minimum weight one is unique.

			% \begin{flem}[Isolation lemma]
			% 	Assigning each edge random weights in $\{1,2,\ldots,2m\}$, the minimum weight perfect matching is unique with probability at least $1/2$.
			% \end{flem}
			% Further, this lemma is not particular to perfect matchings and works even for objects such as spanning trees.

		\subsubsection{Lecture 21} % 27-10-2022

			\begin{flem}[Isolation Lemma, \cite{matching-mvv}]
				Let $E$ be a set of $m$ elements and $\mathcal{S} \subseteq 2^E$ an arbitrary family of subsets of $E$. Independently and uniformly randomly assign to each element of $E$ a weight in $\{1,2,\ldots,N\}$. Then,
				\[ \Pr\left[ \mathcal{S} \text{ has a minimum weight set} \right] \ge 1 - \frac{m}{N}, \]
				where the weight of a set is the sum of the weights of the elements in it.
			\end{flem}
			We get the desideratum in the context of matchings on setting $E$ to be the set of edges and $\mathcal{S}$ to be the collection of perfect matchings. 
			\begin{proof}
				Let $E = \{e_1,\ldots,e_m\}$. Split $\mathcal{S}$ into two parts $\mathcal{S}_0,\mathcal{S}_1$, where $\mathcal{S}_0 = \{ T \in \mathcal{S} : e_1 \in T \}$ and $\mathcal{S}_1 = \mathcal{S} \setminus \mathcal{S}_0$. Let us look at the event $E$ that there is a minimum weight set that contains $e_1$ and a minimum weight set that does not contain $e_1$. This means that the minimum weight set in $\mathcal{S}_0,\mathcal{S}_1$ are equal.\\
				What happens if we fix the weights of all elements other than $e_1$? The minimum weight in $\mathcal{S}_1$ is determined, and the minimum weight in $\mathcal{S_0}$ is just equal to some fixed quantity plus the weight of $e_1$. In particular, there is at most one value of $w(e_1)$ such that the two minimum weights are equal. Therefore, $\Pr[E] \le 1/N$. In general, taking the union bound, we have
				\[ \Pr[\text{there exist two min wt sets}] = \Pr\left[ \bigcup_{i \in [m]} \text{there exist min wt sets containing $e_i$ and not containing $e_i$} \right] \le \frac{m}{N}. \]
			\end{proof}
			Later, \textbf{*****} proved that the above is in fact true with $\left( 1 - \frac{1}{N} \right)^m$ instead. Note that the above is true if we replace the set weights are drawn from with any set of size $N$, so perturbing about $\log N$ bits ensures a unique solution.\\

			The isolation lemma has several surprising applications, for example that \textsf{UNIQUE-SAT}\footnote{This is \textsf{SAT}, except that we know that if there is a satisfying assignment, it is unique.} is \textsf{NP}-hard.\\

			We next look at derandomization. We cannot derandomize the isolation lemma in all its generality, but we can for specific families that have some structure.\\
			For example, this is very easy for spanning trees and it suffices to assign distinct weights to all edges. Our main goal is that of derandomizing \emph{bipartite perfect matching}. We will only be able to derandomize it to $O(\log^2n)$ random bits unfortunately, which is equivalent to giving $n^{O(\log n)}$ weight assignments with the assurance that one of them gives a minimum weight matching.\\

			The high-level view of the proof is the following.\\
			The weight construction is done in $\log n$ rounds. We start off with some huge (exponentially large) family of perfect matchings. We then come up with some weight function such that the set of perfect matchings of minimum weight is comparatively smaller. We then come up with another weight function (with about $\log n$ bits) to break ties among these minimum weight perfect matchings and make the set even smaller. Further, we ensure that the older non-minimum weight matchings do not suddenly enter this family by appending the bits of the new weight function to the bits of the previous weight function. Each of these bit sequences we append are $\log n$ bits, and because there are $\log n$ rounds we end at $\log^2 n$ bits in all.

			As before, let the edges be $e_1,\ldots,e_m$.\\
			For starters, observe that if $w(e_i) = 2^i$ for all $i$, then all subsets have distinct weights.\\
			Let $M_1,M_2$ be two minimum weight perfect matchings. Observe that $M_1 \cup M_2$ is a union of cycles (and possibly isolated edges contained in both $M_1,M_2$). Further, each cycle in $M_1 \cup M_2$ has zero ``alternating weight''. This is the difference of the sum of all ``odd'' edges in the cycle and the sum of all ``even'' edges in the cycle. If we instead had that the $M_1$ sum was greater than the $M_2$ sum, we could switch out the edges in the cycle in $M_1$ for edges in the cycle in $M_2$ to get a matching of weight strictly less than that of $M_1$, yielding a contradiction.

			\begin{flem}
				\label{lem: union of perfect matchings}
				Let $E' \subseteq E$ be the union of all minimum weight perfect matchings. Then, each cycle in $G = (V,E')$ is has zero alternating weight.
			\end{flem}
			
			\begin{corollary}
				If $w$ is a weight assignment such that a cycle $C$ has nonzero alternating weight, then the union of minimum weight perfect weight matchings (with respect to $w$) does \emph{not} contain $C$.
			\end{corollary}

			The above corollary is the key idea. For a suitable weight assignment on a cycle, we can get rid of at least one edge in the cycle, and this ensures that all matchings containing that edge are rid of. Our goal then is to maximize the number of edge-disjoint cycles in the graph.\\
			Given a cycle $C$ and a weight assignment $w$, let $\aw(C)$ be the alternating weight of $C$ under $w$.

			\begin{fprop}
				Let $C_1,\ldots,C_k$ be an arbitrary collection of cycles. Then, for some $j \in \{1,2,\ldots,m^2k\}$, the weight function defined by $w(e_i) = 2^i \pmod j$ for each $i$ assigns a nonzero alternating weight to every cycle $C_r$.
			\end{fprop}
			\begin{proof}
				We would like to show that for some $j$, $j$ is not a factor of $\aw(C_1) \aw(C_1) \cdots \aw(C_k)$. This product is at most $2^{m^2k}$. Recalling that the lcm of the first $n$ numbers is greater than $2^n$ for sufficiently large $n$, we have that $2^{m^2k}$ is less than the lcm of $[m^2k]$, so there is some number in $[m^2k]$ that does not divide $2^{m^2k}$.
			\end{proof}

			Note that the list of weight assignments we give as above does not rqeuire knowledge of which cycles we are working with. That is, if we have polynomially many cycles, we can give a polynomially large list of weight assignments with the guarantee that one of these assignments removes all the cycles.

			% The overall process is the following. Set $G^{(0)} = G$. Consider a large set of edge-disjoint cycles, and assign weights $w^{(0)}$ to the edges in each cycle such that the cycle becomes non-alternating. Let $G^{(1)}$ to be the union of all minimum weight perfect matchings under $w^{(0)}$. More generally, given $G^{(i)}$, consider a large set of edge-disjoint cycles in the graph and assign weights $w^{(i)}$

		\subsubsection{Lecture 22}

			Now, we have exponentially many cycles that we must remove somehow, but each step seems to only remove polynomially many cycles. To take care of this, we can change our perspective by looking at the number of \emph{edges} removed in each step instead.

			\begin{flem}
				In a graph with $m$ edges and $n$ vertices, there exist at least $\Omega\left(\frac{m-n}{\log(m-n)}\right)$ edge-disjoint cycles.
			\end{flem}
			The idea of the proof is that any graph with $n$ vertices and $m$ edges has $O(\log(m-n))$ edges. Removing this cycle and then repeating until there are no cycles remaining gives the desired claim.
			% erdos posa?
			% More precisely, we may assume that every vertex has degree $3$ by contracting all degree $2$ vertices.\\
			This allows us to be done in $O(\log^2 n)$.\\
			An alternate approach uses the following lemma.

			\begin{flem}
				A graph with no cycles of length at most $4 \log n$ has average degree at most $5/2$.
			\end{flem}

			We have a third approach more efficient than the above two, however, which again returns to the idea of removing cycles.

			\begin{flem}[Teo, Koh]
				\label{lemma: teo koh}
				A graph $G$ with no cycles of length at most $r$ has are at most $n^4$ cycles of length at most $2r$. 
			\end{flem}

			In just $O(\log n)$ rounds, we can remove \emph{all} cycles! We start by removing all cycles of length at most $3$, then we remove all cycles of length at most $6$, then at most $12$, and so on. The above lemma tells us that each step is efficient.

			\begin{proof}[Proof of \Cref{lemma: teo koh}]
				Observe that for any two vertices, there is at most one path between them of length $\le (r/2)$. For each cycle, fix $4$ equidistant vertices on the cycle, and associate the cycle with the tuple $(u_1,u_2,u_3,u_4)$. The key of the argument is that by the observation, no two cycles have the same tuple!\\
				More precisely, suppose that two distinct cycles have the same tuple $(u_1,u_2,u_3,u_4)$. Note that between the two cycles, at least one of the intervals $(u_i,u_{i+1})$ must have different edges (otherwise, they would be the same cycle). This yields a contradiction to our observation.
			\end{proof}

			Now, all that remains is to prove \Cref{lem: union of perfect matchings}. Recall Hall's theorem, which states that for a bipartite graph $G = (X,Y,E)$, if $|N(S)| \ge |S|$ for every $S \subseteq X$, then $G$ has a perfect matching. In particular, this can be used to prove that a (non-empty) regular bipartite graph has a perfect matching. This in turn implies that a $d$-regular bipartite graph can be decomposed into the union of $d$ edge-disjoint perfect matchings. This holds true for multigraphs as well.

			\begin{proof}[Proof of \Cref{lem: union of perfect matchings}]
				Say there are $k$ minimum weigt perfect matchings, each of weight $\lambda$. As in the lemma statement, let $E'$ be the union of all minimum weight perfect matchings. Let $H$ be the multigraph equal to the disjoint union of all minimum weight perfect matchings. Note that $H$ is a $k$-regular bipartite (multi)graph. The total weight of the edges in $H$ is equal to $k\lambda$. Suppose that $H$ has a cycle $C$ of nonzero alternating weight. In $C$, remove the alternating set of edges of larger weight, and add the complimentary alternating set of edges to get a new graph $H'$. Note that $H'$ remains $k$-regular so can be decomposed into $k$ perfect matchings, and the total weight of edges in $H'$ is strictly less than $k\lambda$. This implies that some matching among these $k$ has weight less than $\lambda$, yielding a contradiction.
			\end{proof}