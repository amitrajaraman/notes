\section{Reed-Solomon Codes}
	% guest lecturer: prof mrinal

	\subsection{Lecture 11: Introduction}

		\subsubsection{Lecture 11}

			Randomly choosing strings from $\{0,1\}^n$ tends to yield a good code, with high distance between points with high probability. This is true even for exponentially many points, say $2^{0.1n}$. At the heart of getting good codes is derandomizing this process to get good explicit codes.\\
			In some sense, good codes, expander graphs, and extractors are equivalent. In fact, some of the best known expander constructions today come from coding theoretic constructions.\\
			The subject of coding theory lies at the intersection of numerous disparate fields, such as (theoretical) computer science, electrical engineering, and math. Interestingly, the same objects are studied in all the disciplines, merely from different perspectives.\\

			\begin{fdef}[Reed-Solomon Code]
				Let $\F$ be a finite field, and $k,n \in \N$ with $n \ge k,|\F|$. Also fix some distinct $\alpha_1,\ldots,\alpha_n$. The message space of the \emph{Reed Solomon code} $\RS(k,n)$ is
				\[ \{ g(x) \in \F[x] : \deg(g) \le k-1 \}. \]
				That is, we identify $k$-dimensional vectors in $\F^k$ with corresponding polynomials.
				A polynomial $g$ is encoded as
				\[ \Enc(g) = ( g(\alpha_1),\ldots,g(\alpha_n) ). \]
			\end{fdef}
			Note that the number of possible messages if $|\F|^k$. Let us look at some basic properties of this code.
			\begin{enumerate}
				\item A Reed-Solomon code is a linear code. This follows immediately from the fact that $(\alpha g+h)(\alpha_i) = \alpha g(\alpha_i) + h(\alpha_i)$, and if $g,h$ are of degree at most $k-1$ then so is $\alpha g + h$.
				\item The code has rate $k/n$.
				\item The distance of the code is $n-k+1$. Indeed, by the Fundamental Theorem of Algebra, two polynomials can coincide in value at at most $k-1$ points (otherwise their difference, a nonzero polynomial of degree at most $k-1$, would have more than $k-1$ roots).
			\end{enumerate}
			Observe that given a vector $(g_1,\ldots,g_k) \in \F^k$, we encode it as
			\[ \begin{pmatrix} 1 & \alpha_1 & \alpha_1^2 & \cdots & \alpha_1^{k-1} \\ 1 & \alpha_2 & \alpha_2^2 & \cdots & \alpha_2^{k-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & \alpha_n & \alpha_n^2 & \cdots & \alpha_n^{k-1} \end{pmatrix} \begin{pmatrix} g_1 & g_2 & \vdots & g_k \end{pmatrix}. \]
			Also note that the above properties do not depend on our choice of $(\alpha_i)$.\\

			It turns out that Reed-Solomon codes are optimal in some sense.
			\begin{ftheo}%[Singleton]
				Reed-Solomon codes match the singleton bound.
			\end{ftheo}
			This says that over large fields, they essentially match the best possible rate-distance trade-off.

	\subsection{Lecture 11-12: Decoding}

		\subsubsection{Lecture 11 (continued)}

			The matrix multiplication scheme above describes a simple way to encode RS codes. How do we decode them? That is, if $r = (r_1,\ldots,r_n) \in \F^n$, what do we decode it to? This asks to find the RS codeword $c$ closest to $r$; find \emph{the} RS codeword $c$ such that $d_H(c,r) < (n-k+1)/2$ (if no such codeword exists, say no). \\ %sometimes called unique decoding regime

			In the 60s, numerous decoding algorithms such as those of Peterson, Sugiyama, and Berlekamp-Massey were proposed. Despite being elementary, these are all very clever. In practice, Berlekamp-Massey is typically used (it is nearly linear in the input size).\\
			We shall look at the Welch-Berlekamp algorithm, which was proposed in the 80s. Amusingly, this was not originally published in a paper but in a patent. Some time later, Madhu Sudan et al. decoded\footnote{Pardon the pun.} the patent. The algorithm has had surprisingly pervasive effects in computer science, and the methods have helped resolve several open problems in math as well. Interested readers can see the ``polynomial method'' for more details; while it has been used in the past in math, this introduced it to mainstream theoretical computer science. \\
			The algorithm is as follows.
			% The question is just \emph{noisy} polynomial interpolation.

			\begin{enumerate}
				\item Find a nonzero polynomial $Q(x,y) = A(x) + yB(x)$ such that
				\begin{enumerate}
					\item $\deg(B) < \left\lceil (n-k+1)/2 \right\rceil \eqqcolon d$,
					\item $\deg(A) < d + (k-1) \eqqcolon D$, and
					\item for all $i$, $Q(\alpha_i,r_i) = 0$.
				\end{enumerate}
				\item Set $g(x) = -A(x)/B(x)$. If $g$ is a polynomial and $d_H(\Enc(g),r) < (n-k)/2$ and $\deg(g) \le k-1$, output $g$. Otherwise, say that no codeword exists.
			\end{enumerate}

			Observe that if $g$ is a polynomial, the polynomial is certainly correct.\\
			We must prove that if $g$ is not a polynomial, then no codeword exists. First, however, let us describe how to determine the bivariate polynomial $Q$.\\
			Set $D = (n+k)/2$ and $d = (n-k)/2$. The desired $Q$ is of the form
			\[ Q(x,y) = A_0 + A_1 x + \cdots + A_{D-1} x^{D-1} + B_0y + B_1xy + \cdots + B_{d-1} x^{d-1}. \]
			Then, we wish to determine coefficients $(A_i)$ and $(B_i)$ such that for all $i$
			\[ Q(x,y) = A_0 + A_1 \alpha_i + \cdots + A_{D-1} \alpha_i^{D-1} + B_0 r_i + B_1 r_i\alpha_i + \cdots + B_{d-1} r_i\alpha_i^{n-1} = 0. \]
			This is just a linear system of $n$ equations. If $D+d > n$, which is indeed true, we may solve it for a non-trivial solution.\\
			To compute $g$ using $A,B$, we just have to do polynomial long division from high school (there are better methods to do this).\\
			All that remains is to check correctness.

			\begin{ftheo}
				If there exists a polynomial $h \in \F[x]$ of degree at most $k-1$ such that $d_H(\Enc(h),r) < (n-k+1)/2$, the Welch-Berlekamp algorithms outputs it.
			\end{ftheo}
			\begin{proof}
				Let $Q(x,y) = A(x) + yB(x)$ with $\deg(A) < D$, $\deg(B) < d$, and $Q(\alpha_i,r_i) = 0$ for all $i$ be the polynomial output in the first step of the algorithm.\\

				Consider $U(x) = Q(x,h(x))$. This is $A(x) + h(x) B(x)$. For starters, the degree of $U$ is at most $D-1 < (n+k-1)/2$. If $h(\alpha_i) = r_i$, then $U(\alpha_i) = 0$. Consequently, the number of zeroes of $U$ on $\{\alpha_1,\ldots,\alpha_n\}$ is at least the number of agreements between $\Enc(h)$ and $r$. Due to the distance contraint on $h$, there are at least $(n+k-1)/2$ such agreements.\\
				Therefore, $U$ has more zeros than degree, so $U$ must be the zero polynomial (!) and therefore $g$ is indeed a polynomial and equal to $h$.
			\end{proof}
			The idea of the construction is just that the first step forces us to have a $Q$ of large degree, while the second (assuming a valid $h$ exists) forces $Q$ to have small degree. The sweet spot in the middle is precisely where we lie.\\

			Next class, we shall look at decoding Reed-Solomon codes beyond the error limit of half the minimum distance. One option is to output any codeword in the given radius. The more useful (albeit more stringent) notion is to output all these codewords. Such algorithms are called ``list decoding algorithms''.
			We must ensure that the amount we go beyond $(n-k+1)/2$ is not so high that the list becomes exponentially large. This is guaranteed by the following.

			\begin{ftheo}[Johnson]
				\label{johnson bound}
				Let $\mathcal{C}$ be a code of block length $n$ with distance $\Delta$. Then, for all $r \in \F^n$, the number of codewords in $\mathcal{C}$ within distance $n - \sqrt{n(n-\Delta)}$ (the ``Johnson radius'') of $r$ is $\poly(q,n,\Delta)$.
			\end{ftheo}

			For Reed-Solomon codes, this value is equal to
			\[ n - \sqrt{n\left( n - (n-k+1) \right)} \approx n-\sqrt{nk}. \]
			This is \emph{far} larger than the minimum distance. If $k = 0.01n$, say, then the minimum distance is about $0.49n$, but the Johnson radius is about $0.9n$! Next class, we shall describe how to output all the codewords within the prescribed radius.\\
			We also remark that there exist Reed-Solomon codes where we go beyond the Johnson radius while still having polynomially many codewords in the given radius. Such codes are not \emph{explicitly} known, however, and are obtained by taking a sufficiently large field and choosing random evaluatin points $\alpha_i$. % past year, recent ersult, add citation
			The question of when in general the Johnson bound is not tight does not have many satisfactory answers to date. In particular, we do not know if the Johnson bound is non-tight for \emph{all} Reed-Solomon codes.

		\subsubsection{Lecture 12}

			Given $\mathcal{S} = \{(\alpha_i,\beta_i)\}_{i=1}^n$, we aim to find \emph{all} polynomials $f \in \F_q[x]$, $\deg f < k$, such that
			\[ \agr(f,\mathcal{S}) \coloneqq \left|\left\{ u \in [n] : f(\alpha_i) = \beta_i \right\}\right| \ge t. \]
			Last lecture, we saw the setting where $t \ge (n+k-1)/2$.\\
			Due to \nameref{johnson bound}, the setting where $t \ge \sqrt{nk}$ enters consideration. We shall now look at how to decode Reed-Solomon codes up to the Johnson radius. \\ % two in 96, one in 98

			First, we shall look at the case where $t \ge 2\sqrt{nk}$. % Sudan 96, Sudan 96, Guruswami-Sudan
			The basic idea is that we take the Welsh-Berlekamp algorithm, but look at polynomials that are higher degree in $y$.

			\begin{enumerate}
				\item Set $\ell \approx \sqrt{n/(k-1)}$. Find nonzero $Q(x,y) = A_0(x) + A_1(x) + y^2 A_2(x) + \cdots y^\ell A_\ell(x)$ such that $\deg(A_i) \le n/\ell$ for each $i$, and $Q(\alpha_i,\beta_i) = 0$ for each $i$.
				\item Find all factors of $Q(x,y)$ of the form $(y - h(x))$, where $\deg(h) < k$ and $\agr(h,\mathcal{S}) \ge t$. Output all such $h$.
			\end{enumerate}
			This is very similar in spirit to the earlier algorithm. Indeed, $Q(x,f(x)) = 0$ just says that $(y-f(x))$ divides $Q(x,y)$. \\
			The first step of the algorithm is exactly as earlier and amounts to solving a system of linear equations. There is an algorithm, that factors polynomials of $\deg d$ on $\F_q$ in time $\poly(d,\log q)$.\footnote{This algorithm is randomized, and no such deterministic algorithm is known} This is rather interesting, as it means we are able to factorize elements of the polynomial ring. Compare this to the integer ring, where we cannot factorize elements efficiently. We do not describe this algorithm.

			\begin{proof}[Proof of correctness]
				The number of variables in the system of linear equations in the first step is $(\ell+1)\left(\frac{n}{\ell}+1\right) > n$, which is more than the number of datapoints, so such a $Q$ exists.\\
				Let $f \in \F[x]$ be of degree $<k$ and agree with $\mathcal{S}$ at more than $2\sqrt{nk}$ points. Then, we wish to show that $R(x) \coloneqq Q(x,f(x)) \equiv 0$. We have $\deg (R)\le (k-1)\ell + n\ell$. On the other hand, as before, if $f(\alpha_i) = \beta_i$, $R(\alpha_i) = Q(\alpha_i,\beta_i) = 0$. If the number of agreements is more than $\deg(R)$, we are done. That is,
				\[ t > \frac{n}{\ell} + (k-1)\ell. \]
				The quantity on the right is minimized for $2\sqrt{n(k-1)}$ and the corresponding $\ell$ is approximately $\sqrt{n/(k-1)}$.
			\end{proof}

			Now, let us modify the algorithm slightly to $\sqrt{2nk}$.
			\begin{enumerate}
				\item Set $D \approx \sqrt{2n(k-1)}$. Find nonzero $Q(x,y) = A_0(x) + A_1(x) + y^2 A_2(x) + \cdots y^{D/(k-1)} A_{D/(k-1)}(x)$ such that $\deg(A_i) \le D - (k-1)i$ for each $i$, and $Q(\alpha_i,\beta_i) = 0$ for each $i$.
				\item Find all factors of $Q(x,y)$ of the form $(y - h(x))$, where $\deg(h) < k$ and $\agr(h,\mathcal{S}) \ge t$. Output all such $h$.
			\end{enumerate}

			\begin{proof}[Proof of correctness]
				The key observation is that in the above argument, we can push the degree of most $A_i$ higher, without affecting the bound on the overall degree. Let $\deg A_i = D_i$, so the degree of $f^i A_i$ is at most $i(k-1) + D_i$. If we want the overall degree to be $D$, then we get $\deg(A_i) \le D - (k-1)i$.\\
				The new number of variables is approximately
				\[ \sum_{i=0}^{D/(k-1)} D - (k-1)i \approx \frac{D^2}{k-1} - \frac{D^2}{2(k-1)} = \frac{D^2}{2(k-1)}. \]
				So, we want a $D$ such that the above is greater than $D$.\\
				For the second part of the argument, we have $\deg(R) \le D$ by definition, so we are fine if $t > D$.\\
				Overall, this gives a bound of around $\sqrt{2n(k-1)}$.\\
			\end{proof}

			Finally, let us look at how to get a bound of $t \ge \sqrt{nk}$. This argument is slightly more involved than the short jump it took to get from $2\sqrt{nk}$ to $\sqrt{2nk}$.\\
			We shall begin with a brief discussion of the \emph{method of multiplicities}, which is something like the polynomial method on steroids.

			\begin{fdef}
				A polynomial $Q(x,y)$ is said to have a zero of multiplicity $\ge r$ at $(\alpha,\beta)$ if for all $i,j$ such that $i+j<r$,
				\[ \frac{\partial Q}{\partial x^i y^j} (\alpha,\beta) = 0. \]
			\end{fdef}

			\begin{enumerate}
				\item Set $D,r$ such that $D/r \approx \sqrt{n(k-1)}$. Find nonzero $Q(x,y) = A_0(x) + A_1(x) + y^2 A_2(x) + \cdots y^{D/(k-1)} A_{D/(k-1)}(x)$ such that $\deg(A_i) \le D - (k-1)i$ for each $i$, and $Q$ passes through $(\alpha_i,\beta_i)$ with multiplicity $r$.
				\item Find all factors of $Q(x,y)$ of the form $(y - h(x))$, where $\deg(h) < k$ and $\agr(h,\mathcal{S}) \ge t$. Output all such $h$.
			\end{enumerate}
			
			\begin{proof}[Proof of Correctness]
				The number of variables now remains $\frac{D^2}{2(k-1)}$, but the number of constraints has increased to about $\binom{r+1}{2}n$. So, we require
				\[ \frac{D^2}{2(k-1)} \ge \binom{r+1}{2} n. \]
				Approximately, this requires.
				\[ \frac{D^2}{r^2} \ge n(k-1). \]
				Now, due to our additional multiplicity constraints, if $f(\alpha_i) = \beta_i$, then $R(x) = Q(x,f(x))$ vanishes with multiplicity at least $r$ at $\alpha_i$. % maybe add argument for this
				Now, we have that there are at most $D/r$ such distinct $f$. The observation is that each point of agreement gives us $r$ zeros, not just one.\\
				We require
				\[ t \ge \frac{D}{r}. \]
				In all, this gives us the required bound $\sqrt{n(k-1)}$!
			\end{proof}
			% brushing under rug of derivative details


			We again draw attention to the part where we used the fact that a nonzero degree $d$ univariate polynomials has at most $d/r$ distinct zeros of multiplicity $\ge r$.
			% the PRECISE 2\sqrt{nk}, \sqrt{2nk}, \sqrt{nk} sequence frequently appears in all sorts of places (ex: approximating algebraic irrationals by rationals), and we have no idea why!
			Using this fact, we can consider another code.

			\begin{fdef}[Multiplicity code]
				Let $\F$ be a finite field of size at least $n$, $\alpha_1,\ldots,\alpha_n \in \F$. The message set is $\{f \in \F[x], \deg f < k\}$. We map $f$ to the $n$-dimensional vector $M$ over $\F^r$, where
				\[ (M_{i})_j = \frac{\partial^{j-1} f}{\partial x^{j-1}}(\alpha_i). \]
			\end{fdef}
			% Neilson in early 2000s. claim to fame is that multivariate analogue of multiplicity code has amazing local properties. what if we don't want to correct the whole thing and just want to know what is there at a single point? really efficient local decoding (don't even need to see the whole string)! Reed-Muller codes are multivariate Reed-Solomon. nearly linear rate, sublinear query complexity!
			% list-decodable WAY beyond Johnson-bound.

			The rate of this code is approximately $k/nr$, which is worse than before. The distance however, jumps up to $n - \frac{k-1}{r}$!\\
			A unique decoding algorithm for the multiplicity is very similar to Berlekamp-Welch, and we omit the details.\\
			Next class, we shall prove incredible list decoding results, namely that for any $\epsilon > 0$, for sufficiently large $r$, multiplicity codes can be efficiently decoded for agreement $(1+\epsilon)\frac{k}{r}$. We can get arbitrarily close to the (hard) bound!\\ % achieved decoding capacity!
			% not only can you decode, list size depends on constant!!! (only depends on epsilon)
			Recent state-of-the-art expander graphs are constructed using multiplicity codes! % but not reed-solomon codes