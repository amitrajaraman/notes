\section{Hardness v. Randomness}

\subsection{Lecture 13, 15: Hardness}

	\subsubsection{Lecture 13}

		A general question one can ask is this: for any polynomial-time randomized algorithm, is it possible to derandomize it (possibly using more space) to get a polynomial-time deterministic algorithm doing the same job?\\
		It has been shown that given a ``hard'' function, one can construct very good pseudorandom bits. However, no explicit hard functions are known.\\

		Consider the notion of \emph{worst case hardness}. For example, if we can show for some language that no algorithm that runs in $O(n^{10})$ can compute the output correctly on all inputs, then the language is hard in some sense.\\
		We also have the notion of \emph{average case hardness}. Here, if we can show for some language that no algorithm that runs in $O(n^{10})$ can compute the output correctly on more than $3/4$ of the inputs, then the language is hard in some sense.\\
		It is not too difficult to see that average case hardness is a stronger notion than worst case hardness.\\

		Problems that are average case hard yield good pseudorandom generators. Another question of concern is converting worst case hardness to average case hardness, which is done through error correcting codes.\\
		The rough idea behind the second question is the following. Error-correcting codes, when given two words as input that are close, make them far apart.

		\begin{fdef}[$\RP$]
			A language $L$ is said to be in $\RP$ if there is a randomized algorithm $\mathcal{A}$ running in polynomial time such that
			\begin{enumerate}
				\item for $x \in L$,
				\[ \Pr_{r} \left[\mathcal{A}(x,r) = \mathsf{yes}\right] \ge \frac{1}{2}. \]
				\item for $x \not\in L$,
				\[ \Pr_{r} \left[\mathcal{A}(x,r) = \mathsf{yes}\right] \ge 0. \]
			\end{enumerate}
		\end{fdef}
		% explain x, r?

		For example, the algorithm we saw in \Cref{subsec: connectivity} was in $\mathsf{RP}$.

		\begin{fdef}[$\BPP$]
			A language $L$ is said to be in $\BPP$ if there is a randomized algorithm $\mathcal{A}$ running in polynomial time such that
			\begin{enumerate}
				\item for $x \in L$,
				\[ \Pr_{r} \left[\mathcal{A}(x,r) = \mathsf{yes}\right] \ge \frac{2}{3}. \]
				\item for $x \not\in L$,
				\[ \Pr_{r} \left[\mathcal{A}(x,r) = \mathsf{yes}\right] \le \frac{1}{3}. \]
			\end{enumerate}
		\end{fdef}
		
		Here, by randomized algorithms, we mean probabilistic turing machines.\\

		Next, let us look at pseudorandom distributions. The goal of these is to find some universal set of random bits which we can substitute in place of the (ideally) uniformly random bits. \\
		Denote by $U_m$ the uniform distribution on $\{0,1\}^m$.\\
		The idea of this is that a distribution $D$ is pseudorandom (with respect to $\mathcal{A}$) if for the given algorithm $\mathcal{A}$, for all inputs $x$,
		\[ \left| \Pr_{r \sim U_m}[ B(x,r) = \mathsf{yes}] - \Pr_{r \sim D}[ B(x,r) = \mathsf{yes} ] \right| \le \epsilon. \]

		One neat way to think about algorithms \emph{with input} is \href{https://en.wikipedia.org/wiki/Boolean_circuit}{circuits}.\\
		In fact, any deterministic algorithm can be viewed as a family $(C_n)_{n \ge 1}$ of circuits, with $C_n$ computing the output correctly if the input is of size $n$. We can view a randomized algorithm as a deterministic one with two inputs, $x$ and $r$. The circuit then has $n+m$ input leaves, where $n$ is the size of the input $x$ and $m$ is the number of random bits $r$. If we fix the $x$ part of the input, we get a circuit in the remaining input, namely $r_1,\ldots,r_m$.

		\begin{fdef}[Pseudorandom distribution]
			A distribution $D$ on $\{0,1\}^m$ is called \emph{$(S,\epsilon)$-pseudorandom} if for any circuit $C$ on $m$ input gates and size at most $S$,
			\[ \left| \Pr_{r \sim U_m} [ C(r) = 1 ] - \Pr_{r \sim D} [C(r) = 1] \right| \le \epsilon. \]
		\end{fdef}
		When the above happens, we say that $D$ ``fools'' all circuits of size at most $S$. % An algorithm running in time $S_n$ can be viewed as a family of circuits $(C_n)$ of size $(S_n)$.

		% maybe change this to have G itself as the function.
		\begin{fdef}[Pseudorandom generator]
			A function $G : \{0,1\}^* \to \{0,1\}^*$ is said to be a \emph{$m(\ell)$-pseudorandom generator} if for $r \in \{0,1\}^\ell$,
			\begin{enumerate}[label=(\alph*)]
				\item $G(r) \in \{0,1\}^{m(\ell)}$,
				\item $G(r)$ can be computed in $2^{O(\ell)}$ time, and
				\item the distribution over $\{0,1\}^{m(\ell)}$ which takes a uniformly random element $s$ of $\{0,1\}^\ell$ and takes value $G(s)$ is $(m(\ell)^3 , 1/10)$-pseudorandom.
			\end{enumerate}
		\end{fdef}
		The existence of the above would imply that any randomized algorithm in $\mathsf{BPP}$ using $m$  random bits and running time $m^3$ can be simulated by a deterministic algorithm with running time $O(2^{O(\ell)}m^3)$. We merely run the algorithm on $G(r)$ for all $r \in \{0,1\}^\ell$, and output whatever answer (\textsf{yes} or \textsf{no}) occurs more. \\
		If we want to completely derandomize our randomized polynomial-time algorithm to get a deterministic polynomial time algorithm, we want that $m = 2^{\Omega(\ell)}$. In particular, if a pseudorandom generator with $\ell = O(\log m)$ exists, then $\mathsf{BPP} = \mathsf{P}$. When this is true, we say that the PRG has ``exponential stretch''. % if we have m^9 algo for instance, pad the input to make it m^3 size. \ell only increases by a constant factor, so its fine.
		% We don't know how to go from \ell to \ell+1, forget 2^{\ell}.

		Our goal in the next few lectures will be to show that ``circuit lower bounds'' imply the existence of pseudorandom generators.

		\begin{ftheo}
			There exists a ``PRG'' with exponential stretch which satisfies only (a) and (c) in the definition.
		\end{ftheo}
		\begin{proof}
			Choose a random $G$ -- for each $s \in \{0,1\}^\ell$, set $G(s)$ to be a uniformly random string in $\{0,1\}^{m}$.\\
			Fix a circuit $C$ of size at most $m^3$. Suppose that $\Pr_{r \sim U_m}[C(r) = 1] = p$, and let
			\[ B_C \coloneqq \{ r \in \{0,1\}^m : C(r) = 1 \}. \]
			Note that the random variable
			\[ X_C \coloneqq \left| \left\{ s \in \{0,1\}^\ell : G(s) \in B_C \right\} \right| \]
			is the sum of $2^{\ell}$ Bernoulli random variables equal to $1$ with probability $p$. By the Chernoff bound,
			\[ \Pr_G\left[ |X_C - \E[X_C]| > \epsilon 2^\ell \right] \le 2e^{-(\epsilon 2^\ell)^2/3\E[X_C]} = 2e^{-(\epsilon 2^\ell)^2 /(3p \cdot 2^{\ell})} \le 2e^{-\epsilon^2 2^{\ell} / 3}. \]
			Now, the number of circuits with size at most $m^3$ is bounded from above by $2^{3m^3}$. An application of the union bound yields that
			\[ \Pr_G[ \text{for some $C$ of size at most $m^3$, } |X_C - \E[X_C| > \epsilon 2^\ell ] \le e^{-(\epsilon^2 2^\ell/3) + (\ln 2)(3m^3 + 1)}. \]
			For $\epsilon = 1/10$, $\ell = 4\log m$, and sufficiently large $m$, the RHS is less than $1$, so there exists some $G$ that satisfies (a) and (c) in the definition of a PRG.
		\end{proof}

	\subsubsection{Lecture 15}

		In the previous lecture, we had said the following (after defining what a PRG is).

		\begin{ftheo}
			If a $2^{\Omega(\ell)}$-PRG exists, $\BPP = \PP$.
		\end{ftheo}
		
		Note that a $2^\ell$-PRG does not exist. Indeed, if it did, we could design a circuit that is $1$ precisely at each point in $\{0,1\}^{2^\ell}$ that is mapped to by the PRG, and is $0$ everywhere else.\\
		While no PRGs are known that fool \emph{all} circuits of size bounded by $m(\ell)^3$, there are PRGs known under more specific conditions on the circuit. For example, we can get a PRG that fools any randomized algorithm that is log-space.\footnote{This does not make sense in our current framework, but it is possible to modify the definition of PRGs appropriately. In this setting, we do not have exponential stretch, but we can go from $\Omega(\log^2 m)$ to $m$. The question of whether $\mathsf{RL} = \mathsf{L}$ is a huge open question.} It is also known that there exist (non-trivial) PRGs which fool constant-depth circuits. \\ % slightly better in fact, log depth or something.

		Now, what are \emph{circuit lower bounds}? We had remarked in the previous lecture that they imply the existence of PRGs.

		\begin{fdef}[Worst-case hardness]
			For $f : \{0,1\}^n \to \{0,1\}$, its \emph{worst-case hardness} $H_\text{worst}(f)$ is the largest number $S$ such that for any circuit of size at most $S$, there exists some $x \in \{0,1\}^n$ such that $C(x) \ne f(x)$. 
		\end{fdef}
		We cannot compute the function using a circuit of size any smaller than its worst-case hardness. The implementation of the truth table yields that the worst-case hardness of any function is at most about $O(2^n)$. \\
		
		Does there exist any function which is actually this hard? There are $2^{2^n}$ functions from $\{0,1\}^n \to \{0,1\}$, and there are (about) $2^S$ circuits of size at most $S$. Consequently, some functions do require an $S$ of at least about $2^n/n$.\\
		However, no such function is explicitly known -- this is another huge open question! In fact, the hardest explicit function we know has worst-case hardness just $3n - o(n)$. % One could of course guess that NP-hard questions (or exp-time questions) are this hard, but we do not know how to prove it.
		As mentioned at the beginning of this section, we can use (worst-case) hard functions to design good pseudorandom generators.

		\begin{fdef}[Average-case hardness]
			For $f : \{0,1\}^n \to \{0,1\}$, its \emph{average-case hardness} $H_\text{avg}(f)$ is the largest number $S$ such that for any circuit of size $S$,
			\[ \Pr_{x \sim U_n} \left[ C(x) \ne f(x) \right] < \frac{1}{2} + \frac{1}{2^{n/10}}. \]
			% amended from 1/S to 1/2^{n/10}.
		\end{fdef}
		Note that we can trivially get a circuit that is equal to $f$ with probability $\ge 1/2$, either set it as the constant $0$ or the constant $1$ (depending on which value $f$ takes more often).\\
		Clearly, the average-case hardness of any function is at least the worst-case hardness.

		\begin{ftheo}
			If (for sufficiently large $n$) there exists a function computable in time $2^{O(n)}$ with $H_\avg(f) \ge 2^{2n/3}$, then there exists a $(2^{\ell/45})$-PRG and in particular, $\BPP = \PP$.
		\end{ftheo}
		% there are also things called ``algorithm lower bounds'', but they are not as interesting.
		This links the worlds of algorithms (in the time complexity of $f$), circuits, and derandomization.\\

		Before moving to the proof of the above, let us try to go from $\ell$ to $\ell+1$.

		\begin{prop}
			Let $f : \{0,1\}^\ell \to \{0,1\}$ be such that $H_\avg(f) \ge \ell^4$. Then, $G$ defined by
			\[ G(r) = (r_1,\ldots,r_\ell,f(r)) = (r,f(r)) \]
			is a PRG.
		\end{prop}
		That is, we would like to say that the output of the function cannot be predicted for a given input. The above merely says that unpredictability implies indistinguishability.
		% \begin{proof}
		% 	We would like to show for any circuit of size $(\ell+1)^3$,
		% 	\[ \left| \Pr_{r \sim U_{\ell}} [C(r,f(r)) = 1] - \Pr_{r \sim U_{\ell+1}}[C(s) = 1] \right| \le \frac{1}{10}. \]

		% \end{proof}

		% take a bunch of subsets of size n/10 of [n] with low intersections, apply a func \{0,1\}^{n/10} \to \{0,1\} on each of them to add a few more bits.

		\begin{ftheo}
			Let $D$ be a distribution on $\{0,1\}^m$. Suppose that for any $i$ and any circuit of size $2S$,
			\[ \Pr_{y \sim D} [C(y_1,\ldots,y_i) = y_{i+1}] < \frac{1}{2} + \epsilon. \]
			Then, for any circuit $B$ of size $S$,
			\[ \left| \Pr_{y \sim D} [B(y) = 1] - \Pr_{y \sim U_m}[B(y) = 1] \right| < m\epsilon. \]
		\end{ftheo}