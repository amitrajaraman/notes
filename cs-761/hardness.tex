\section{Hardness v. Randomness}

\subsection{Lecture 13, 15: Hardness}

	\subsubsection{Lecture 13}

		A general question one can ask is this: for any polynomial-time randomized algorithm, is it possible to derandomize it (possibly using more space) to get a polynomial-time deterministic algorithm doing the same job?\\
		It has been shown that given a ``hard'' function, one can construct very good pseudorandom bits. However, no explicit hard functions are known.\\

		Consider the notion of \emph{worst case hardness}. For example, if we can show for some language that no algorithm that runs in $O(n^{10})$ can compute the output correctly on all inputs, then the language is hard in some sense.\\
		We also have the notion of \emph{average case hardness}. Here, if we can show for some language that no algorithm that runs in $O(n^{10})$ can compute the output correctly on more than $3/4$ of the inputs, then the language is hard in some sense.\\
		It is not too difficult to see that average case hardness is a stronger notion than worst case hardness.\\

		Problems that are average case hard yield good pseudorandom generators. Another question of concern is converting worst case hardness to average case hardness, which is done through error correcting codes.\\
		The rough idea behind the second question is the following. Error-correcting codes, when given two words as input that are close, make them far apart.

		\begin{fdef}[$\RP$]
			A language $L$ is said to be in $\RP$ if there is a randomized algorithm $\mathcal{A}$ running in polynomial time such that
			\begin{enumerate}
				\item for $x \in L$,
				\[ \Pr_{r} \left[\mathcal{A}(x,r) = \mathsf{yes}\right] \ge \frac{1}{2}. \]
				\item for $x \not\in L$,
				\[ \Pr_{r} \left[\mathcal{A}(x,r) = \mathsf{yes}\right] \ge 0. \]
			\end{enumerate}
		\end{fdef}
		% explain x, r?

		For example, the algorithm we saw in \Cref{subsec: connectivity} was in $\mathsf{RP}$.

		\begin{fdef}[$\BPP$]
			A language $L$ is said to be in $\BPP$ if there is a randomized algorithm $\mathcal{A}$ running in polynomial time such that
			\begin{enumerate}
				\item for $x \in L$,
				\[ \Pr_{r} \left[\mathcal{A}(x,r) = \mathsf{yes}\right] \ge \frac{2}{3}. \]
				\item for $x \not\in L$,
				\[ \Pr_{r} \left[\mathcal{A}(x,r) = \mathsf{yes}\right] \le \frac{1}{3}. \]
			\end{enumerate}
		\end{fdef}
		
		Here, by randomized algorithms, we mean probabilistic turing machines.\\

		Next, let us look at pseudorandom distributions. The goal of these is to find some universal set of random bits which we can substitute in place of the (ideally) uniformly random bits. \\
		Denote by $U_m$ the uniform distribution on $\{0,1\}^m$.\\
		The idea of this is that a distribution $D$ is pseudorandom (with respect to $\mathcal{A}$) if for the given algorithm $\mathcal{A}$, for all inputs $x$,
		\[ \left| \Pr_{r \sim U_m}[ B(x,r) = \mathsf{yes}] - \Pr_{r \sim D}[ B(x,r) = \mathsf{yes} ] \right| \le \epsilon. \]

		One neat way to think about algorithms \emph{with input} is \href{https://en.wikipedia.org/wiki/Boolean_circuit}{circuits}.\\
		In fact, any deterministic algorithm can be viewed as a family $(C_n)_{n \ge 1}$ of circuits, with $C_n$ computing the output correctly if the input is of size $n$. We can view a randomized algorithm as a deterministic one with two inputs, $x$ and $r$. The circuit then has $n+m$ input leaves, where $n$ is the size of the input $x$ and $m$ is the number of random bits $r$. If we fix the $x$ part of the input, we get a circuit in the remaining input, namely $r_1,\ldots,r_m$.

		\begin{fdef}[Pseudorandom distribution]
			A distribution $D$ on $\{0,1\}^m$ is called \emph{$(S,\epsilon)$-pseudorandom} if for any circuit $C$ on $m$ input gates and size at most $S$,
			\[ \left| \Pr_{r \sim U_m} [ C(r) = 1 ] - \Pr_{r \sim D} [C(r) = 1] \right| \le \epsilon. \]
		\end{fdef}
		When the above happens, we say that $D$ ``fools'' all circuits of size at most $S$. % An algorithm running in time $S_n$ can be viewed as a family of circuits $(C_n)$ of size $(S_n)$.

		% maybe change this to have G itself as the function.
		\begin{fdef}[Pseudorandom generator]
			A function $G : \{0,1\}^* \to \{0,1\}^*$ is said to be a \emph{$m(\ell)$-pseudorandom generator} if for $r \in \{0,1\}^\ell$,
			\begin{enumerate}[label=(\alph*)]
				\item $G(r) \in \{0,1\}^{m(\ell)}$,
				\item $G(r)$ can be computed in $2^{O(\ell)}$ time, and
				\item the distribution over $\{0,1\}^{m(\ell)}$ which takes a uniformly random element $s$ of $\{0,1\}^\ell$ and takes value $G(s)$ is $(m(\ell)^3 , 1/10)$-pseudorandom.
			\end{enumerate}
		\end{fdef}
		The existence of the above would imply that any randomized algorithm in $\mathsf{BPP}$ using $m$  random bits and running time $m^3$ can be simulated by a deterministic algorithm with running time $O(2^{O(\ell)}m^3)$. We merely run the algorithm on $G(r)$ for all $r \in \{0,1\}^\ell$, and output whatever answer (\textsf{yes} or \textsf{no}) occurs more. \\
		If we want to completely derandomize our randomized polynomial-time algorithm to get a deterministic polynomial time algorithm, we want that $m = 2^{\Omega(\ell)}$. In particular, if a pseudorandom generator with $\ell = O(\log m)$ exists, then $\mathsf{BPP} = \mathsf{P}$. When this is true, we say that the PRG has ``exponential stretch''. % if we have m^9 algo for instance, pad the input to make it m^3 size. \ell only increases by a constant factor, so its fine.
		% We don't know how to go from \ell to \ell+1, forget 2^{\ell}.

		Our goal in the next few lectures will be to show that ``circuit lower bounds'' imply the existence of pseudorandom generators.

		\begin{ftheo}
			There exists a ``PRG'' with exponential stretch which satisfies only (a) and (c) in the definition.
		\end{ftheo}
		\begin{proof}
			Choose a random $G$ -- for each $s \in \{0,1\}^\ell$, set $G(s)$ to be a uniformly random string in $\{0,1\}^{m}$.\\
			Fix a circuit $C$ of size at most $m^3$. Suppose that $\Pr_{r \sim U_m}[C(r) = 1] = p$, and let
			\[ B_C \coloneqq \{ r \in \{0,1\}^m : C(r) = 1 \}. \]
			Note that the random variable
			\[ X_C \coloneqq \left| \left\{ s \in \{0,1\}^\ell : G(s) \in B_C \right\} \right| \]
			is the sum of $2^{\ell}$ Bernoulli random variables equal to $1$ with probability $p$. By the Chernoff bound,
			\[ \Pr_G\left[ |X_C - \E[X_C]| > \epsilon 2^\ell \right] \le 2e^{-(\epsilon 2^\ell)^2/3\E[X_C]} = 2e^{-(\epsilon 2^\ell)^2 /(3p \cdot 2^{\ell})} \le 2e^{-\epsilon^2 2^{\ell} / 3}. \]
			Now, the number of circuits with size at most $m^3$ is bounded from above by $2^{3m^3}$. An application of the union bound yields that
			\[ \Pr_G[ \text{for some $C$ of size at most $m^3$, } |X_C - \E[X_C| > \epsilon 2^\ell ] \le e^{-(\epsilon^2 2^\ell/3) + (\ln 2)(3m^3 + 1)}. \]
			For $\epsilon = 1/10$, $\ell = 4\log m$, and sufficiently large $m$, the RHS is less than $1$, so there exists some $G$ that satisfies (a) and (c) in the definition of a PRG.
		\end{proof}

	\subsubsection{Lecture 15}

		In the previous lecture, we had said the following (after defining what a PRG is).

		\begin{ftheo}
			If a $2^{\Omega(\ell)}$-PRG exists, $\BPP = \PP$.
		\end{ftheo}
		
		Note that a $2^\ell$-PRG does not exist. Indeed, if it did, we could design a circuit that is $1$ precisely at each point in $\{0,1\}^{2^\ell}$ that is mapped to by the PRG, and is $0$ everywhere else.\\
		While no PRGs are known that fool \emph{all} circuits of size bounded by $m(\ell)^3$, there are PRGs known under more specific conditions on the circuit. For example, we can get a PRG that fools any randomized algorithm that is log-space.\footnote{This does not make sense in our current framework, but it is possible to modify the definition of PRGs appropriately. In this setting, we do not have exponential stretch, but we can go from $\Omega(\log^2 m)$ to $m$. The question of whether $\mathsf{RL} = \mathsf{L}$ is a huge open question.} It is also known that there exist (non-trivial) PRGs which fool constant-depth circuits. \\ % slightly better in fact, log depth or something.

		Now, what are \emph{circuit lower bounds}? We had remarked in the previous lecture that they imply the existence of PRGs.

		\begin{fdef}[Worst-case hardness]
			For $f : \{0,1\}^n \to \{0,1\}$, its \emph{worst-case hardness} $H_\text{worst}(f)$ is the largest number $S$ such that for any circuit of size at most $S$, there exists some $x \in \{0,1\}^n$ such that $C(x) \ne f(x)$. 
		\end{fdef}
		We cannot compute the function using a circuit of size any smaller than its worst-case hardness. The implementation of the truth table yields that the worst-case hardness of any function is at most about $O(2^n)$. \\
		
		Does there exist any function which is actually this hard? There are $2^{2^n}$ functions from $\{0,1\}^n \to \{0,1\}$, and there are (about) $2^S$ circuits of size at most $S$. Consequently, some functions do require an $S$ of at least about $2^n/n$.\\
		However, no such function is explicitly known -- this is another huge open question! In fact, the hardest explicit function we know has worst-case hardness just $3n - o(n)$. % One could of course guess that NP-hard questions (or exp-time questions) are this hard, but we do not know how to prove it.
		As mentioned at the beginning of this section, we can use (worst-case) hard functions to design good pseudorandom generators.

		\begin{fdef}[Average-case hardness]
			For $f : \{0,1\}^n \to \{0,1\}$, its \emph{average-case hardness} $H_\text{avg}(f)$ is the largest number $S$ such that for any circuit of size $S$,
			\[ \Pr_{x \sim U_n} \left[ C(x) \ne f(x) \right] > \frac{1}{2} + \frac{1}{S}. \]
			% amended from 1/S to 1/2^{n/10}.
		\end{fdef}
		Note that we can trivially get a circuit that is equal to $f$ with probability $\ge 1/2$, either set it as the constant $0$ or the constant $1$ (depending on which value $f$ takes more often).\\
		Clearly, the average-case hardness of any function is at least the worst-case hardness.

\subsection{Lectures 15, 16: Average-case hardness to derandomization}

	\subsubsection{Lecture 15 (contd.)}

		\begin{ftheo}[Nisan-Wigderson]
			\label{theo: hard to prg}
			If (for sufficiently large $n$) there exists a function computable in time $2^{O(n)}$ with $H_\avg(f) \ge 2^{2n/3}$, then there exists a $(2^{\ell/45})$-PRG and in particular, $\BPP = \PP$.
		\end{ftheo}
		% there are also things called ``algorithm lower bounds'', but they are not as interesting.
		This links the worlds of algorithms (in the time complexity of $f$), circuits, and derandomization.\\

		Before moving to the proof of the above, let us try to go from $\ell$ to $\ell+1$.

		\begin{prop}
			Let $f : \{0,1\}^\ell \to \{0,1\}$ be such that $H_\avg(f) \ge \ell^4$. Then, $G$ defined by
			\[ G(r) = (r_1,\ldots,r_\ell,f(r)) = (r,f(r)) \]
			is a PRG.
		\end{prop}
		That is, we would like to say that the output of the function cannot be predicted for a given input. The above merely says that unpredictability implies indistinguishability.
		% \begin{proof}
		% 	We would like to show for any circuit of size $(\ell+1)^3$,
		% 	\[ \left| \Pr_{r \sim U_{\ell}} [C(r,f(r)) = 1] - \Pr_{r \sim U_{\ell+1}}[C(s) = 1] \right| \le \frac{1}{10}. \]

		% \end{proof}

		% take a bunch of subsets of size n/10 of [n] with low intersections, apply a func \{0,1\}^{n/10} \to \{0,1\} on each of them to add a few more bits.

		\begin{ftheo}[Yao's Theorem]
			\label{yao}
			Let $D$ be a distribution on $\{0,1\}^m$. Suppose that for any $i$ and any circuit of size $2S$,
			\[ \Pr_{y \sim D} [C(y_1,\ldots,y_i) = y_{i+1}] < \frac{1}{2} + \epsilon. \]
			Then, for any circuit $B$ of size $S$,
			\[ \left| \Pr_{y \sim D} [B(y) = 1] - \Pr_{y \sim U_m}[B(y) = 1] \right| < m\epsilon. \]
		\end{ftheo}

	\subsubsection{Lecture 16}

		\begin{proof}[Proof of \nameref{yao}]
			We shall show the contrapositive of the statement. Let $B$ be a circuit of size $S$ such that
			\[ \Pr_{y \sim D} [B(y) = 1] - \Pr_{y \sim U_m}[B(y) = 1] \ge m\epsilon. \]
			We remove the modulus because we can consider the probability that it is $0$ otherwise. Define a sequence of distributions $D_0,D_1,\ldots,D_m$ as follows, where $D_i$ is obtained by drawing $x$ from $D$, and then replacing the first $i$ coordinates with draws from $U_m$. That is, a draw is $(y_1,\ldots,y_i,z_{i+1},\ldots,z_m)$, where $y \sim D$ and $z \sim U_m$. Note that $D_0 = U_m$ and $D_m = D$. Let
			\[ P_i = \Pr_{r \sim D_i} [B(r) = 1]. \]
			Because $P_m - P_0 \ge m\epsilon$, there is some $i$ such that $P_i - P_{i-1} \ge \epsilon$. % That is,
			% \[ \Pr_{\substack{y \sim D \\ z \sim U_m}} \left[B(y_1,\ldots,y_i,z_{i+1},\ldots,z_m) = 1\right] - \Pr_{\substack{y \sim D \\ z \sim U_m}} \left[B(y_1,\ldots,y_{i-1},z_{i},\ldots,z_m) = 1\right] \ge \epsilon. \]
			Note that $D_i$ and $D_{i-1}$ differ only at the $i$th bit.\\
			% We shall try to synthesise a circuit that takes $y_1,\ldots,y_{i-1}$ and predicts $y_i$ with some decent probability.\\
			% Now, we have
			% \[ \E_{z \sim U_m} \left[ \Pr_{\substack{y \sim D \\ z \sim U_m}} \left[B(y_1,\ldots,y_i,z_{i+1},\ldots,z_m) = 1\right] - \Pr_{\substack{y \sim D \\ z_i \sim U_1}} \left[B(y_1,\ldots,y_{i-1},z_{i},\ldots,z_m) = 1\right] \right] \ge \epsilon. \]
			% Therefore, there exists some choice of $z_{i+1},\ldots,z_{m}$ such that the inner difference of probabilities is at least $\epsilon$ -- fix these to get a reduced circuit $B'$. We then have
			% \[ \Pr_{y \sim D} \left[B'(y_1,\ldots,y_i) = 1\right] - \Pr_{\substack{y \sim D \\ z_i \sim U_1}} \left[B'(y_1,\ldots,y_{i-1},z_{i}) = 1 \right] \ge \epsilon. \]
			We shall give an algorithm to predict $y_i$ given $y_1,\ldots,y_{i-1}$ for $y \sim D$ and a random choice of $z \sim U_m$. If $B(y_1,\ldots,y_{i-1},z_{i},\ldots,z_m)$, then output $z_i$, and if it is $0$ then output $1-z_i$. For the sake of succinctness, let $x = (y_1,\ldots,y_{i-1},z_i,\ldots,z_m)$. Now, the probability of success is
			\[ \frac{1}{2} \left( \underbrace{\Pr[B(x) = 1 \mid y_i = z_i]}_{P_i} + \underbrace{\Pr[B(x) = 0 \mid y_i = 1-z_i]}_{(1-\alpha)\text{, say}} \right) \]
			We have
			\[ P_{i-1} = \Pr[B(x) = 1] = \frac{1}{2} \left( \Pr[B(x) = 1 \mid y_i = z_i] + \Pr[B(x) = 1 \mid y_i = 1-z_i] \right) = \frac{1}{2} (P_i + \alpha). \]
			Therefore,
			\[ \text{probability of success} = \frac{1}{2}(P_i + 1-\alpha) = \frac{1}{2} + P_i - P_{i-1} \ge \frac{1}{2} + \epsilon. \]
			To get the final circuit $C$, note that on a random choice of $z \sim U_m$ in our ``algorithm'', we succeed with probability at least $(1/2)+\epsilon$. Therefore, there exists some specific choice which gives a probability of success at least $(1/2)+\epsilon$, which is precisely what we want.
		\end{proof}


		Let us now come to the proof of \Cref{theo: hard to prg}. The idea is as follows. We would like to consider a bunch of subsets of $[\ell]$, and apply a hard function $f$ to each of them to get one extra bit to append. In all, the number of bits we append is the number of subsets we choose. If we choose all subsets to be disjoint, then the resulting new bits are completely independent of each other, but we do not get exponentially many new bits. Therefore, we allow some small amount of intersection of the subsets, and thus some small amount of correlation, without compromising the uncorrelation of the new bits by too much.

		\begin{fdef}
			An \emph{$(l,k,d)$-combinatorial design} is a collection $I_1,\ldots,I_r$ of size $k$ subsets such that for distinct $i,j \in [r]$, $|I_i \cap I_j| \le d$.
		\end{fdef}

		\begin{fprop}
			For $k=\ell/30, d=k/3$, there exists an $(l,k,d)$-design of size at least $2^{d/10} \ge 2^{\ell/900}$.
		\end{fprop}
		We do not prove this.

		\begin{proof}[Proof of \nameref{theo: hard to prg}]
			Set $\ell = 900 \log n$, and $k$ as from the above.\\
			Fix some combinatorial design $\mathcal{I} = \{I_1,\ldots,I_n\}$ guaranteed by the above, and let $f : \{0,1\}^k \to \{0,1\}$ be a hard function. Then, the final pseudorandom bits we output are $f(z_{I_r})$ for each $r \in [n]$. For simplicity, denote $f(I_r) = f(z_{I_r})$. \\
			Let $f$ be computable in time $2^{O(k)}$ and $H_\avg(f) \ge 2^{2k/3}$. Denote the resulting PRG by $\NW_{\mathcal{I}}^f$. We shall show that $\NW_{\mathcal{I}}^f(U_\ell)$ is $(2^{2k/3}/2 , 1/10)$-pseudorandom, which is $(n^{20}/2, 1/10)$-pseudorandom. \\
			Now, we shall use \nameref{yao}, by showing unpredictability instead. That is, we are done if we show that for any circuit $C$ of size at most $n^{20}$,
			\[ \Pr_{z \sim U_\ell}\left[ C(f(z_{I_1}),\ldots,f(z_{I_{i-1}})) = f(z_{I_i}) \right] \le \frac{1}{2} + \frac{\epsilon}{n}. \]
			Let $f_j(z) = f(z_{I_j})$ for each $j$.\\
			Suppose otherwise. Let $z' = z_{[\ell] \setminus I_i}$, and $z'' = z_{I_i}$, so
			\[ \Pr_{z \sim U_\ell}\left[ C(f_1(z',z''),\ldots,f_{i-1}(z',z'')) = f(z'') \right] > \frac{1}{2} + \frac{\epsilon}{n}. \]
			Ignore $z'$ for now, fixing their values as something (this will be done in precisely the same way as in Yao's Theorem), abuse notation to denote the new functions by $f_j$ as well. Then,
			\[ \Pr_{z \sim U_\ell}\left[ C(f_1(z''),\ldots,f_{i-1}(z'')) = f(z'') \right] > \frac{1}{2} + \frac{\epsilon}{n}. \]
			using this, we get a circuit for $f$ that succeeds with probability at least $(1/2)+\frac{\epsilon}{n}$. Note that each $f_{j}(z'')$ uses at most $d$ bits. By taking $(i-1)$ trivial circuits of $f$, which are each of size at most about $d 2^d$, we get a circuit for $f$ of size $d 2^d 2^{d/10} + 2^{2d}/2 \le 2^{2d}$, contradicting the hardness of $f$.
		\end{proof}

\subsection{Lecture 17: Worst-case hardness to average-case hardness}

	\subsubsection{Lecture 17}

		Now, we would like to convert a function that is worst-case hard to some other function that is average-case hard.

		\begin{fdef}
			Given $0 < \rho < 1$ and a function $f : \{0,1\}^n \to \{0,1\}$, define $H_\avg^\rho(f)$ to be the largest $S$ such that for any circuit $C$ of size $S$,
			\[ \Pr_{x \sim U_m} [f(x) = C(x)] < \rho. \]
		\end{fdef}

		In particular, we have $H_\worst(f) = H_\avg^1(f)$.\\

		Given a function $f : \{0,1\}^n \to \{0,1\}$ with worst-case hardness $H_\worst(f)$, we may view it as a string $\ttbl(f)$ in $\{0,1\}^{2^n}$. For any circuit $S$ of size less than $H_\worst(f)$, we have $\ttbl(f) \ne \ttbl(C)$. This idea of using a few inequalities to generate a large number of inequalities is reminiscent of error correcting codes -- maybe we can convert the truth table to an element of $\{0,1\}^{2^{2n}}$, say, using an encoder $E$ of relative distance $1/4$, and our new average-case hard function $f'$ is defined by $\ttbl(f') = E(\ttbl(f))$.\\
		However, this trick does not allow us to show that \emph{all} circuits differ from $f'$ on a constant fraction! It only does so for circuits whose truth table is equal to some image of the encoding function. \\
		Can we do something in the backward direction? Given a circuit $B$ on $2n$ inputs of size $S^{1/5}$ (say) that agrees with $f'$ significantly (on a $\frac{1}{2} + \frac{1}{S}$ fraction), is it possible to construct a small circuit $C$ on $n$ inputs of size $S$ that agrees with $f$ everywhere?\\

		Let us do something slightly weaker, and suppose that $B$ agrees with $f'$ on a $0.9$ fraction. This essentially asks us to decode the code. However, the size of the input string to the decoding algorithm is $2^{2n}$, so we need a very fast decoding algorithm.\\
		In usual decoding, we look at the entire corrupted encoded string, and try to retrieve the entire message. In \emph{local decoding}, we read a small part of the corrupted string to recover some portion of the original message. Indeed, finally, we do not want to know the values of $f$ at \emph{every} point, only at the input point $x$.\\
		We can compute the value of $B$ at this small part, and use it to recover the specific bit corresponding to $f(x)$.
		% any algo is a circuit, not converse!

		\begin{fdef}[Local decoding]
			Let $E : \{0,1\}^N \to \{0,1\}^M$ be a encoder that runs in $\poly(N)$. A \emph{local decoder} for handling $\rho$ errors is an algorithm $D$ such that given random access to a string $y \in \{0,1\}^M$ with $\Delta(y,E(x)) \le \rho$ for some $x \in \{0,1\}^n$ and an index $j \in [N]$, runs in time $\polylog(M)$ and outputs $x_j$ with probability at least $2/3$.
		\end{fdef}

		\begin{ftheo}
			Suppose we have an encoder $E$ with a local decoder $D$ for handling $\rho$ errors. Further suppose we have functions $f_n : \{0,1\}^n \to \{0,1\}$ in $\EXP$ that have worst-case hardness $H_\worst(f) \ge S(n)$. Then, there exists $\epsilon > 0$ and $\hat{f}_{m} \in \EXP$ on $m$ bits such that
			\[ H_\avg^{1-\rho}(\hat{f}_n) \ge (S (2 \epsilon m))^{\epsilon}. \]
		\end{ftheo}
		The proof is exactly as in the preceding paragraphs. We have $(\hat{f}(x))_{x \in \{0,1\}^m} = E( (f(x))_{x \in \{0,1\}^n} )$ Let $N = 2^n$ and $M = 2^m$. Suppose instead that $B$ is a circuit of size  $T = (S(\epsilon m))^\epsilon$ such that
		\[ \Pr_{x \sim U_m}[B(x) = f(x)] \ge 1 - \rho. \]
		Now, we have that
		\[ \Delta( ( B(x) )_{x \in \{0,1\}^m} , ( \hat{f}(x) )_{x \in \{0,1\}^m} ) < \rho. \]
		Fix $x$ to be the string in $\{0,1\}^n$ that we wish to obtain the value of (of $f$). Each time we compute $B(y)$ for some $y$, we incur a time of $T$.\\
		We still need to eliminate the randomness in the local decoding algorithm. To do this, modify the encoding algorithm by repeating it enough times that the error probability is less than $1/N^2$. Then, for a fixed $f,x$, this implies that there exists some \emph{fixed} set of random bits which outputs $f(x)$ correctly for all $x \in [N]$.
		% lec 18
		% It is possible to go from functions with high $H_\avg^{1-\rho}$ to functions which are average-case hard, but we omit the details.\\

\subsection{Lecture 18--19: Local decoding}

	\subsubsection{Lecture 18}

		In this lecture, we elaborate a bit more on local decoding. Recall Reed-Solomon codes from \Cref{reed-solomon-codes}. Are these locally decodable?\\
		Given a polynomial
		\[ m = a_0 + a_1x + \cdots + a_{d-1}x^{d-1} \]
		and $(m(\alpha_1),m(\alpha_2),\ldots,m(\alpha_n))$, can we recover a specific $a_i$ by looking at a few of the $m(\alpha_i)$? They do not seem very suitable for local decoding, so let us look at some other codes that are more amenable to this.

		\begin{fdef}[Reed-Muller codes]
			Let $\F$ be a finite field, and $\ell,d \in \N$ such that $|\F| > d$. Also fix $S_1,\ldots,S_\ell \subseteq \F$ The message space of the \emph{Reed Solomon code} $\RM(n,\ell,d)$ is
			\[ \{ p(x_1,\ldots,x_\ell) \in \F[x_1,\ldots,x_\ell] : \deg(p) \le d \}. \]
			A polynomial $p$ is encoded as
			\[ \Enc(p) = (p(\alpha_1,\ldots,\alpha_\ell))_{\alpha \in (S_1 \times \cdots \times S_\ell)}. \]
		\end{fdef}

		In our setting, we fix all the $S_i$ to be $\F$.\\
		That is, the encoding goes from $\F^{\binom{d+\ell}{\ell}}$ to $\F^{|\F|^\ell}$. What is the distance of this code? Given a nonzero polynomial $p$ over $\ell$ variables of degree at most $d$, what is the largest number of zeros it can have?

		\begin{fprop}
			Any polynomial $p \in \F[x_1,\ldots,x_\ell]$ of degree at most $d$ has at most $d |\F|^{\ell-1}$ zeros. That is,
			\[ \Pr_{\alpha \sim \F^\ell} [p(\alpha) = 0] \le \frac{d}{|\F|} \]
		\end{fprop}
		\begin{proof}
			Assume wlog that the degree of $p$ is $d$. The idea is that we will partition $\F^\ell$ into a bunch of ``lines'' and show that on each line, the probability is at most $d/|\F|$. For $\alpha \in \F^\ell, r \in \F^\ell$, consider the line
			\[ L_{\alpha,r} = \{ \alpha + tr : t \in \F \}. \]
			We shall show that for some clever choice of $r$, the polynomial does not become the zero polynomial on this line for any $\alpha$. Restricted to this line, the function becomes a polynomial in $t$. We want to show that this is a nonzero polynomial
			\[ p(\alpha_1 + tr_1,\ldots,\alpha_\ell + tr_\ell). \]
			in $t$. Let $P_d$ be the degree $d$ part of $P$, and note that the coefficient of $t^d$ in this polynomial is $P_d(r_1,\ldots,r_\ell)$, \emph{independent of $\alpha$}! Further, $P_d$ cannot be identically zero on $\F^\ell$ because this would imply that the degree of $p$ is less than $d$ (this uses the fact that $|\F| > d$). Therefore, the polynomial is nonzero for some choice of $r$. This means that the univariate polynomial is nonzero, so has at most $d/|\F|$ zeros, and we are done.
		\end{proof}

		Are the Reed-Muller codes locally decodable? Let us change our perspective slightly, changing the message space from the coefficients to the evaluations of $\F$ at some $\binom{\ell+d}{d}$ (fixed and specific) points -- there exists a choice of such points which uniquely determines the polynomial.\\
		In the absence of errors, this makes local decoding trivial. What do we do in the presence of errors?\\
		Suppose we want to evaluate the polynomial at some point $\beta$ given the evaluations at all points (with an $\epsilon$ fraction of errors). If we manage to come up with some line through $\beta$ that has relatively few errors, then we can use Reed-Solomon decoding on this line to compute what $p(\beta)$ is precisely. Suppose that we choose this line randomly. Then, the expected number of errors is
		\begin{align*}
			\E_{\text{random line $\ell$ through $\beta$}}[\text{number of corruptions on $\ell$}] &= \indic_{\text{error at $\beta$}} \frac{1}{(|\F|^\ell-1)/(|\F|-1)} (\text{number of errors not at $\beta$}) \\
				&\le 1 + \epsilon|\F|.
		\end{align*}
		Therefore, by a Markov argument,
		\[ \Pr_{\ell} [\text{$\ell$ has less than $3(\epsilon|\F|+1)$ errors}] \ge \frac{2}{3} \]
		and we are done.\\

		In all, we choose a random line through $\beta$, apply Reed-Solomon coding on this line, then use the resultant polynomial to compute $p(\beta)$.\\
		Here, the local decoding algorithm runs in $O(|\F|)$ time, which we wish to be $\polylog(|\F|^{\ell})$. For sufficiently large $\ell$ ($\Omega( (|\F|/\log|\F|)^\delta)$ for some constant $\delta > 0$), this is indeed true.\\

		When we try to convert this to the binary setting however, one major issue pops up. We can of course view $\F$ as a string over $\{0,1\}^{\log |\F|}$, but in this case the notion of ``error'' changes. An $\epsilon$ fraction corruption means that an $\epsilon$ fraction of the \emph{bits} are corrupted, not points in $\F^\ell$. Indeed, an $\epsilon$ fraction of bits being corrupted means that an $\epsilon \log|\F|$ fraction of the points in $\F^\ell$ could be corrupted. \\
		We would like a coding scheme over the binary alphabet that can tolerate a constant fraction of errors, and Reed-Muller codes do not seem to satisfy this.
		% The way we fix this is by further concatenating a string over the binary alphabet, to yield a coding known as the \emph{Walsh-Hadamard coding}. We shall see this next lecture.

	\subsubsection{Lecture 19}

		In the last lecture, we saw that the relative distance of the Reed Muller code was $1 - d/|\F|$, when viewed as a code on alphabet $|\F|$. When viewed as a code on alphabet $\{0,1\}$ however, this goes to $(1 - d/|\F|)/\log|\F|$. This issue of the relative distance being $o(1)$ cannot be fixed even by changing $\F,\ell,d$.\\
		To fix this, we shall do the following: for each element of $\F$ (each coordinate when viewed as a code on alphabet $\F$), we shall replace it with another codeword, possibly larger. That is, if we encode it as $x \in \F^{|\F|^\ell}$ under the Reed-Muller code, we encode each $x_i$ as another element $\{0,1\}^{t}$, where $t$ will end up being $\log |\F|$.\\

		This second code is the \emph{Walsh-Hadamard code}, defined as follows. The encoding is a function $\WH : \{0,1\}^k \to \{0,1\}^{2^k}$, where for each $S \subseteq [k]$, we have $(\WH(x))_S = \bigoplus_{i \in S} x_i$.\\
		We claim that the relative distance of this code is $1/2$. Indeed, if we change $r$ bits, all coordinates corresponding to subsets that contain an odd number of these $r$ bits will change.\\
		Further, it turns out that this optimal.

		\begin{fprop}
			For any $\delta > (1/2)$, there exists $n_0$ such that no code with more than $2^{n_0}$ codewords has relative distance $\Delta$.
		\end{fprop}
		\begin{proof}[Proof sketch]
			Let us just look at the case where the code is over $\{0,1\}^{n_0}$, and suppose instead that we have a mapping $f : \{0,1\}^{n_0} \to \{-1,1\}^m$ with relative distance $\Delta > 1/2$. Note that $\langle  f(x),f(y) \rangle < 0$ for any distinct $x,y \in \{0,1\}^{n_0}$. The result follows by bounding the number of such vectors from above.
		\end{proof}
		
		In addition, the Walsh-Hadamard code is indeed locally decodable. Given $x$ and some corruption of $\WH(x)$, we can consider sets of the form $T$ and $T \cup \{i\}$, where $i \not\in T$. Adding (XORing) the two should give $x_i$ in the absence of corruption. When there is corruption, we can just choose a bunch of random $T$ and perform this same operation, taking the majority finally. The probability that both $T$ and $T \cup \{i\}$ are uncorrupted is at least $1-2\rho$, so for $\rho < (1/2)$, we are fine.\\
		
		In conclusion, our final code is $\WH(\RM(x))$.\footnote{mildly abusing notation to mean that we apply $\WH$ on a coordinate-by-coordinate basis to $\RM(x)$.} Here, $\WH$ is a mapping from $\{0,1\}^{\log|\F|} \to \{0,1\}^{|\F|}$. The relative distance of this code is $(1/2)(1 - d/|\F|)$, which is $\Theta(1)$ for appropriate $d,|\F|$! We can handle an error fraction of about $\rho \approx \Delta/2 \approx (1/4)$.\\
		One interesting thing is that due to the previous proposition, we cannot even do better than $1/4$ using a coding theory-based proof like this.

		Now, we have gone from exponential $H_\worst$ to exponential $H_\avg^{1-\rho}$, which in the limiting case is $H_\avg^{3/4}$. How do we go from this to $H_\avg$? We do not delve into the details of this, but the main result used is the following.

		\begin{ftheo}[Yao's XOR Lemma]
			Given a function $f : \{0,1\}^n \to \{0,1\}$, define the function $\hat{f} : \{0,1\}^{nk} \to \{0,1\}$ defined by
			\[ f(\overline{x}_1 , \overline{x}_2 , \ldots , \overline{x}_k) = f(\overline{x}_1) \oplus f(\overline{x}_2) \oplus \cdots \oplus f(\overline{x}_k), \]
			where each $\overline{x}_i$ is in $\{0,1\}^n$.\\
			If $\delta > 0$ and $\epsilon > 2(1-\delta)^k$,
			\[ H_\avg^{(1/2) + \epsilon} (\hat{f}) \ge \frac{\epsilon^2}{400n} H_\avg^{1-\delta}(f). \]
		\end{ftheo}
		Given a function with exponentially large $H_\avg^{1-\delta}$, making $\epsilon$ appropriately exponentially small does the job (around $H_\avg^{1-\delta}(f)^{-1/3}$).\\
		Alternatively, one way to go directly from $H_\worst$ to $H_\avg$ is to use \emph{local list decoding} for the Reed-Muller and Walsh-Hadamard combination we saw earlier in the lecture.\\

		Therefore, if we have a function that has exponential worst-case hardness, $\BPP = \PP$!