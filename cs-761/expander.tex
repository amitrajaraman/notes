\section{Expander graphs and applications}


	\subsection{Lectures 6--7: Magical graphs and two applications}
	
		\subsubsection{Lecture 6}

			% reference material: Chapter 1 of Hoory, Linial, Wigderson: ``Expander Graphs and Applications''

			Expander graphs are interesting because they are ``pseudorandom'' -- they behave like random objects.

			We recall the subject of error correcting codes, pioneered by Shannon in 1948. It studies the idea of introducing ``redundancy'' when transmitting messages so that the messages are understandable even in the presence of errors.
			% "bit by bit" documentary about shannon

			\begin{definition}
				A \emph{code} $\mathcal{C}$ is a subset of $\{0,1\}^n$. The elements of a code are called \emph{codewords}.\\
			\end{definition}
			\begin{definition}
				Given $x,y \in \{0,1\}^n$, the \emph{Hamming distance} $d_H(x,y)$ between $x$ and $y$ is $|\{i \in [n] : x_i \ne y_i\}|$. The distance $d_H(\mathcal{C})$ of a code $\mathcal{C}$ is $\min_{\substack{x,y \in \mathcal{C} \\ x \ne y}} d_H(x,y)$.
			\end{definition}

			The idea of this is that given a word in $\{0,1\}^k$, we translate it bijectively into a codeword in $\{0,1\}^n$ and transmit it. Upon receiving the message, we decode the received word in some way to get a word.\\
			One simple way is to decode a received word as the codeword closest to it, in the sense of the Hamming distance. This scheme allows the correction of errors if the received word is at Hamming distance less than $(1/2)d_H(\mathcal{C})$ from the transmitted word.

			\begin{definition}
				The \emph{rate} of a code is defined by
				\[ \Rt(\mathcal{C}) = \frac{\log |\mathcal{C}|}{n}. \]
				We also define the \emph{relative distance}
				\[ \delta(\mathcal{C}) = \frac{d_H(\mathcal{C})}{n}. \]
			\end{definition}

			One question that should immediately come to mind is: given a relative distance, what is the minimum rate required to achieve it? In less formal terms, what is the minimum amount of redundancy needed? We state it more formally.

			\begin{problem*}
				Given constants $\delta_0,r_0 \in (0,1)$, when can we construct codes $\{\mathcal{C}_n\}_{n \in \N}$ such that $\delta(\mathcal{C}_n) \to \delta_0$ and $\Rt(\mathcal{C}_n) \to r_0$?
			\end{problem*}
			This also presents another follow-up question: if codes of the above form exist, do there exist efficient encoding and decoding algorithms for the code? We do not look at this


			Consider another question.
			\begin{problem*}
				Suppose we have an algorithm $\mathcal{A}$ with ``one-sided error''. This means that if $x$ is in the language $L$ of interest, $\mathcal{A}(x)$ is yes with probability $1$, but if $x$ is not in the language $L$, $\mathcal{A}(x)$ is no with probability $\frac{15}{16}$.\\
				How would one go about making the error probability very small, without using too many random bits?
			\end{problem*}

			One simple idea which we have discusses is to repeat the experiment a large number of times and output no if we get a no at any point. Indeed, if we repeat it $\ell$ times, the error probability goes down to $\le (1/16)^{\ell}$. \\
			However, the fault with this is that if the algorithm uses $k$ independent random bits (say), then repeating it $\ell$ times requires $\ell k$ independent random bits! Could we make it $\ell+k$? It turns out that this \emph{is} possible.\\

			The two questions we have described seem incredibly different, but the answers to both are yes, with the ideas behind both involving ``expander graphs''. Before getting to this, we define something else. \\

			\begin{fdef}[Magical graphs]
				A bipartite graph $G = (L\sqcup R, E)$ is said to be \emph{$(n,m,d)$-magical}, $m \ge (3n/4)$, if
				\begin{enumerate}
					\item $|L|=n$, $|R|=m$,
					\item for any $v \in L$, $\deg(v) = d$, and
					\item for every subset $S \subseteq L$ with $|S| \le n/10d$, $|\Gamma(S)| \ge (5d/8)|S|$.
				\end{enumerate}
			\end{fdef}

			Above, $\Gamma(S)$ denotes the neighbourhood of $S$.\\
			Typically, $n$ and $m$ are of similar orders and $d$ is a constant.
			This says that any ``small'' subset expands a lot -- the neighbours of the vertices in the subset do not coincide too much. Ideally, with no intersection between neighbourhoods, we would have $|\Gamma(S)| = d|S|$, and we are demanding about half of this.\\
			First, we shall see why magical graphs exist. Following this, we connect them to the questions we looked at earlier.\\

			\begin{ftheo}
				For $d\ge 24$ and sufficiently large $n$, $(n,m,d)$-magical graphs exist.
			\end{ftheo}
			\begin{proof}
				For each vertex in $L$, choose its $d$ neighbours randomly. Let $S \subseteq L$ with $|S| = s \le n/10d$ and $T = R$ with $|T| = (5d/8)s$,
				\[ \Pr\left[ \Gamma(S) \subseteq T \right] \le \left( \frac{|T|}{m} \right)^{ds} \le \left( \frac{5ds}{8m} \right)^{ds}. \]
				This is for a \emph{fixed} $S,T$ however. Using the union bound,
				\begin{align*}
					\Pr\left[ \text{$\exists S,T$ as above such that }\Gamma(S) \subseteq T \right] &\le \sum_{S,T} \left( \frac{5ds}{8m} \right)^{ds} \\
						&\le  \sum_{s=1}^{n/10d} \binom{n}{s} \binom{m}{5ds/8} \left(\frac{5ds}{8m}\right)^{ds} \\
						&\le \sum_{s=1}^{n/10d} \left(\frac{ne}{s}\right)^s \left(\frac{8me}{5ds}\right)^{5ds/8} \left( \frac{5ds}{8m} \right)^{ds} & \left(\binom{n}{k} \ge (ne/k)^k\right) \\ % Watch Ryan O'Donnell's lectures for more info on the bounds FINISH THE CALCULATIONS!
						&= \sum_{s=1}^{n/10d} \left( \frac{ne}{s} \right)^s e^{5ds/8} \left(\frac{5ds}{8m}\right)^{3ds/8} \\
						&\le \sum_{s=1}^{n/10d} \left( \frac{ne}{s} \right)^s e^{5ds/8} \left(\frac{5ds}{6n}\right)^{3ds/8} & (m\ge 3n/4) \\
						&= \sum_{s=1}^{n/10d} \left(\frac{s}{n}\right)^{s(3d/8 - 1)} e^{s(5d/8+1)} \left(5d/6\right)^{3ds/8} \\
						&\le \sum_{s=1}^{n/10d}  (10d)^{-s(3d/8-1)} e^{s(5d/8+1)} \left(5d/6\right)^{3ds/8} & (s/n \le 1/10d) \\
						&\le \sum_{s=1}^{\infty}  (10d)^{-s(3d/8-1)} e^{s(5d/8+1)} \left(5d/6\right)^{3ds/8} \\
						&= \frac{\alpha}{1-\alpha},
				\end{align*}
				where $\alpha = (10d)^{1-(3d/8)} e^{(5d/8)+1} (5d/6)^{3d/8}$. The above is less than $1$ when $\alpha < 1/2$. To check for what values of $d$ this is true,
				\begin{align*}
					\log \alpha &= \left(1-\frac{3d}{8}\right)(\log 10 + \log d) + 1 + \frac{5d}{8} + \frac{3d}{8}\left(\log(5/6) + \log d\right) \\
						&= \log d + d \left( \frac{5}{8} - \frac{3}{8}\log(10) + \frac{3}{8}\log(5/6) \right) + (1+\log 10) \\
					\frac{\dif \log \alpha}{\dif d} &\approx \frac{1}{d} - 0.306,
				\end{align*}
				which is negative for $1/d < 0.306$ (or equivalently, $d \ge 5$). Since $\alpha$ is decreasing in $d$ for $d \ge 24$, it suffices to check that $\alpha < 1/2$ when $d=24$. Indeed, it is easily verified that $\alpha \approx 0.413 < 1/2$ in this case, completing the proof.
			\end{proof}

			Now, let us look at reduction of randomness using magical graphs.\\
			Let $\mathcal{A}$ be an algorithm that uses $k$ random bits with error probability $<1/16$. Take $n=2^k$, and let $G=(L\sqcup R,E)$ be a $(n,n,d)$-magical graph. Choose a random vertex $v\in L$, and take all $d$ neighbours $u_1,\ldots,u_d$ of $v$. Each $u_i$ can be thought of as a $k$ bit string. For $i=1,\ldots,d$, run $\mathcal{A}$ with $u_i$ as the choice of random bits. Observe that we are only using $k$ random bits here, namely in the choice of $v$.\\
			
			Why does the error probability go down?\\
			Let $B \subseteq \{0,1\}^k$ be the set of ``bad'' inputs for algorithm $\mathcal{A}$. We know that $|B| \le n/16$. What is the probability of failure when we run it $d$ times as described above? The algorithm fails iff every $u_i$ is in $B$.\\
			We claim that there are less than $n/10d$ such vertices $v$ with every neighbour in $B$. Suppose instead that there are is a set $S$ with $n/10d$ vertices with all neighbours in $B$. Then,
			\[ \frac{n}{16} > |B| \ge \Gamma(S) \ge \frac{5d}{8}|S| \ge \frac{n}{16}, \]
			which is a contradiction.\\
			Therefore, the probability of failure is at most $1/10d$.\\
			We can make $d$ very large (up to a limit forced by $n = 2^k$), so the probability of failure can be made very small. Using the same number of random bits, we have managed to significantly decrease the error probability.\\
			The issue now however is that the above scheme requires the construction of exponentially large magical graphs. We require a very efficient algorithm (polynomial in $\log n$) to sample the neighbours of a random vertex in a magical graph.

			% next class, expander graphs for codes

		\subsubsection{Lecture 7}


			\begin{lemma}
				\label{exists vert with unique neighbour}
				Given a $(n,m,d)$-magical graph, for any $S \subseteq L$ of size at most $n/10d$, there exists $v \in \Gamma(S)$ such that $v$ has a unique neighbour in $S$.
			\end{lemma}
			\begin{proof}
				Suppose instead that no such $v$ exists. Then,
				\[ d|S| = |e(\Gamma(S),S)| \ge 2|\Gamma(S)| \ge 2 \cdot \frac{5d}{8}|S|, \]
				a contradiction.
			\end{proof}

			Consider some $(n,m=3n/4,d)$-magical graph, and let $M$ be the $m\times n$ adjacency matrix of the graph, where $M_{ij}$ is $1$ iff there is an edge between the $i$th vertex on the right and the $j$th vertex on the left, and $0$ otherwise.\\

			\begin{fdef}
				A code $\mathcal{C} \subseteq \{0,1\}^n$ is said to be a \emph{linear code} if it is a subspace when viewed as a subset of the vector space $\F_2^n$.
			\end{fdef}
			This is equivalent to saying that if $x,y \in \mathcal{C}$, then $x+y \in \mathcal{C}$. Observe that the distance of a linear code is equal to the minimum weight of its codeword. \\

			Consider the code defined by
			\[ \mathcal{C} = \{ x : Mx = 0 \}, \]
			the null space of $M$ (over $\F_2$). In coding theory lingo, one would say that $M$ is the parity check matrix of $\mathcal{C}$.\\
			Evidently,
			\begin{align*}
				|\mathcal{C}| &= 2^{n - \rank(M)} \text{ and}
				\Rt(\mathcal{C}) &= \frac{n-\rank(M)}{n} \ge \frac{n-m}{n} \ge \frac{1}{4}.
			\end{align*}

			We claim that $d_H(C) > n/10d$, so the relative distance is at least $1/10d$. Suppose instead that there is some $x \in \mathcal{C}$ such that $\wt(x) \le n/10d$. Let $S \subseteq [n]$ be the subset of $L$ such that $v \in S$ iff $x_v \ne 0$. By \Cref{exists vert with unique neighbour}, there exists some $u \in R$ such that $M_{uv} = 1$ for precisely one $v \in S$. However, this implies that $(Mx)_u = \sum_v M_{uv} x_{v} = 1$.

	\subsection{Lectures 7--8: Testing connectivity of undirected graphs}

		\subsubsection{Lecture 7 (continued)}

			\begin{problem*}
				Given a graph $G$ and two vertices $s,t \in V(G)$, determine if there is a path between $s$ and $t$.
			\end{problem*}

			To test connectivity of a graph, we can just test the above by iterating through all $t \in V(G)$. We work in the setting where running time is not an issue (as long as it is polynomial), but we have space constraints.\\
			
			One way to do this, of course, is through a depth-first/breadth-first search. This requires $\Omega(n)$ space however, which is more than we can afford.\\

			Recall that there is a path between $s,t$ iff $((I+A)^n)_{st} \ne 0$. Indeed, $(A^i)_{st}$ gives the number of length $i$ walks from $s$ to $t$, and $(I+A)^n$ is just some weighted of sum of the $A^i$. Can we compute $(I+A)^n$ is $O(\log^2 n)$ space, say? We remark that here, space means that the size of the input and output tapes are ``large'', but we cannot alter the input tape and once we write something to the output tape, we cannot change it; we have an intermediate tape of ``small'' size which is what we use for computation.\\
			The answer is yes, but we do not say why.\\

			Consider the following algorithm, that is also $O(\log^2 n)$ space.\\

			\begin{algorithm}[H]
				\DontPrintSemicolon
				\SetNoFillComment
				\SetKwProg{Fn}{}{}{}
				\SetKwFunction{isPath}{{isPath}}
				\KwIn{An graph $G$, and vertices $s,t \in V(G)$}
				\KwOut{Connectivity of $s,t$}
				\Fn{\isPath{$s,t,k=2^{\ell}$}} {
				\tcp*{Outputs whether there is a path between $s$ and $t$ of length at most $k$} 
					\If{$\ell=0$} {
						\Return{\textsf{yes} iff $s,t$ are adjacent}
					}
					\ForEach{$v \in V(G)$} {
						\Return{\textsf{yes} iff \isPath{$s,v,2^{\ell-1}$} and \isPath{$s,v,2^{\ell-1}$}}
					}
				}
				\Return{\isPath{$s,t,n$}}
				\caption{Checking connectivity of two vertices in an undirected graph}
			\end{algorithm}
			Observe that the depth of this recursion tree is $O(\log n)$. In each recursion call, we use $O(\log n)$ space. As a result, we use $O(\log^2 n)$ space in all.\\

			All the algorithms we have presented thus far work in the setting of directed graphs as well.\\
			Can we check connectivity in $O(\log n)$ space? We present a randomized algorithm due to Reingold \cite{reingold-undirected-conn-logspace} that does so. The algorithm is as follows.\\

			\begin{algorithm}[H]
				\DontPrintSemicolon
				\SetNoFillComment
			    \SetKw{KwGoTo}{go to}
				\KwIn{An graph $G$, and vertices $s,t \in V(G)$}
				\KwOut{Connectivity of $s,t$}
				$v\gets s$\;
				Uniformly randomly choose a neighbour $u$ of $v$\; \label{algoline:neighbour-choosing}
				\eIf{$u=t$} {
					\Return{\textsf{yes}}
				} {
					$v\gets u$\;
					\KwGoTo line \ref{algoline:neighbour-choosing}\;
				}
				\caption{Checking connectivity of two vertices in an undirected graph}
			\end{algorithm}

			Suppose that $G$ is connected. For the sake of simplicity, suppose that the graph is $d$-regular. If it is not, add self-loops to make it so. Also, after doing this, add another self-loop at each vertex.\\
			We claim that
			\[ \Pr\left[t\text{ is seen in $O(n^3\log n)$ steps}\right] \ge \frac{1}{2}. \]
			This algorithm does \emph{not} work for directed graphs. Indeed, consider the graph
			
			\begin{center}
			\begin{tikzpicture}[
		            > = stealth, % arrow head style
		            shorten > = 1pt, % don't touch arrow head to node
		            auto,
		            node distance = 1.8cm, % distance between nodes
		            semithick % line style
		        ]

		        \tikzstyle{every state}=[
		            draw = black,
		            fill = white,
		            minimum size = 6mm
		        ]

		        \tikzstyle{mid}=[
		        	draw = white,
		        	fill = white
		        ]

		        \node[state] (s) {$s$};
		        \node[state] (v1) [right of=s] {};
		        \node[state] (v2) [right of=v1] {};
		        \node[mid] (vmid) [right of=v2] {$\cdots$};
		        \node[state] (v3) [right of=vmid] {};
		        \node[state] (t) [right of=v3] {$t$};

		        \path[->] (s) edge (v1);
		        \path[->] (v1) edge (v2);
		        \path[->] (v2) edge (vmid);
		        \path[->] (vmid) edge (v3);
		        \path[->] (v3) edge (t);
		        \path[->,bend left=20] (v1) edge (s);
		        \path[->,bend left=35] (v2) edge (s);
		        \path[->,bend left=50] (v3) edge (s);
		    \end{tikzpicture}
			\end{center}

			where it takes exponential time for the probability of seeing $t$ to go over $1/2$.\\

			The transition matrix of the random walk is defined by
			\[ M_{ij} = \frac{1}{d}e(i,j), \]
			where $e(i,j)$ is the number of edges between $i$ and $j$.\\
			Given the initial probability vector $x^{(0)} = \indic_s$ ($x^{(0)}_s = 1$ and $x^{(0)}_v = 0$ for $v \ne s$), the probability distribution of vertices after $t$ steps of the random walk is given by $x^{(t)} = M^tx^{(0)}$.\\
			Consider the uniform probability vector $u$, where $u_v = 1/n$ for all $v$, and observe that $Mu = u$. $u$ is called a stationary distribution of the random walk.\\
			We claim that $u$ is the only stationary distribution of the walk (this assumes that $G$ is connected -- why?).

			% next class, convergence theorem

		\subsubsection{Lecture 8}

			% oops didn't make very good notes this lec

			Denote by $p^{(t)}$ the probability vector at time $t$, and $p_i^{(t)}$ the probability that we are at vertex $i$ at times $t$. By definition,
			\[ p_i^{(t)} = \frac{1}{d} \sum_{j \leftrightarrow i} p_j^{(t-1)}. \]
			Now, we wish to analyze $M^t$ to show that $p^{(t)}$ is close to the stationary distribution for somewhat large $t$.\\
			Because $M$ is symmetric, we can write $M = UDU^\top$, where $U$ is an orthogonal matrix of eigenvectors and $D$ is a diagonal matrix of the real eigenvalues.
			% Also, $M$ has all real eigenvalues, and if there are distinct eigenvalues $\lambda_i,\lambda_j$, the corresponding eigenvectors $u_j,u_j$ are orthogonal.
			As a result,
			\[ p^{(t+1)} = (UDU^\top)^t p^{(0)} = UD^tU^\top p^{(0)}. \]
			Therefore, what matters is $D^t$. Let the eigenvalues of $M$ be $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_n$. \\
			We trivially have that $\lambda_1 = 1$ is an eigenvalue of $M$ with eigenvector being the stationary distribution $u_1$ that we saw last lecture. Observe that $1$ is the largest eigenvalue (in absolute value) of $M$ because for any vector $x$, $\lambda \max_i x_i = \max_i (Mx)_i \le \max_i x_i$.\\
			If $|\lambda_j| < 1$ for $j > 1$, then $\lim_{t \to \infty} p^{(t)} = u_1$ (all the other entries of $D^t$ decay to $0$).\\
			To show fast convergence, we would like to show that the other eigenvalues are bounded away from $1$.

			\begin{fdef}
				Given a graph $G$, $\lambda(G) = \max\{|\lambda_2(G)|,|\lambda_n(G)|\}$. Equivalently,
				\[ \lambda(G) = \max_{x : \langle x,u_1\rangle = 0} \frac{\norm{Mx}}{\norm{x}}. \]
				If $G$ is obvious, we denote this as merely $\lambda$.
			\end{fdef}
			 $(1-\lambda)$ is referred to as the \emph{spectral gap} of the graph. We would like to show that the spectral gap of the graph is ``large''.

			We have
			\[ \norm{p^{(t)} - u_1} = \norm{M^t (p^{(0)} - u_1)} \]
			Observe that because $p^{(0)}$ is a probability distribution, $\langle p^{(0)} , u_1 \rangle = 1/n = \langle u_1,u_1 \rangle$, so $p^{(0)}-u_1$ is orthogonal to $u_1$. As a result,
			\[ \norm{p^{(t)} - u_1} \le \lambda^t \norm{p^{(0)} - u_1}. \]
			Suppose we want $\norm{p^{(t)} - u_1} \le 1/n^2$. For our specific choice of $p^{(0)}$ concentrated on a single point, it is easily verified that $\norm{p^{(0)} - u_1} \le 1$. Therefore, for $t > \log_{\lambda} \epsilon$, $\norm{p^{(t)} - u_1} \le 1/n^2$.\\
			We have that the convergence time is
			\[ \log_\lambda \epsilon = O\left( \frac{1}{1-\lambda} \log n \right). \]
			We would like to bound the spectral gap from below, perhaps by an inverse polynomial.\\

			It is true that
			\[ \lambda_2 = \max_{\substack{x \perp u_1 \\ \norm{x} = 1}} x^\top M x \]
			Indeed, if $x = \alpha_2 u_2 + \cdots + \alpha_n u_n$ with $\sum \alpha_i^2 = 1$,
			\[ x^\top M x = \lambda_2 \alpha_2^2 + \cdots + \lambda_n \alpha_n^2 \le \lambda_2. \]
			We shall bound $\lambda_2$ for every graph, and use it for $H = G^2$, the multigraph with an edge from $i$ to $j$ if there was a length $2$ walk from $i$ to $j$. The random walk matrix on $H$ is just $M^2$, so $\lambda_2(H) = \lambda(G)^2$. Now,
			\begin{align*}
				x^\top M x &= \sum_i x_i \sum_{j \in \Gamma(i)} \frac{1}{d} x_j \\
					&= \sum_{(i,i) \in E} \frac{x_i^2}{d} + \sum_{\substack{(i,j) \in E \\ i \ne j}} \frac{2x_ix_j}{d} \\
					&= \sum_{(i,i) \in E} \frac{x_i^2}{d} + \sum_{\substack{(i,j) \in E \\ i \ne j}} \frac{x_i^2 + x_j^2 - (x_i - x_j)^2}{d} \\
					&= \underbrace{\sum_{(i,j) \in E} \frac{x_i^2}{d}}_{1} + \sum_{\substack{(i,j) \in E \\ i \ne j}} \frac{(x_i-x_j)^2}{d}.
			\end{align*}
			We want to bound the second expression from below. Because $\langle x,u_1\rangle = 0$, there exist coordinates of both positive and negative sign. Further, by the pigeonhole principle, there exists some coordinate $i_1$ such that $x_{i_1}$ is of absolute value at least $1/\sqrt{n}$. Assume that the sign of this coordinate is positive. Let $i_k$ be a coordinate of negative sign, and $i_1 i_2 \cdots i_k$ a path from $i_1$ to $i_k$. Then, using the Cauchy-Schwarz inequality,
			\begin{align*}
				\sum_{\substack{(i,j) \in E \\ i \ne j}} (x_i-x_j)^2 &= \sum_{1 \le j \le (n-1)} (x_{i_{j+1}} - x_{i_{j}})^2 \\
					&\ge \frac{1}{n} \left(\sum_{1 \le j \le (n-1)} |x_{i_{j+1}} - x_{i_j}|\right)^2 \\
					&\ge \frac{1}{n} \left(\sum_{1 \le j \le (n-1)} x_{i_{j+1}} - x_{i_j}\right)^2 \ge \frac{1}{n^2}.
			\end{align*}
			Therefore, the convergence time is $O\left( n^2d\log n \right) = O(n^3\log n)$.