\section{Expander graphs and applications}

	\subsection{Lectures 6--7: Magical graphs and two applications}
	
		\subsubsection{Lecture 6}
			\label{lec 6}

			% reference material: Chapter 1 of Hoory, Linial, Wigderson: ``Expander Graphs and Applications''

			Expander graphs are interesting because they are ``pseudorandom'' -- they behave like random objects.

			We recall the subject of error correcting codes, pioneered by Shannon in 1948. It studies the idea of introducing ``redundancy'' when transmitting messages so that the messages are understandable even in the presence of errors.
			% "bit by bit" documentary about shannon

			\begin{definition}
				A \emph{code} $\mathcal{C}$ is a subset of $\{0,1\}^n$. The elements of a code are called \emph{codewords}.\\
			\end{definition}
			\begin{definition}
				Given $x,y \in \{0,1\}^n$, the \emph{Hamming distance} $d_H(x,y)$ between $x$ and $y$ is $|\{i \in [n] : x_i \ne y_i\}|$. The distance $d_H(\mathcal{C})$ of a code $\mathcal{C}$ is $\min_{\substack{x,y \in \mathcal{C} \\ x \ne y}} d_H(x,y)$.
			\end{definition}

			The idea of this is that given a word in $\{0,1\}^k$, we translate it bijectively into a codeword in $\{0,1\}^n$ and transmit it. Upon receiving the message, we decode the received word in some way to get a word.\\
			One simple way is to decode a received word as the codeword closest to it, in the sense of the Hamming distance. This scheme allows the correction of errors if the received word is at Hamming distance less than $(1/2)d_H(\mathcal{C})$ from the transmitted word.

			\begin{definition}
				The \emph{rate} of a code is defined by
				\[ \Rt(\mathcal{C}) = \frac{\log |\mathcal{C}|}{n}. \]
				We also define the \emph{relative distance}
				\[ \delta(\mathcal{C}) = \frac{d_H(\mathcal{C})}{n}. \]
			\end{definition}

			One question that should immediately come to mind is: given a relative distance, what is the minimum rate required to achieve it? In less formal terms, what is the minimum amount of redundancy needed? We state it more formally.

			\begin{problem*}
				Given constants $\delta_0,r_0 \in (0,1)$, when can we construct codes $\{\mathcal{C}_n\}_{n \in \N}$ such that $\delta(\mathcal{C}_n) \to \delta_0$ and $\Rt(\mathcal{C}_n) \to r_0$?
			\end{problem*}
			This also presents another follow-up question: if codes of the above form exist, do there exist efficient encoding and decoding algorithms for the code? We do not look at this


			Consider another question.
			\begin{problem*}
				Suppose we have an algorithm $\mathcal{A}$ with ``one-sided error''. This means that if $x$ is in the language $L$ of interest, $\mathcal{A}(x)$ is yes with probability $1$, but if $x$ is not in the language $L$, $\mathcal{A}(x)$ is no with probability $\frac{15}{16}$.\\
				How would one go about making the error probability very small, without using too many random bits?
			\end{problem*}

			One simple idea which we have discusses is to repeat the experiment a large number of times and output no if we get a no at any point. Indeed, if we repeat it $\ell$ times, the error probability goes down to $\le (1/16)^{\ell}$. \\
			However, the fault with this is that if the algorithm uses $k$ independent random bits (say), then repeating it $\ell$ times requires $\ell k$ independent random bits! Could we make it $\ell+k$? It turns out that this \emph{is} possible.\\

			The two questions we have described seem incredibly different, but the answers to both are yes, with the ideas behind both involving ``expander graphs''. Before getting to this, we define something else. \\

			\begin{fdef}[Magical graphs]
				A bipartite graph $G = (L\sqcup R, E)$ is said to be \emph{$(n,m,d)$-magical}, $m \ge (3n/4)$, if
				\begin{enumerate}
					\item $|L|=n$, $|R|=m$,
					\item for any $v \in L$, $\deg(v) = d$, and
					\item for every subset $S \subseteq L$ with $|S| \le n/10d$, $|\Gamma(S)| \ge (5d/8)|S|$.
				\end{enumerate}
			\end{fdef}

			Above, $\Gamma(S)$ denotes the neighbourhood of $S$.\\
			Typically, $n$ and $m$ are of similar orders and $d$ is a constant.
			This says that any ``small'' subset expands a lot -- the neighbours of the vertices in the subset do not coincide too much. Ideally, with no intersection between neighbourhoods, we would have $|\Gamma(S)| = d|S|$, and we are demanding about half of this.\\
			First, we shall see why magical graphs exist. Following this, we connect them to the questions we looked at earlier.\\

			\begin{ftheo}
				For $d\ge 24$ and sufficiently large $n$, $(n,m,d)$-magical graphs exist.
			\end{ftheo}
			\begin{proof}
				For each vertex in $L$, choose its $d$ neighbours randomly. Let $S \subseteq L$ with $|S| = s \le n/10d$ and $T = R$ with $|T| = (5d/8)s$,
				\[ \Pr\left[ \Gamma(S) \subseteq T \right] \le \left( \frac{|T|}{m} \right)^{ds} \le \left( \frac{5ds}{8m} \right)^{ds}. \]
				This is for a \emph{fixed} $S,T$ however. Using the union bound,
				\begin{align*}
					\Pr\left[ \text{$\exists S,T$ as above such that }\Gamma(S) \subseteq T \right] &\le \sum_{S,T} \left( \frac{5ds}{8m} \right)^{ds} \\
						&\le  \sum_{s=1}^{n/10d} \binom{n}{s} \binom{m}{5ds/8} \left(\frac{5ds}{8m}\right)^{ds} \\
						&\le \sum_{s=1}^{n/10d} \left(\frac{ne}{s}\right)^s \left(\frac{8me}{5ds}\right)^{5ds/8} \left( \frac{5ds}{8m} \right)^{ds} & \left(\binom{n}{k} \ge (ne/k)^k\right) \\ % Watch Ryan O'Donnell's lectures for more info on the bounds FINISH THE CALCULATIONS!
						&= \sum_{s=1}^{n/10d} \left( \frac{ne}{s} \right)^s e^{5ds/8} \left(\frac{5ds}{8m}\right)^{3ds/8} \\
						&\le \sum_{s=1}^{n/10d} \left( \frac{ne}{s} \right)^s e^{5ds/8} \left(\frac{5ds}{6n}\right)^{3ds/8} & (m\ge 3n/4) \\
						&= \sum_{s=1}^{n/10d} \left(\frac{s}{n}\right)^{s(3d/8 - 1)} e^{s(5d/8+1)} \left(5d/6\right)^{3ds/8} \\
						&\le \sum_{s=1}^{n/10d}  (10d)^{-s(3d/8-1)} e^{s(5d/8+1)} \left(5d/6\right)^{3ds/8} & (s/n \le 1/10d) \\
						&\le \sum_{s=1}^{\infty}  (10d)^{-s(3d/8-1)} e^{s(5d/8+1)} \left(5d/6\right)^{3ds/8} \\
						&= \frac{\alpha}{1-\alpha},
				\end{align*}
				where $\alpha = (10d)^{1-(3d/8)} e^{(5d/8)+1} (5d/6)^{3d/8}$. The above is less than $1$ when $\alpha < 1/2$. To check for what values of $d$ this is true,
				\begin{align*}
					\log \alpha &= \left(1-\frac{3d}{8}\right)(\log 10 + \log d) + 1 + \frac{5d}{8} + \frac{3d}{8}\left(\log(5/6) + \log d\right) \\
						&= \log d + d \left( \frac{5}{8} - \frac{3}{8}\log(10) + \frac{3}{8}\log(5/6) \right) + (1+\log 10) \\
					\frac{\dif \log \alpha}{\dif d} &\approx \frac{1}{d} - 0.306,
				\end{align*}
				which is negative for $1/d < 0.306$ (or equivalently, $d \ge 5$). Since $\alpha$ is decreasing in $d$ for $d \ge 24$, it suffices to check that $\alpha < 1/2$ when $d=24$. Indeed, it is easily verified that $\alpha \approx 0.413 < 1/2$ in this case, completing the proof.
			\end{proof}

			Now, let us look at reduction of randomness using magical graphs.\\
			Let $\mathcal{A}$ be an algorithm that uses $k$ random bits with error probability $<1/16$. Take $n=2^k$, and let $G=(L\sqcup R,E)$ be a $(n,n,d)$-magical graph. Choose a random vertex $v\in L$, and take all $d$ neighbours $u_1,\ldots,u_d$ of $v$. Each $u_i$ can be thought of as a $k$ bit string. For $i=1,\ldots,d$, run $\mathcal{A}$ with $u_i$ as the choice of random bits. Observe that we are only using $k$ random bits here, namely in the choice of $v$.\\
			
			Why does the error probability go down?\\
			Let $B \subseteq \{0,1\}^k$ be the set of ``bad'' inputs for algorithm $\mathcal{A}$. We know that $|B| \le n/16$. What is the probability of failure when we run it $d$ times as described above? The algorithm fails iff every $u_i$ is in $B$.\\
			We claim that there are less than $n/10d$ such vertices $v$ with every neighbour in $B$. Suppose instead that there are is a set $S$ with $n/10d$ vertices with all neighbours in $B$. Then,
			\[ \frac{n}{16} > |B| \ge \Gamma(S) \ge \frac{5d}{8}|S| \ge \frac{n}{16}, \]
			which is a contradiction.\\
			Therefore, the probability of failure is at most $1/10d$.\\
			We can make $d$ very large (up to a limit forced by $n = 2^k$), so the probability of failure can be made very small. Using the same number of random bits, we have managed to significantly decrease the error probability.\\
			The issue now however is that the above scheme requires the construction of exponentially large magical graphs. We require a very efficient algorithm (polynomial in $\log n$) to sample the neighbours of a random vertex in a magical graph.

			% next class, expander graphs for codes

		\subsubsection{Lecture 7}


			\begin{lemma}
				\label{exists vert with unique neighbour}
				Given a $(n,m,d)$-magical graph, for any $S \subseteq L$ of size at most $n/10d$, there exists $v \in \Gamma(S)$ such that $v$ has a unique neighbour in $S$.
			\end{lemma}
			\begin{proof}
				Suppose instead that no such $v$ exists. Then,
				\[ d|S| = |e(\Gamma(S),S)| \ge 2|\Gamma(S)| \ge 2 \cdot \frac{5d}{8}|S|, \]
				a contradiction.
			\end{proof}

			Consider some $(n,m=3n/4,d)$-magical graph, and let $M$ be the $m\times n$ adjacency matrix of the graph, where $M_{ij}$ is $1$ iff there is an edge between the $i$th vertex on the right and the $j$th vertex on the left, and $0$ otherwise.\\

			\begin{fdef}
				A code $\mathcal{C} \subseteq \{0,1\}^n$ is said to be a \emph{linear code} if it is a subspace when viewed as a subset of the vector space $\F_2^n$.
			\end{fdef}
			This is equivalent to saying that if $x,y \in \mathcal{C}$, then $x+y \in \mathcal{C}$. Observe that the distance of a linear code is equal to the minimum weight of its codeword. \\

			Consider the code defined by
			\[ \mathcal{C} = \{ x : Mx = 0 \}, \]
			the null space of $M$ (over $\F_2$). In coding theory lingo, one would say that $M$ is the parity check matrix of $\mathcal{C}$.\\
			Evidently,
			\begin{align*}
				|\mathcal{C}| &= 2^{n - \rank(M)} \text{ and}
				\Rt(\mathcal{C}) &= \frac{n-\rank(M)}{n} \ge \frac{n-m}{n} \ge \frac{1}{4}.
			\end{align*}

			We claim that $d_H(C) > n/10d$, so the relative distance is at least $1/10d$. Suppose instead that there is some $x \in \mathcal{C}$ such that $\wt(x) \le n/10d$. Let $S \subseteq [n]$ be the subset of $L$ such that $v \in S$ iff $x_v \ne 0$. By \Cref{exists vert with unique neighbour}, there exists some $u \in R$ such that $M_{uv} = 1$ for precisely one $v \in S$. However, this implies that $(Mx)_u = \sum_v M_{uv} x_{v} = 1$.

	\subsection{Lectures 7, 8, 10: Testing connectivity of undirected graphs}

		\subsubsection{Lecture 7 (continued)}

			\begin{problem*}
				Given a graph $G$ and two vertices $s,t \in V(G)$, determine if there is a path between $s$ and $t$.
			\end{problem*}

			To test connectivity of a graph, we can just test the above by iterating through all $t \in V(G)$. We work in the setting where running time is not an issue (as long as it is polynomial), but we have space constraints.\\
			
			One way to do this, of course, is through a depth-first/breadth-first search. This requires $\Omega(n)$ space however, which is more than we can afford.\\

			Recall that there is a path between $s,t$ iff $((I+A)^n)_{st} \ne 0$. Indeed, $(A^i)_{st}$ gives the number of length $i$ walks from $s$ to $t$, and $(I+A)^n$ is just some weighted of sum of the $A^i$. Can we compute $(I+A)^n$ is $O(\log^2 n)$ space, say? We remark that here, space means that the size of the input and output tapes are ``large'', but we cannot alter the input tape and once we write something to the output tape, we cannot change it; we have an intermediate tape of ``small'' size which is what we use for computation.\\
			The answer is yes, but we do not say why.\\

			Consider the following algorithm, that is also $O(\log^2 n)$ space.\\

			\begin{algorithm}[H]
				\DontPrintSemicolon
				\SetNoFillComment
				\SetKwProg{Fn}{}{}{}
				\SetKwFunction{isPath}{{isPath}}
				\KwIn{An graph $G$, and vertices $s,t \in V(G)$}
				\KwOut{Connectivity of $s,t$}
				\Fn{\isPath{$s,t,k=2^{\ell}$}} {
				\tcp*{Outputs whether there is a path between $s$ and $t$ of length at most $k$} 
					\If{$\ell=0$} {
						\Return{\textsf{yes} iff $s,t$ are adjacent}
					}
					\ForEach{$v \in V(G)$} {
						\Return{\textsf{yes} iff \isPath{$s,v,2^{\ell-1}$} and \isPath{$s,v,2^{\ell-1}$}}
					}
				}
				\Return{\isPath{$s,t,n$}}
				\caption{Checking connectivity of two vertices in an undirected graph}
			\end{algorithm}
			Observe that the depth of this recursion tree is $O(\log n)$. In each recursion call, we use $O(\log n)$ space. As a result, we use $O(\log^2 n)$ space in all.\\

			All the algorithms we have presented thus far work in the setting of directed graphs as well.\\
			Can we check connectivity in $O(\log n)$ space? We present a randomized algorithm due to Reingold \cite{reingold-undirected-conn-logspace} that does so. The algorithm is as follows.\\

			\begin{algorithm}[H]
				\DontPrintSemicolon
				\SetNoFillComment
			    \SetKw{KwGoTo}{go to}
				\KwIn{An graph $G$, and vertices $s,t \in V(G)$}
				\KwOut{Connectivity of $s,t$}
				$v\gets s$\;
				Uniformly randomly choose a neighbour $u$ of $v$\; \label{algoline:neighbour-choosing}
				\eIf{$u=t$} {
					\Return{\textsf{yes}}
				} {
					$v\gets u$\;
					\KwGoTo line \ref{algoline:neighbour-choosing}\;
				}
				\caption{Checking connectivity of two vertices in an undirected graph}
			\end{algorithm}

			Suppose that $G$ is connected. For the sake of simplicity, suppose that the graph is $d$-regular. If it is not, add self-loops to make it so. Also, after doing this, add another self-loop at each vertex.\\
			We claim that
			\[ \Pr\left[t\text{ is seen in $O(n^3\log n)$ steps}\right] \ge \frac{1}{2}. \]
			This algorithm does \emph{not} work for directed graphs. Indeed, consider the graph
			
			\begin{center}
			\begin{tikzpicture}[
		            > = stealth, % arrow head style
		            shorten > = 1pt, % don't touch arrow head to node
		            auto,
		            node distance = 1.8cm, % distance between nodes
		            semithick % line style
		        ]

		        \tikzstyle{every state}=[
		            draw = black,
		            fill = white,
		            minimum size = 6mm
		        ]

		        \tikzstyle{mid}=[
		        	draw = white,
		        	fill = white
		        ]

		        \node[state] (s) {$s$};
		        \node[state] (v1) [right of=s] {};
		        \node[state] (v2) [right of=v1] {};
		        \node[mid] (vmid) [right of=v2] {$\cdots$};
		        \node[state] (v3) [right of=vmid] {};
		        \node[state] (t) [right of=v3] {$t$};

		        \path[->] (s) edge (v1);
		        \path[->] (v1) edge (v2);
		        \path[->] (v2) edge (vmid);
		        \path[->] (vmid) edge (v3);
		        \path[->] (v3) edge (t);
		        \path[->,bend left=20] (v1) edge (s);
		        \path[->,bend left=35] (v2) edge (s);
		        \path[->,bend left=50] (v3) edge (s);
		    \end{tikzpicture}
			\end{center}

			where it takes exponential time for the probability of seeing $t$ to go over $1/2$.\\

			The transition matrix of the random walk is defined by
			\[ M_{ij} = \frac{1}{d}e(i,j), \]
			where $e(i,j)$ is the number of edges between $i$ and $j$.\\
			Given the initial probability vector $x^{(0)} = \indic_s$ ($x^{(0)}_s = 1$ and $x^{(0)}_v = 0$ for $v \ne s$), the probability distribution of vertices after $t$ steps of the random walk is given by $x^{(t)} = M^tx^{(0)}$.\\
			Consider the uniform probability vector $u$, where $u_v = 1/n$ for all $v$, and observe that $Mu = u$. $u$ is called a stationary distribution of the random walk.\\
			We claim that $u$ is the only stationary distribution of the walk (this assumes that $G$ is connected -- why?).

			% next class, convergence theorem

		\subsubsection{Lecture 8}

			% oops didn't make very good notes this lec

			Denote by $p^{(t)}$ the probability vector at time $t$, and $p_i^{(t)}$ the probability that we are at vertex $i$ at times $t$. By definition,
			\[ p_i^{(t)} = \frac{1}{d} \sum_{j \leftrightarrow i} p_j^{(t-1)}. \]
			Now, we wish to analyze $M^t$ to show that $p^{(t)}$ is close to the stationary distribution for somewhat large $t$.\\
			Because $M$ is symmetric, we can write $M = UDU^\top$, where $U$ is an orthogonal matrix of eigenvectors and $D$ is a diagonal matrix of the real eigenvalues.
			% Also, $M$ has all real eigenvalues, and if there are distinct eigenvalues $\lambda_i,\lambda_j$, the corresponding eigenvectors $u_j,u_j$ are orthogonal.
			As a result,
			\[ p^{(t+1)} = (UDU^\top)^t p^{(0)} = UD^tU^\top p^{(0)}. \]
			Therefore, what matters is $D^t$. Let the eigenvalues of $M$ be $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_n$. \\
			We trivially have that $\lambda_1 = 1$ is an eigenvalue of $M$ with eigenvector being the stationary distribution $u_1$ that we saw last lecture. Observe that $1$ is the largest eigenvalue (in absolute value) of $M$ because for any vector $x$, $\lambda \max_i x_i = \max_i (Mx)_i \le \max_i x_i$.\\
			If $|\lambda_j| < 1$ for $j > 1$, then $\lim_{t \to \infty} p^{(t)} = u_1$ (all the other entries of $D^t$ decay to $0$).\\
			To show fast convergence, we would like to show that the other eigenvalues are bounded away from $1$.

			\begin{fdef}
				Given a graph $G$, $\lambda(G) = \max\{|\lambda_2(G)|,|\lambda_n(G)|\}$. Equivalently,
				\[ \lambda(G) = \max_{x : \langle x,u_1\rangle = 0} \frac{\norm{Mx}}{\norm{x}}. \]
				If $G$ is obvious, we denote this as merely $\lambda$.
			\end{fdef}
			 $(1-\lambda)$ is referred to as the \emph{spectral gap} of the graph. We would like to show that the spectral gap of the graph is ``large''.

			We have
			\[ \norm{p^{(t)} - u_1} = \norm{M^t (p^{(0)} - u_1)} \]
			Observe that because $p^{(0)}$ is a probability distribution, $\langle p^{(0)} , u_1 \rangle = 1/n = \langle u_1,u_1 \rangle$, so $p^{(0)}-u_1$ is orthogonal to $u_1$. As a result,
			\[ \norm{p^{(t)} - u_1} \le \lambda^t \norm{p^{(0)} - u_1}. \]
			Suppose we want $\norm{p^{(t)} - u_1} \le 1/n^2$. For our specific choice of $p^{(0)}$ concentrated on a single point, it is easily verified that $\norm{p^{(0)} - u_1} \le 1$. Therefore, for $t > \log_{\lambda} \epsilon$, $\norm{p^{(t)} - u_1} \le 1/n^2$.\\
			We have that the convergence time is
			\[ \log_\lambda \epsilon = O\left( \frac{1}{1-\lambda} \log n \right). \]
			We would like to bound the spectral gap from below, perhaps by an inverse polynomial.\\

			It is true that
			\[ \lambda_2 = \max_{\substack{x \perp u_1 \\ \norm{x} = 1}} x^\top M x \]
			Indeed, if $x = \alpha_2 u_2 + \cdots + \alpha_n u_n$ with $\sum \alpha_i^2 = 1$,
			\[ x^\top M x = \lambda_2 \alpha_2^2 + \cdots + \lambda_n \alpha_n^2 \le \lambda_2. \]
			We shall bound $\lambda_2$ for every graph, and use it for $H = G^2$, the multigraph with an edge from $i$ to $j$ if there was a length $2$ walk from $i$ to $j$. The random walk matrix on $H$ is just $M^2$, so $\lambda_2(H) = \lambda(G)^2$. Now,
			\begin{align*}
				x^\top M x &= \sum_i x_i \sum_{j \in \Gamma(i)} \frac{1}{d} x_j \\
					&= \sum_{(i,i) \in E} \frac{x_i^2}{d} + \sum_{\substack{(i,j) \in E \\ i \ne j}} \frac{2x_ix_j}{d} \\
					&= \sum_{(i,i) \in E} \frac{x_i^2}{d} + \sum_{\substack{(i,j) \in E \\ i \ne j}} \frac{x_i^2 + x_j^2 - (x_i - x_j)^2}{d} \\
					&= \underbrace{\sum_{(i,j) \in E} \frac{x_i^2}{d}}_{1} + \sum_{\substack{(i,j) \in E \\ i \ne j}} \frac{(x_i-x_j)^2}{d}.
			\end{align*}
			We want to bound the second expression from below. Because $\langle x,u_1\rangle = 0$, there exist coordinates of both positive and negative sign. Further, by the pigeonhole principle, there exists some coordinate $i_1$ such that $x_{i_1}$ is of absolute value at least $1/\sqrt{n}$. Assume that the sign of this coordinate is positive. Let $i_k$ be a coordinate of negative sign, and $i_1 i_2 \cdots i_k$ a path from $i_1$ to $i_k$. Then, using the Cauchy-Schwarz inequality,
			\begin{align*}
				\sum_{\substack{(i,j) \in E \\ i \ne j}} (x_i-x_j)^2 &= \sum_{1 \le j \le (n-1)} (x_{i_{j+1}} - x_{i_{j}})^2 \\
					&\ge \frac{1}{n} \left(\sum_{1 \le j \le (n-1)} |x_{i_{j+1}} - x_{i_j}|\right)^2 \\
					&\ge \frac{1}{n} \left(\sum_{1 \le j \le (n-1)} x_{i_{j+1}} - x_{i_j}\right)^2 \ge \frac{1}{n^2}.
			\end{align*}
			Therefore, the largest eigenvalue of any graph is at most $1 - 1/nd^2$.

		\subsubsection{Lecture 10}

			By arguments discussed in Lecture $8$, we know that the random walk on an expander graph mixes in $O(\log n)$ time. This leads to a \emph{deterministic} polynomial time algorithm to determine $s,t$ connectivity in log-space -- merely iterate over all paths of length $\log n$. That is, we iterate over all elements of $[d]^{\log n}$ and for each such element, we follow the corresponding path and check if we see $t$ anywhere.\\

			How do we extend this to arbitrary graphs? Given a graph $G$, we would like to get a graph $G'$ such that
			\begin{itemize}
				\item $G'$ has polynomially many vertices,
				\item $G'$ has ``constant'' degree (independent of $n$),
				\item $G'$ preserves connectivity ($s,t$ are connected in $G$ iff some $s',t'$ are connected in $G'$), and
				\item each component of $G'$ is an expander.
			\end{itemize}

			Let us look at a couple of operations one can do on a $(n,d,\lambda)$-expander $G$.

			\paragraph{Squaring ($G^2$)} the vertex set is the same as that of $G$, and there is an edge between $u,v$ for each length $2$ walk between $u,v \in G$. This is a multigraph with self-loops. $G^2$ is a $(n,d^2,\lambda^2)$-expander; the random walk matrix on $G^2$ is just $M^2$, where $M$ is the random walk matrix on $G$. Because the second eigenvalue has gone down, connectivity has improved. However, the degree has increased.

			\paragraph{Zig-zag product ($G \zigzag H$)} Let $G$ be a $(n,D,\lambda_1)$-expander and $H$ a $(D,d,\lambda_2)$-expander. Note that the degree of the first graph is the number of vertices in the second! The goal of this product is to decrease the degree, without changing expansion too much. \\
			The idea is very similar to that we saw in Lecture 9 to derandomize algorithms, and we shall ``derandomize the random walk in $G$ using $H$'' -- instead of going to a random neighbour in $G$, we determine which vertex to travel to using $H$.\\
			$G \zigzag H$ has $nD$ vertices and is $d^2$-regular. Suppose that the edges at each vertex in $G$ are put in bijection with $V(H)$ in some arbitrary manner.
			\begin{itemize}
				\item The vertex set of $G\zigzag H$ is $V(G) \times V(H)$; we shall replace each vertex of $G$ with the vertices of $H$.
				\item Let $v = (a_G,a_H) \in V(G\zigzag H)$ and $(i,j) \in [d]^2$. The $(i,j)$th neighbour $(b_G,b_H)$ of $(a_G,a_H)$ is as follows.
				\begin{enumerate}
					\item Let $a_H'$ be the $i$th neighbour of $a_H$ in $H$.
					\item Let $(b_G,b_H')$ be the $a_H'$th neighbour of $a_H$ in $G$ (recall that we had put the neighbours of a given vertex in bijection with $V(H)$). That is, $b_G$ is the $a_H'$th neighbour of $a_G$ and $a_G$ is the $b_H'$th neighbour of $b_G$.
					\item Let $b_H$ be the $j$th neighbour of $b_H'$.
				\end{enumerate}
			\end{itemize}

			\begin{lemma}
				Suppose that $H$ is a non-bipartite graph, and let $w_H,w_H'$ be any vertices in $H$. Then, $s,t$ are connected in $G$ iff $(s,w_H)$ and $(t,w_H')$ are connected in $G \zigzag H$.
			\end{lemma}
			\begin{proof}
				It suffices to show that for any $v \in V(G)$, the ``cloud'' $v \times V(H)$ is connected. Indeed, the connectivity of clouds is identical to the connectivity of $G$. Consider some arbitrary $a_H,b_H$ with a $2$-walk $a_Hv_Hb_H$ in $H$ between them. Due to the non-bipartiteness of $H$, we are done if we show that there is a path between $(v,a_H)$ and $(v,b_H)$. Indeed, considering some neighbour $(w,c_H)$ of $(v,a_H)$ in $G\zigzag H$ such that $w$ is the $v_H$th neighbour of $v$, it is seen that both $(v,a_H)$ and $(v,b_H)$ are adjacent to $(w,c_H)$ in $G \zigzag H$, so we are done.
			\end{proof}

			% preserves connectivity -- why?
			% (s,t) just check any vertex in the s cloud and t cloud -- why is each cloud connected??? CHECK!!!

			What happens to the eigenvalue of $G\zigzag H$? For the sake of simplicity, let $\gamma_1 = 1-\lambda_1$ and $\gamma_2 = 1-\lambda_2$ be the corresponding spectral gaps. We claim that
			\begin{flem}
				\label{lem: zigzag spectral gap}
				It is true that
				% \begin{equation}
				\[ \gamma(G\zigzag H) \ge \gamma_1 \gamma_2^2. \]
				% \end{equation}
			\end{flem}
			We prove the above shortly, and first describe how this results in a conversion of our graph to an expander graph.\\
		
			First of all, how do we construct the $(D,d,\lambda_2)$ graph $H$?\\
			There exists a simple method to construct a $(D^4,D,1/8)$ graph $H$ that we do not describe.\\
			To generate a larger graph with bounded spectral value, we can do the following.
			\begin{enumerate}
				\item Set $G_1$ as $H^2$.
				\item For each $k$, set $G_{k+1}$ as $G_k^2 \zigzag H$.
			\end{enumerate}
			We claim that $G_k$ is a $(D^{4k},D^2,1/2)$ graph (by $1/2$ we mean that the eigenvalue is at most $1/2$).\\
			The first two parameters are easily verified.
			It may be shown using \Cref{lem: zigzag spectral gap} without much trouble that $\lambda(G \zigzag H) \le \lambda_1 + 2\lambda_2$.\\
			Then, an inductive argument yields the eigenvalue bound for $G_k$. Indeed, $\lambda(G_1) \le 1/2$ and $\lambda(G_{k+1}) \le \lambda(G_k)^2 + 2 \lambda(H) \le (1/2)^2 + 2/8 = 1/2$.
			Before coming to the proof of \Cref{lem: zigzag spectral gap}, we describe the construction used to convert $G$ to a graph $G'$ satisfying the conditions described earlier.\\
			\begin{algorithm}[H]
				\DontPrintSemicolon
				\SetNoFillComment
				\SetKwProg{Fn}{}{}{}
				\SetKwFunction{regularize}{{regularize}}
				% \Fn{\isPath{$s,t,k=2^{\ell}$}} {
				\KwIn{An graph $G$}
				\KwOut{A graph $G'$ each of whose components is an expander}
				$G_0 \gets \regularize(G)$\;
				\For{$1 \le k \le O(\log n)$} {
					$G_{k+1} \gets G_k^2 \zigzag H$
				}
				\caption{Converting an arbitrary graph to an expander}
			\end{algorithm}
			First, make $G$ a regular graph. % that weird 3-regular thing prof describes
			$G_0$ is an $(n,D^2,1-\frac{1}{\poly(n)})$ graph.
			We claim that $G_k$ is an $(nD^{4k},D^2,17/18)$.\\
			The first two parameters are straightforward. Let $\gamma_k = 1 - \lambda(G_k)$. Assume that for some $k$ $\gamma_{k-1} \le 1/18$. Then,
			\begin{align*}
				\gamma_{k+1} &\ge \left(1 - \left(1 - \gamma_k\right)^2\right) \left( \frac{7}{8} \right)^2 \\
					&= (2\gamma_k - \gamma_k^2) \frac{49}{64} \\
					&\ge \left( \frac{35}{18} \gamma_k \right) \frac{49}{64} \ge \frac{5}{4} \gamma_k.
			\end{align*}
			Consequently, $\gamma$ increases from $1/\poly(n)$ to a constant in $O(\log n)$ steps!

			The only thing that remains is to prove \Cref{lem: zigzag spectral gap}.

			\begin{lemma}
				\label{lem: error of walk from complete}
				Let $C$ be a random walk matrix with eigenvalue $\lambda$. Then, it is possible to write $C = (1-\lambda)J + \lambda E$ for some matrix $E$ with spectral norm %\footnote{the largest absolute value of an eigenvalue}
				equal to $1$.
			\end{lemma}
			\begin{proof}
				Let $v$ be a vector, and let $v_1,v_2$ be its components along and orthogonal to the all ones vector. Observe that all eigenvalues of $J$ other than the first are equal to $0$. As a result,
				\begin{align*}
					\norm{Ev} &= \frac{1}{\lambda} \norm{Cv - (1-\lambda)Jv} \\
						&= \frac{1}{\lambda} \norm{v_1 + Cv_2 - (1-\lambda)v_1} \\
						&= \frac{1}{\lambda} \norm{\lambda v_1 + Cv_2} \\
						&= \frac{1}{\lambda} \sqrt{\lambda^2 \norm{v_1}^2 + \norm{Cv_2}^2} \\
						&\le  \frac{1}{\lambda} \sqrt{\lambda^2 \norm{v_1}^2 + \lambda^2 \norm{v_2}^2} = \norm{v}.
				\end{align*}
				The above inequality is tight on setting $v_2$ as the second eigenvector of $C$, so the spectral norm is precisely $1$.
			\end{proof}

			\begin{proof}[Proof of \Cref{lem: zigzag spectral gap}]
				Let $B$ be the random walk matrix of $H$, and let $\tilde{B} = I_n \otimes B$.\footnote{this is a $nD \times nD$ matrix, with $n$ blocks on the diagonal that are all $B$s.} Let $\tilde{A}$ be the matrix that encodes the second edge (using $G$-edges) traversed in the construction of the zigzag product. Then, the transition matrix $M$ of $G \zigzag H$ is just $\tilde{B} \tilde{A} \tilde{B}$!\\
				Now, denote by $J$ the matrix that has all elements $1/n$. 

				Now, use \Cref{lem: error of walk from complete} on $\tilde{B}$.
				\begin{align*}
					M &= \tilde{B} \tilde{A} \tilde{B} \\
						&= \left( \gamma_2 \tilde{J} + \lambda_2 \tilde{E} \right) \tilde{A} \left( \gamma_2 \tilde{J} + \lambda_2 \tilde{E} \right) \\
						&= \gamma_2^2 \tilde{J} \tilde{A} \tilde{J} + \gamma_2(1-\gamma_2) \left( \tilde{E}\tilde{A}\tilde{J} + \tilde{J}\tilde{A}\tilde{E} \right) + (1-\gamma_2)^2 \tilde{E}\tilde{A}\tilde{E} \\
						&= \gamma_2^2 \tilde{J} \tilde{A} \tilde{J} + (1-\gamma_2^2) X,
				\end{align*}
				for some matrix $X$. Observe that the spectral norm of $X$ is at most $1$ because $\tilde{E},\tilde{A},\tilde{J}$ all have spectral norms at most $1$, and so by submultiplicativity of the spectral norm,\footnote{$\norm{AB} \le \norm{A}\norm{B}$. This is obvious because $\norm{ABx} \le \norm{A}\norm{Bx} \le \norm{A}\norm{B}\norm{x}$.} $\tilde{E}\tilde{A}\tilde{J}$, $\tilde{J}\tilde{A}\tilde{E}$, and $\tilde{E}\tilde{A}\tilde{E}$ all have spectral norms at most $1$.\\
				Now, we wish to bound the second eigenvalue of $M$. Note that $\tilde{J}\tilde{A}\tilde{J}$ is precisely the random walk matrix of $G \otimes K_n$, which has second eigenvalue equal to that of $G$. Recalling very carefully that the second eigenvector of $M$ is some vector $v$ orthogonal to $\textbf{1}$, we have
				\begin{align*}
					\norm{Mv}_2 &= \norm{\gamma_2^2 \tilde{J} \tilde{A} \tilde{J} x} + \norm{(1-\gamma_2^2)X x} \\
						&\le \left(\gamma_2^2 \lambda_1 + (1-\gamma_2^2)\right) \norm{v} = (1-\gamma_2^2\gamma_1) \norm{v},
				\end{align*}
				completing the proof.
			\end{proof}
			
	\subsection{Lecture 9: Expander graphs}

		\subsubsection{Lecture 9}

			\begin{fdef}
				A graph $G$ is said to be an \emph{$(n,d,\lambda)$-expander} if $|V(G)| = n$, $G$ is $d$-regular, and $\lambda$ is the second largest eigenvalue $\lambda(G)$ of $G$ in absolute value.
			\end{fdef}

			\begin{fdef}[Spectral expanders]
				A sequence $\{G_n\}_{n \ge 0}$ of $d$-regular graphs %(where the size of the graphs increases with $n$)
				is said to be a \emph{spectral expander family} if for some $\lambda < 1$, $\lambda(G_i) \le \lambda$ for all $i$.
			\end{fdef}

			We saw last lecture that random walks on expander graphs converge to the uniform distribution in $O(\log n)$ steps.\\
			This means that there are only $d^{O(\log n)} = \poly(n)$ paths to explore. Therefore, there is a \emph{deterministic} polynomial time algorithm to determine connectivity of expander graphs.

			\begin{fdef}[Sparsity]
				Given a $d$-regular graph $G$, define the \emph{sparsity}
				\[ h(G) = \min_{\substack{S \subseteq V \\ |S| \le (n/2)}} \frac{|E(S,\overline{S})|}{d|S|}. \]
			\end{fdef}
			Some definitions use $|E(S,\overline{S})|/|S|$ instead.\\
			It is natural to see that $h(G)$ measures (in some sense) how well-connected a graph is. If it is low, there is some ``bottleneck'' in the graph where the random walk can get stuck -- a set of high measure with very few outgoing edges.\\
			It is clear that $h(G) \le 1$.

			\begin{fdef}[Combinatorial expanders]
				A sequence $\{G_n\}_{n \ge 0}$ of $d$-regular graphs %(where the size of the graphs increases with $n$)
				is said to be a \emph{combinatorial expander family} if for some $h > 0$, $h(G_i) \ge h$ for all $i$.
			\end{fdef}

			\begin{ftheo}[Cheeger's Inequality]
				\label{cheeger inequality}
				For any graph $G$ with second eigenvalue $\lambda_2$ and sparsity $h$,
				\[ \frac{1-\lambda_2}{2} \le h \le \sqrt{2(1-\lambda_2)}. \]
				In particular, spectral expanders are combinatorial expanders and vice-versa.
			\end{ftheo}
			We prove this later.\\

			\emph{Markov chain Monte Carlo} methods find many uses nowadays in problems such as sampling random spanning trees, random independent sets etc. The idea in these is that we start with an arbitrary spanning tree (say), and then randomly move to a ``neighbouring'' spanning tree -- add a random edge not in the spanning tree and remove a random edge from the cycle thus formed. After sufficiently many steps, we are at a(n almost) uniformly random spanning tree. This massive graph composed of spanning trees as vertices ends up being an expander! Because the graph of spanning trees has only exponentially many vertices, we get a polynomial time algorithm to randomly sample spanning trees.

			\begin{fex}
				The $n$-cycle $C_n$ has sparsity $h(C_n) = 2/n$, and $\lambda(C_n) = \cos(2\pi/n) \approx 1 - (2\pi/n)^2$.\\
				The hypercube graph $H_n \coloneqq P_2^{\otimes n}$. We have $h(H_n) = 1/n$ and $\lambda(H_n) = 1 - \frac{1}{k}$.\\
				Each of these give a case where the appropriately inequality in \nameref{cheeger inequality} is (asymptotically) tight.
			\end{fex}

			What guarantee do we even have that expanders exist? It turns out that a random $d$-regular graph is a (combinatorial) expander with high probability!\\
			
			However, how do we construct expander graphs? Our goal is to use expander graphs to reduce randomness in algorithms, so it does not make sense to construct them using the above random argument. We also want the algorithm itself to run in polylog time -- this requirement makes sense in light of our remarks towards the end of \nameref{lec 6}.

			\begin{fex}
				Let $p$ be a prime and consider the $3$-regular graph over $\F_p^*$, where each $x$ is adjacent to $x+1,x-1,x^{-1}$. This graph is an expander, but the proof of this is not very straightforward.
			\end{fex}

			\begin{ftheo}[Expander Mixing Lemma]
				Let $G$ be a $(n,d,\lambda)$-expander. Then, for any $S,T \subseteq V$,
				\[ \left| E(S,T) - \frac{d}{n}|S||T| \right| \le d\lambda \sqrt{|S||T|}. \]
			\end{ftheo}
			If $G$ were a random graph, then the expected number of edges between $S,T$ is precisely $(d/n)|S||T|$ -- of the $d|S|$ edges out of $S$, we expect a $|T|/n$ fraction to be incident on $T$.\\
			% This can be used to save randomness -- $E(S,T)/dn - (|S|/n)(|T|/n)$.
			\begin{proof}
				Let $M$ be the transition matrix of the random walk on $G$; it is equal to $(1/d)$ times the adjacency matrix of $G$. For any set $X$, let $\indic_X$ be the indicator vector of $X$ with $1$s at vertices in $X$ and $0$ elsewhere.\\
				Observe that
				\[ \frac{1}{d}E(S,T) = \indic_S^\top M \indic_T. \]
				Now, we have $M = \sum_i \lambda_i u_i u_i^\top$ using the spectral theorem, where $(u_i)$ are orthonormal eigenvectors of $M$ with corresponding real eigenvalues $(\lambda_i)$. Note in particular that $\lambda_1 = 1$ and $u_i$ is the vector with all coordinates having value $1/\sqrt{n}$.\\
				Let $\indic_S = \sum_i \alpha_i u_i$ and $\indic_T = \sum_i \beta_i u_i$. Note in particular that $\alpha_1 = \langle\indic_S,u_1\rangle = |S|/\sqrt{n}$ and $\beta_1 = |T|/\sqrt{n}$. \\
				Using orthonormality,
				\begin{align*}
					\frac{1}{d}E(S,T) &= \left( \sum_i \alpha_i u_i \right) \left( \sum_i \lambda_i u_iu_i^\top \right) \left( \sum_i \beta_i u_i \right) \\
						&= \sum_i \alpha_i \beta_i \lambda_i \\
						&= \alpha_1\beta_1 + \sum_{i=2}^n \alpha_i\beta_i \lambda_i \\
						&= \frac{|S||T|}{n} + \sum_{i=2}^n \alpha_i \beta_i \lambda_i.
				\end{align*}
				Therefore,
				\begin{align*}
					\left| E(S,T) - \frac{d}{n} |S||T| \right| &= \left|\sum_{i=2}^n \alpha_i \beta_i \lambda_i \right| \\
						&\le d\lambda \sum_{i=2}^n \left|\alpha_i \beta_i \right| \\
						&\le d\lambda \sqrt{ \left( \sum_{i=2}^n \alpha_i^2 \right) \left( \sum_{i=2}^n \beta_i^2 \right) } \\
						&\le d\lambda \sqrt{\norm{\indic_S}\norm{\indic_T}} = d\lambda\sqrt{|S||T|}.
				\end{align*}

				% Letting $J$  be the all $1$s matrix of appropriate size,
				% \begin{align*}
				% 	\left| e(X,Y) - \frac{d}{n}|X||Y| \right| &= \left| \indic_X^\top A \indic_Y - \frac{d}{n} \indic_X J \indic_Y \right| \\
				% 		&= \left| \indic_X^\top \left( A - \frac{d}{n} J \right) \indic_Y \right| \\
				% 		&\le \norm{\indic_X^\top} \norm{A - \frac{d}{n} J} \norm{\indic_Y}.
				% \end{align*}
				% We claim that $\norm{A - (d/n)J} \le \lambda$. Since $A$ is symmetric with all row sums as $d$, it commutes with $J$, and so they can be simultaneously diagonalized. Further, $n$ is an eigenvalue of $J$ and $d$ is an eigenvalue of $A$ with eigenvector $\mathbf{1}$.
			\end{proof}

			We now see how to save randomness using expanders.\\
			Let $\mathcal{A}$ be an algorithm that uses $k$ independent random bits. Let $G$ be a $(2^k,d,\lambda)$-expander. Starting at an arbitrary vertex $v_1$, perform a random walk for $\ell$ steps through vertices $v_1,v_2,\ldots,v_{\ell}$. Run the algorithm on each of these inputs $v_1,\ldots,v_{\ell}$ (interpreting the $2^k$ elements of $V(G)$ as length $k$ bit strings.\\
			Recall that if $\mathcal{A}$ once (using $k$ bits) has error probability $\beta$, running the algorithm $\ell$ times (using $k\ell$ bits) reduces this to error probability $\beta^\ell$. It turns out that running the algorithm $\ell$ times as described above (using $k+\ell\log d$ bits) reduces the error probability to $(\beta+\lambda)^{\ell}$!\\
			In purely graph theoretic terms, this says the following.

			\begin{ftheo}
				Let $G$ be a $(n,d,\lambda)$-expander, and let $B \subseteq V$ be of size $\beta n$. Starting at a random vertex $v_1$, consider $\ell$ steps of the random walk going through vertices $v_1,v_2,\ldots,v_{\ell}$. Then,
				\[ \Pr \left[ \text{all $v_i$ are in $B$} \right] \le (\beta+\lambda)^\ell. \]
			\end{ftheo}
			\begin{proof}[Proof sketch]
				Consider the diagonal matrix $D$ with $1$s at vertices in $B$ and $0$ elsewhere. Let $p^{(0)}$ be the initial (uniform) distribution of $v_0$. Observe that the $\ell_1$ norm of $(BM)^{i} p^{(0)}$ is precisely the probability that $v_i$ is in $B$. To bound this, we split a given vector into its component along and orthogonal to the uniform distribution. The norm of the second part decreases by $\lambda$ at every step.
			\end{proof}

			\begin{ftheo}[Alon-Boppana bound]
				For any $(n,d,\lambda)$-expander,
				\[ \lambda = \frac{2\sqrt{d-1}}{d}(1 - o(1)). \]
			\end{ftheo}

			\begin{fdef}[Ramanujan Graph]
				A $(n,d,\lambda)$-expander is said to be a \emph{Ramanujan graph} if
				\[ \lambda \le \frac{2\sqrt{d-1}}{d}. \]
			\end{fdef}

			It was proved in 2014 by Adam Marcus, Daniel Spielman, and Nikhil Srivastava that there exist infinite families of bipartite Ramanujan graphs of every degree greater than $2$.

	\subsection{Lecture 11: Reed-Solomon Codes}
		% guest lecturer: prof mrinal

		\subsubsection{Lecture 11}

			Randomly choosing strings from $\{0,1\}^n$ tends to yield a good code, with high distance between points with high probability. This is true even for exponentially many points, say $2^{0.1n}$. At the heart of getting good codes is derandomizing this process to get good explicit codes.\\
			In some sense, good codes, expander graphs, and extractors are equivalent. In fact, some of the best known expander constructions today come from coding theoretic constructions.\\
			The subject of coding theory lies at the intersection of numerous disparate fields, such as (theoretical) computer science, electrical engineering, and math. Interestingly, the same objects are studied in all the disciplines, merely from different perspectives.\\

			\begin{fdef}[Reed-Solomon Code]
				Let $\F$ be a finite field, and $k,n \in \N$ with $n \ge k,|\F|$. Also fix some distinct $\alpha_1,\ldots,\alpha_n$. The message space of the \emph{Reed Solomon code} $\RS(k,n)$ is
				\[ \{ g(x) \in \F[x] : \deg(g) \le k-1 \}. \]
				That is, we identify $k$-dimensional vectors in $\F^k$ with corresponding polynomials.
				A polynomial $g$ is encoded as
				\[ \Enc(g) = ( g(\alpha_1),\ldots,g(\alpha_n) ). \]
			\end{fdef}
			Note that the number of possible messages if $|\F|^k$. Let us look at some basic properties of this code.
			\begin{enumerate}
				\item A Reed-Solomon code is a linear code. This follows immediately from the fact that $(\alpha g+h)(\alpha_i) = \alpha g(\alpha_i) + h(\alpha_i)$, and if $g,h$ are of degree at most $k-1$ then so is $\alpha g + h$.
				\item The code has rate $k/n$.
				\item The distance of the code is $n-k+1$. Indeed, by the Fundamental Theorem of Algebra, two polynomials can coincide in value at at most $k-1$ points (otherwise their difference, a nonzero polynomial of degree at most $k-1$, would have more than $k-1$ roots).
			\end{enumerate}
			Observe that given a vector $(g_1,\ldots,g_k) \in \F^k$, we encode it as
			\[ \begin{pmatrix} 1 & \alpha_1 & \alpha_1^2 & \cdots & \alpha_1^{k-1} \\ 1 & \alpha_2 & \alpha_2^2 & \cdots & \alpha_2^{k-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & \alpha_n & \alpha_n^2 & \cdots & \alpha_n^{k-1} \end{pmatrix} \begin{pmatrix} g_1 & g_2 & \vdots & g_k \end{pmatrix}. \]
			Also note that the above properties do not depend on our choice of $(\alpha_i)$.\\

			It turns out that Reed-Solomon codes are optimal in some sense.
			\begin{ftheo}%[Singleton]
				Reed-Solomon codes match the singleton bound.
			\end{ftheo}
			This says that over large fields, they essentially match the best possible rate-distance trade-off.

			The matrix multiplication scheme above describes a simple way to encode RS codes. How do we decode them? That is, if $r = (r_1,\ldots,r_n) \in \F^n$, what do we decode it to? This asks to find the RS codeword $c$ closest to $r$; find \emph{the} RS codeword $c$ such that $d_H(c,r) < (n-k+1)/2$ (if no such codeword exists, say no). \\ %sometimes called unique decoding regime

			In the 60s, numerous decoding algorithms such as those of Peterson, Sugiyama, and Berlekamp-Massey were proposed. Despite being elementary, these are all very clever. In practice, Berlekamp-Massey is typically used (it is nearly linear in the input size).\\
			We shall look at the Welch-Berlekamp algorithm, which was proposed in the 80s. Amusingly, this was not originally published in a paper but in a patent. Some time later, Madhu Sudan et al. decoded\footnote{Pardon the pun.} the patent. The algorithm has had surprisingly pervasive effects in computer science, and the methods have helped resolve several open problems in math as well. Interested readers can see the ``polynomial method'' for more details; while it has been used in the past in math, this introduced it to mainstream theoretical computer science. \\
			The algorithm is as follows.
			% The question is just \emph{noisy} polynomial interpolation.

			\begin{enumerate}
				\item Find a nonzero polynomial $Q(x,y) = A(x) + yB(x)$ such that
				\begin{enumerate}
					\item $\deg(B) < \left\lceil (n-k+1)/2 \right\rceil \eqqcolon d$,
					\item $\deg(A) < d + (k-1) \eqqcolon D$, and
					\item for all $i$, $Q(\alpha_i,r_i) = 0$.
				\end{enumerate}
				\item Set $g(x) = -A(x)/B(x)$. If $g$ is a polynomial and $d_H(\Enc(g),r) < (n-k)/2$ and $\deg(g) \le k-1$, output $g$. Otherwise, say that no codeword exists.
			\end{enumerate}

			Observe that if $g$ is a polynomial, the polynomial is certainly correct.\\
			We must prove that if $g$ is not a polynomial, then no codeword exists. First, however, let us describe how to determine the bivariate polynomial $Q$.\\
			Set $D = (n+k)/2$ and $d = (n-k)/2$. The desired $Q$ is of the form
			\[ Q(x,y) = A_0 + A_1 x + \cdots + A_{D-1} x^{D-1} + B_0y + B_1xy + \cdots + B_{d-1} x^{d-1}. \]
			Then, we wish to determine coefficients $(A_i)$ and $(B_i)$ such that for all $i$
			\[ Q(x,y) = A_0 + A_1 \alpha_i + \cdots + A_{D-1} \alpha_i^{D-1} + B_0 r_i + B_1 r_i\alpha_i + \cdots + B_{d-1} r_i\alpha_i^{n-1} = 0. \]
			This is just a linear system of $n$ equations. If $D+d > n$, which is indeed true, we may solve it for a non-trivial solution.\\
			To compute $g$ using $A,B$, we just have to do polynomial long division from high school (there are better methods to do this).\\
			All that remains is to check correctness.

			\begin{ftheo}
				If there exists a polynomial $h \in \F[x]$ of degree at most $k-1$ such that $d_H(\Enc(h),r) < (n-k+1)/2$, the Welch-Berlekamp algorithms outputs it.
			\end{ftheo}
			\begin{proof}
				Let $Q(x,y) = A(x) + yB(x)$ with $\deg(A) < D$, $\deg(B) < d$, and $Q(\alpha_i,r_i) = 0$ for all $i$ be the polynomial output in the first step of the algorithm.\\

				Consider $U(x) = Q(x,h(x))$. This is $A(x) + h(x) B(x)$. For starters, the degree of $U$ is at most $D-1 < (n+k-1)/2$. If $h(\alpha_i) = r_i$, then $U(\alpha_i) = 0$. Consequently, the number of zeroes of $U$ on $\{\alpha_1,\ldots,\alpha_n\}$ is at least the number of agreements between $\Enc(h)$ and $r$. Due to the distance contraint on $h$, there are at least $(n+k-1)/2$ such agreements.\\
				Therefore, $U$ has more zeros than degree, so $U$ must be the zero polynomial (!) and therefore $g$ is indeed a polynomial and equal to $h$.
			\end{proof}
			The idea of the construction is just that the first step forces us to have a $Q$ of large degree, while the second (assuming a valid $h$ exists) forces $Q$ to have small degree. The sweet spot in the middle is precisely where we lie.\\

			Next class, we shall look at decoding Reed-Solomon codes beyond the error limit of half the minimum distance. One option is to output any codeword in the given radius. The more useful (albeit more stringent) notion is to output all these codewords. Such algorithms are called ``list decoding algorithms''.
			We must ensure that the amount we go beyond $(n-k+1)/2$ is not so high that the list becomes exponentially large. This is guaranteed by the following.

			\begin{ftheo}[Johnson] % Johson*?
				Let $\mathcal{C}$ be a code of block length $n$ with distance $\Delta$. Then, for all $r \in \F^n$, the number of codewords in $\mathcal{C}$ within distance $n - \sqrt{n(n-\Delta)}$ (the ``Johnson radius'') of $r$ is at most $\poly(n)$.
			\end{ftheo}

			For Reed-Solomon codes, this value is equal to
			\[ n - \sqrt{n\left( n - (n-k+1) \right)} \approx n-\sqrt{nk}. \]
			This is \emph{far} larger than the minimum distance. If $k = 0.01n$, say, then the minimum distance is about $0.49n$, but the Johnson radius is about $0.9n$! Next class, we shall describe how to output all the codewords within the prescribed radius.\\
			We also remark that there exist Reed-Solomon codes where we go beyond the Johnson radius while still having polynomially many codewords in the given radius. The question of when this is possible does not have many satisfactory answers to date.