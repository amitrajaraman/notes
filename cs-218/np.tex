\section{NP Hardness and Reductions}

Suppose we have $n$ jobs that take time $S_1,\ldots,S_n$ each and two processors $P_1$ and $P_2$. We wish to schedule the jobs on these two processors such that the overall time taken (the time when the last job completes) is minimized. How do we do this?\\

One idea is that we initially schedule $S_1$ and $S_2$ and when one of the jobs ends, we schedule the next job $S_3$ on the processor that is freed. We repeat this, scheduling jobs whenever processors become free.\\
This does not work -- consider the set of times $\{1,1,100\}$. Our scheduling produces time $101$ (we schedule the first two jobs first), but it is quite easy to see that it can be done in time $100$.\\

The issue seems to be that the ordering of the jobs is problematic. So instead, schedule the jobs in non-increasing order and repeat the above algorithm. Does this work?\\
No, it does not. Consider the set of jobs $\{3,3,2,2,2\}$ -- our algorithm gives a time of $7$ but a time of $6$ can be attained.\\

It in fact turns out that it is impossible (or at least extremely hard) to get an efficient (polynomial time) algorithm for this. It is quite surprising that we solved the interval-scheduling problem without too much difficulty, yet this is incredibly hard.\\
How do we show that this problem is ``hard'' though? What does ``hard'' even mean?

\subsection{Overview and some Definitions}

\begin{fdef}
	A problem $\mathsf{\Pi}$ is said to be in the class $\mathsf{P}$ if there exists an algorithm $\mathcal{A}$ such that for any input $x$, $\mathcal{A}$ finds the correct solution for $x$ in ttime $\textsf{poly}(|x|)$ time.
\end{fdef}

Searching, sorting, interval scheduling, primality testing, max-flow, finding a perfect matching in a bipartite graph are just a few examples of important problems in $\mathsf{P}$.\\
Before we get to the definition of $\mathsf{NP}$, let us start with a few examples.

\paragraph{The $3$-colorability problem.}
Given a graph $G=(V,E)$, check if there exists a $3$-coloring of $G$ -- a function $f:V\to\{1,2,3\}$ such that for any $e=uv \in E$, $f(u) \neq f(v)$.\\
One can think a bit and struggle to come up with an algorithm for the above. Now, suppose someone gives you a $c:V\to\{1,2,3\}$. Then in polynomial time, one can test whether or not this is a valid $3$-colouring (by checking the colours of the two vertices on every edge). That is, while it is difficult to come up with a solution for the algorithm, it is easy to test whether a given input is a solution or not.

\paragraph{The $k$-clique problem.}
Given a graph $G=(V,E)$, does $G$ have a clique of size $k$?\\
A brute-force algorithm would require $\mathcal{O}(n^k)$ time, which \textit{is} polynomial for a fixed $k$. It is not polynomial if we have, say, $k=\sqrt{n}$ however.\\
As in the previous example, for a given subset $S\subseteq V$, we can easily test whether or not it is a $k$-clique (by checking that it has cardinality $k$ and if there is an edge between any two vertices in $S$).

\paragraph{The satisfiability problem.}
Given a CNF formula $\varphi$ over variables $x_1,\ldots,x_n$, does there exist an assignment $\tilde{a}$ such that $\tilde{a}$ satisfies $\varphi$?\\
If $\varphi$ is satisfiable, then there exists an assignment $\tilde{a}$ that ``witnesses'' this. As in the previous examples, it is difficult to come up with an algorithm that finds a satisfying assignment (or determine satisfiability). It is easy to check if a given assignment is satisfying, however.

This sort of property where checking is easy is what leads to the definition of $\mathsf{NP}$.

\begin{fdef}
	A problem $\mathsf{\Pi}$ is said to be in $\mathsf{NP}$ if there is a polynomial time algorithm $\mathcal{T}$ such that
	\begin{itemize}
		\item If input $x$ is a positive instance of $\Pi$ then there is a polynomial length proof $y$ such that $\mathcal{T}$ on inputs $x,y$ outputs \texttt{Yes}.
		\item If input $x$ is a negative instance of $\Pi$ then for any polynomial length proof $y$ such that $\mathcal{T}(x,y)$ outputs \texttt{No}.
	\end{itemize}
\end{fdef}

For example, in the $3$-colorability problem, $x$ would be the graph $G=(V,E)$, $y$ would be a function $c:V\to\{1,2,3\}$, and $\mathcal{T}$ would be the algorithm that checks if $c$ is a $3$-coloring.\\
The ``proof'' just serves as the ``witness'' we mentioned earlier -- an example that proves correctness.\\

Note that the problem $\Pi$ itself is just a yes-no problem. The input $x$ is a positive instance if the answer to $\Pi$ for $x$ is yes and a negative instance if the answer is no.\\

The first important observation to make is that any problem in $\mathsf{P}$ is a problem in $\mathsf{NP}$. For any $y$, we can take $\mathcal{T}$ as the algorithm that just solves the problem and answers yes or no. We ignore the proof and merely determine if $x$ is a positive or negative instance.\\
Therefore,
\[ \mathsf{P} \subseteq \mathsf{NP}. \]

What if the problem itself is not a yes-no problem -- perhaps a search or optimization problem?\\
A problem where the answer is just yes or no is referred to as a \textit{decision problem}. For example, $3$-colorability and primality testing.\\
In a \textit{search problem}, we are asked to find a solution. For example, finding a path between a pair of vertices, sorting, and finding a $3$-coloring.\\
In an \textit{optimization problem}, we are asked to find a ``best'' solution with respect to some optimization parameter. For example, max-flow, min-cut, longest common subsequence.\\

Any search problem has a decision problem variant. If a search problem asks to find a specific solution, the decision problema asks whether there is a solution.\\
For example, finding a $3$-colouring changes to asking whether a $3$-coloring asks.\\
Observe that if we can solve a search problem, we can solve its decision variant as well.\\

Similarly, suppose we are given an optimization problem that asks to find a solution that maximizes/minimizes some parameter. Then the decision variant asks if there is a solution that has parameter at least/at most $v$?\\
For example, finding a max-flow changes to asking if there is a flow of value at least $v$.\\
If we can solve an optimization problem, we can solve its decision variant as well.\\

Now, consider the decision variant of the scheduling problem we discussed earlier -- given $d_1,\ldots,d_n$ and $T$, does there exist a scheduling on two processors such that the total time is at most $T$? It is easy to see that this problem is in $\mathsf{NP}$. Given a scheduling (a proof), we can just find the total time it takes and thus verify if this is at most $T$.

\subsection{Reduction}

Reduction essentially means using other algorithms for problems we wish to solve. For example, consider the problem of determining the existence of a perfect matching in a graph $G=(V,E)$.\\
On the other hand, consider the problem that finds a maximum matching in a graph $G=(V,E)$.\\
It is not too difficult to see that the first problem can be ``reduced'' to the second -- if we find a maximum matching in the graph, we can check whether it has $|V|/2$ vertices and thus solve the first problem.\\
Can we come up with a reduction that goes the other way round? Before doing this in fact, can we reduce the problem of finding the \textit{size} of a maximum matching in a graph to the first problem? Yes, we can.
\begin{itemize}
	\item If $G$ has a perfect matching, then return $|V|/2$.
	\item Let $G_1$ be a graph with vertex set $V\cup\{v\}$, where $v$ is a fresh vertex that has an edge to every vertex in $V$. If $G_1$ has a perfect matching, then $G$ has a matching of size $(|V|-1)/2$ (Why?), which must be a maximum matching if this is the case.
	\item Continuing on, create $G_{i+1}$ by adding a new vertex that is connected to every vertex of $G_{i}$. Then if $k$ is the minimum index such that $G_k$ has a perfect matching, $G$ has a matching of size $(|V|-k)/2$ -- there cannot be any edges within the new vertices in a perfect matching because this would imply the existence of a perfect matching of $G_i$ for some $i<k$ (Why?). Further, this is a maximum matching. If there was a matching of size $(|V|-k+2)/2$, then $G_{k-2}$ must have a matching (Why?).
\end{itemize}
Since $i$ can go up to $n$, we are calling the perfect matching algorithm only $n$ times. This can be decreased to $\log n$ by using a binary search-like scheme.\\
Now that we have done this, the original problem is not too difficult. Suppose the size of a maximum matching is $l$. We can remove an edge and ask if the remaining graph has a matching of size $l$ (that is, if the size of a maximum matching is still equal to $l$). If no, we discard the edge. The remaining graph will just be a matching, which gives us a maximum matching in the original graph.

\subsection{Coping with NP-hardness}

	How do we deal with hard problems?
	\begin{enumerate}
		\item Come up with some heuristics. There are no provable guarantees, but they work somewhat well in practice.
		\item There may be efficient algorithms for some special cases.
		\item Better than brute force algorithms, where we come up with algorithms that are not polynomial, but better than a na\"{i}ve brute-force algorithm.
		\item Approximation algorithms for optimization problems, where we merely want to approximate the answer. For example, suppose we have a graph such that the size of the largest independent set is $\mathsf{OPT}$. Then in an approximation algorithm, we should be able to find an independent set that is of size not less that $\mathsf{OPT}/c$ for some $c \geq 1$ (this is known as a $c$-approximation algorithm). Similarly, for a minimization problem, we would find a solution of size not more than $c\cdot\mathsf{OPT}$.
	\end{enumerate}
	
	First, let us look at an approximation problem for the scheduling problem.\\
	Given $m$ processors $p_1,\ldots,p_m$ and $n$ jobs $j_1,\ldots,j_n$ with durations $d_1,\ldots,d_n$, find a scheduling for these jobs that minimises the total completion time.\\
	Let $\mathcal{S}$ be a schedule such that
	\begin{itemize}
		\item $A_i$ is the set of jobs scheduled on processor $i\in[m]$.
		\item $T_i = \sum_{j \in A_i} d_j$.
		\item The \textit{completion time} is then $\max_{i \in [m]} T_i$.
	\end{itemize}
	We want to minimize the completion time. Let $\mathcal{S}$ be a scheduling that does so, and let its completion time be $T^*$.\\ 
	While the optimization problem itself is NP-complete, the approximation version has a polynomial time algorithm.\\
	Consider the algorithm that schedules the $j$th job on the machine with the smallest load so far (with the jobs initially arranged in any arbitrary order), and $T$ be the completion time achieved by this schedule.

	\begin{lemma}
		With the above notation, $T \leq 2\cdot T^*$, that is, the above described algorithm gives a $2$-approximation.
	\end{lemma}
	\begin{proof}
		Let $T' = \frac{1}{2} \left(\max_j d_j + \frac{1}{m} \sum_{j\in[n]} d_j\right)$. We trivially have $T' \leq T^*$, since $T^*$ is greater than each of the two quantities involved in the average.\\
		Choose $i\in[m]$ such that $T = T_i$, and let $j\in[n]$ be the last job scheduled on it. Then for any $i' \neq i$, when $j$ was scheduled on $i$, $T_{i'} \geq T_i - d_j$.\\
		Therefore, $\sum_{k\in[n]} T_k \geq m(T_i - d_j)$. That is,
		\[ T = T_i \leq d_j + \frac{1}{m} \sum_{k\in[n]} T_k \leq 2T' \leq 2T^*, \]
		proving the lemma.
	\end{proof}

	If we arrange the jobs in non-increasing order of durations, then the above algorithm gives a $(3/2)$-approximation.

	With this line of thought, one might ask whether there exist questions that are not only hard to solve (exactly), but also hard to approximate? There is extensive literature on this side of things as well, but we do not look at these in this course.