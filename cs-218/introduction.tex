\section{Different Types of Algorithms}

\subsection{Asymptotic Notation}

\subsubsection{Big-\texorpdfstring{$\mathcal{O}$}{} Notation}

Consider the following basic algorithm we use to check primality (checking if any $2\leq i\leq \sqrt{n}$ divides $n$):

\begin{algorithm}
	\DontPrintSemicolon
	\KwIn{A non-negative integer $n$}
	\KwOut{If $n$ is prime, output $\tr$  else $\fa$}
	$\var{flag}\gets\tr$\;
	\For{$i=2$ \KwTo $\sqrt{n}$} {
		\If{$i$ divides $n$} {
			$\var{flag}\gets\fa$
		}
	}
	\Return{$\var{flag}$}
	\caption{Algorithm to check if a number is prime}
\end{algorithm}

Is the above a polynomial time algorithm?

No! It is polynomial in $n$, but \textit{not} polynomial in the input size $\log n$.\footnote{A polynomial time algorithm was described in \cite{AKS-primality}.}\\
While analyzing algorithms in general, it is important to look at the input size, the number of bits that constitute the input.

\begin{definition}[Big-$\mathcal{O}$ notation]
$T(n)$ is said to be $\mathcal{O}(f(n))$ if there is some $c\geq 0$ and $N\in\N$ such that for all $n>N$, $T(n)\leq cf(n)$.
\end{definition}

This notation is abused to say, for example, that $\sqrt{n} = \mathcal{O}(n)$ (instead of the correct $\sqrt{n}\in\mathcal{O}(n)$. We also write $\sqrt{n} = 2^{\mathcal{O}(\log n)}$ sometimes, which means that there is some $f(n)\in\mathcal{O}(\log n)$ such that .

\begin{definition}[Big-$\Omega$ notation]
$T(n)$ is said to be $\Omega(f(n))$ if there is some $c\geq 0$ and $N\in\N$ such that for all $n>N$, $T(n)\geq cf(n)$.
\end{definition}

Finally, we have

\begin{definition}[$\Theta$ notation]
$T(n)$ is said to be $\Theta(f(n))$ if there are some $c_1,c_2\geq 0$ and $N\in\N$ such that for all $n>N$, $c_1 f(n) \leq T(n)\leq c_2 f(n)$.\\
Equivalently, $T(n) \in \Theta(f(n))$ if and only if $T(n) \in \mathcal{O}(f(n))$ and $T(n) \in \Omega(f(n))$.
\end{definition}

\subsubsection{A Few Examples}

We give a few examples with the aim of hopefully making the above notation more clear.

\begin{enumerate}
	\item $\mathcal{O}(n)$. Given an array $\var{A}$ containing $n$ integers ($0$-indexed), find the value of the maximum element in the array.\\
	Consider \Cref{algo: maximum element in array} that takes $\mathcal{O}(n)$ time.

	\begin{algorythm}
		\DontPrintSemicolon
		\KwIn{An array $\var{A}$ containing $n$ integers}
		\KwOut{The maximum element in $\var{A}$}
		$\var{max}\gets\var{A[0]}$\;
		\For{$i=1$ \KwTo $n-1$} {
			\If{$\var{A[i]}>\var{max}$} {
				$\var{max}\gets\var{A[i]}$
			}
		}
		\Return{$\var{max}$}
		\caption{Algorithm to find the maximum element in an array}\label{algo: maximum element in array}
	\end{algorythm}
	
	The bits used to represent each $\var{A}[i]$ is $\log m$. We are assuming that comparison takes constant time. Technically the following algorithm is $\mathcal{O}(n \log m)$, but we often blur the details slightly. To be completely correct, we should say that the algorithm performs $\mathcal{O}(n)$ \textit{comparisons}.

	\item $\mathcal{O}(n \log n)$. Given an array $\var{A}$ containing $n$ elements ($0$-indexed), sort the array.\\
	The merge sort algorithm performs $\mathcal{O}(n\log n)$ comparisons. We do not explicitly write out the algorithm.

	\item $\mathcal{O}(n^2)$. Given $n$ points $p_1=(x_1,y_1),\ldots,p_n=(x_n,y_n)$ in the plane, output a pair $i,j$ such that the distance between $p_i$ and $p_j$ is minimum.

	The algorithm is described in \Cref{algo: closest pair of points}.

	\begin{algorythm}
		\DontPrintSemicolon
		\KwIn{$n\geq 2$ points $p_1=(x_1,y_1),\ldots,p_n=(x_n,y_n)$}
		\KwOut{$i,j\in[n]$ such that the distance between $p_i$ and $p_j$ is minimum}
		$\var{i_1},\var{i_2}\gets 0,1$\;
		$\var{min}\gets (x_1-x_2)^2+(y_1-y_2)^2$\;
		\For{$i=1$ \KwTo $n$} {
			\For{$j=i+1$ \KwTo $n$} {
				$\var{d}\gets (x_i-x_j)^2+(y_i-y_j)^2$\;
				\If{$\var{d}<\var{min}$} {
					$\var{min}\gets\var{d}$\;
					$i_1,j_1\gets i,j$\;
				}
			}
		}
		\Return{$\var{i_1},\var{i_2}$}
		\caption{Algorithm to find a closest pair of points}\label{algo: closest pair of points}
	\end{algorythm}
	This problem can in fact be solved in $\mathcal{O}(n\log n)$ time, which we shall see later (you can try thinking about it now).

	\item $\mathcal{O}(n^k)$. Given a graph $G=(V,E)$, find a $S\subseteq V$ such that $|S|=k$ and there is no edge between any two nodes in $S$.\\
	The above is known as the ``independent set problem''. The counterpart with an edge between any two nodes is known as the ``clique problem''.\\
	This is easily done by just checking every subset of size $k$, of which there are $\binom{n}{k}\mathcal{O}(k^2)=\mathcal{O}(n^k k^2)$ (there are $\mathcal{O}(k^2)$ comparisons for each subset). Since $k$ is constant, this is just $\mathcal{O}(n^k)$.

	So for constant $k$, this \textit{is} technically polynomial, but for reasonably large $k$, this is terrible. In fact, if we want to find the largest clique, then a polynomial time algorithm (in $n$) is not known. Indeed, this is an ``NP-hard'' problem, which we shall read more about later.

\end{enumerate}

\begin{exercise}
Let $\var{A}$ be an array of $n$ distinct numbers. A number at location $1<i<n$ is said to be a maxima in the array if $\var{A}[i-1]<\var{A}[i]$ and $\var{A}[i]>\var{A}[i+1]$. Also, $\var{A}[1]$ is a maxima if $\var{A}[2]<\var{A}[1]$ and $\var{A}[n]$ is a maxima if $\var{A}[n]>\var{A}[n-1]$. Find a maxima in the array in time $\mathcal{O}(\log n)$.
\end{exercise}
\begin{solution*}
	The basic idea behind this algorithm is to, at each step, greedily check the half of the array that contains the greater element among the two neighbours of the current element. It is described more precisely in \Cref{algo: find maxima}. We encourage the reader to prove the correctness of this algorithm.
\end{solution*}
	\begin{algorithm}
		\DontPrintSemicolon
		\KwIn{An array $\var{A}$ containing $n$ elements ($1$-indexed)}
		\KwOut{A maxima in $\var{A}$}
		\SetKwProg{Fn}{}{}{}
		\SetKwFunction{FRecurs}{greedyStep}
		\SetKwFunction{Size}{size}
		\Fn{\FRecurs{$\var{B}$}}{
			\If{$\var{size(B)}=1$} {
				\Return{$\var{B}[0]$}
			}
			$\var{mid}\gets\Size{$\var{B}$}/2$\;
			\If{$\var{B}[\var{mid}]>\var{B}[\var{mid}+1]$ and $\var{B}[\var{mid}]>\var{B}[\var{mid}-1]$} {
				\Return{$\var{B}[\var{mid}]$}\;
			}
			\ElseIf{$\var{B}[\var{mid}]\leq\var{B}[\var{mid}-1]$} {
				\Return{$\FRecurs(\var{B}[1:\var{mid}/2])$}
			}
			\ElseIf{$\var{B}[\var{mid}]\leq\var{B}[\var{mid}+1]$} {
				\Return{$\FRecurs(\var{B}[1+\var{mid}/2:\Size{$\var{B}$}])$}
			}
		}
		\Return{\FRecurs{$\var{A}$}}
		\caption{Algorithm to find a maxima in an array}\label{algo: find maxima}
	\end{algorithm}

\subsection{Greedy Algorithms}

Greed is good, and sometimes, it's even optimal.\\

What is a greedy algorithm? It essentially builds a solution in tiny steps, performing some sort of \textit{local} optimization, which ends up optimizing the problem requirement \textit{globally}.\\
It's quite clear that greedy algorithms needn't always work, we have to pick a local criterion that fits our requirements.\\

\subsubsection{Interval Scheduling}

Consider the ``interval-scheduling problem''. We have a supercomputer on which jobs need to be scheduled. We are given $n$ jobs specified by their start and finish times. That is, for $i\in[n]$, we are given $J_i = (s(i),f(i))$. We wish to schedule as many jobs as possible on the computer. That is, given $J = \{J_i = (s(i),f(i)) : i\in[n]\}$, find the largest subset $S\subseteq J$ such that no two intervals in $S$ overlap.\\
We want maximality in terms of \textit{cardinality} of $S$ (the number of jobs scheduled), not the total time covered.

Consider the following greedy algorithms.

\begin{enumerate}
	\item Choose a job with the earliest starting time. This basically says that we never want to leave the computer idle. It is easily seen that this need not work, since the job that starts soonest can take a very long time, obviously resulting in non-optimality.\\
	More concretely, let $J = \{(0,3),(1,2),(2,3)\}$.

	\item Choose the smallest available job. This needn't work either, since the smallest job could overshadow multiple longer jobs that intersect it.\\
	For example, let $J=\{(1,5),(4,6),(6,10)\}$.

	\item Choose the job with the earliest ending time. This \textit{does} work. The idea behind this is that it tries to keep as many resources free as possible. One can try playing around with a few examples to see that it does work.\\
	But how would we prove that it works?
\end{enumerate}

Let $\mathcal{A}$ be the set selected by the (last) greedy algorithm above and $\mathsf{OPT}$ denote an optimal solution. We wish to show that $|\mathcal{A}|=|\mathsf{OPT}|$.\\
Let $\mathcal{A}=\{a_1,\ldots,a_k\}$ and $\mathsf{OPT}=\{b_1,\ldots,b_m\}$.

\begin{lemma}
For $r\in[k]$, $f(a_r) \leq f(b_r)$.
\end{lemma}
\begin{proof}
This is easily shown via induction. For $r=1$, it holds by the definition of $\mathcal{A}$. For $r>1$, we have (by induction) $f(a_{r-1}) \leq f(b_{r-1}) \leq s(b_r)$. Then since $b_r$ itself can be chosen by the algorithm, we must have that $f(a_r) \leq f(b_r)$.
\end{proof}

This also implies that $\mathcal{A}$ is optimal since at each step, the job corresponding to $\mathsf{OPT}$ can be picked by $\mathcal{A}$. Try showing why this implies $|\mathcal{A}|=|\mathsf{OPT}|$.\\
What is the running time of this algorithm? We first sort the jobs according to their finish times, which takes $\mathcal{O}(n\log n)$ time. We then have to scan through all the jobs in this sorted list, which takes $\mathcal{O}(n)$ times. The total running time is $\mathcal{O}(n\log n)$.

\subsubsection{Minimal Spanning Subgraph}

Now, we consider the ``Minimal Spanning Subgraph'' problem. Given an undirected connected graph $G=(V,E)$ and a cost function $c:E\to\Zp$, find a subset $T\subseteq E$ such that $T$ spans all the vertices, $T$ is connected, and it is the set with the least such cost.

It is easy to show that this $T$ must be a tree. Suppose it is connected, spanning and has a cycle $C$. If we delete the costliest edge $e$ on $C$, then on removing $e$ from $T$, $T$ remains connected and spanning, but the weight goes strictly down (since $c$ maps into $\Zp$).

The brute force approach is to just iterate over all distinct spanning trees, but this is obviously quite terrible (exponential).

As it turns out, nearly no matter what greedy strategy we choose, the optimal solution is attained. We give four such algorithms.
\begin{enumerate}[(i)]
	\item Choose an edge $\alpha$ such that $c(\alpha)$ is minimal. Each subsequent edge is chosen from the cheapest remaining edges of $G$ ensuring that we never form any cycles.
	
	\item At each step, delete a costliest edge that does not destroy the connectedness of the graph.
	
	\item Pick a vertex $x_1$ of $G$. Having found vertices $x_1,\ldots,x_k$ an an edge $x_ix_j$, $i<j$, for each vertex $j$ with $j\leq k$, select a cheapest edge of the form $x_ix$, say $x_i x_{k+1}$, where $1\leq i\leq k$ and $x_{k+1}\not\in\{x_1,\ldots,x_k\}$. The process terminates after we have selected $n-1$ edges.
	
	\item This only works if all the edge costs are distinct. First, for each vertex, select the cheapest edge. After this, repeatedly select a cheapest edge between two distinct connected components until the graph becomes connected.
\end{enumerate}

The first algorithm is also known as Kruskal's algorithm. More concretely, what it does is 

\begin{algorithm}
	\DontPrintSemicolon
	\SetNoFillComment
	\KwIn{A connected graph $G=(V,E)$ with $|V|=n$, $|E|=m$, and a cost function $c:E\to\Zp$}
	\KwOut{A minimal spanning subgraph of $G$}
	\SetKwFunction{Sort}{sort}
	\Sort{$E$,$c$} \tcp*{sort the elements of $E$ in non-decreasing cost}
	$T\gets\emptyset$\;
	\For{$1\leq i\leq m$} {
		\If{$T\cup\{e_i\}$ doesn't have a cycle} {
			$T\gets T\cup\{e_i\}$
		}
	}
	\Return{$T$}
	\caption{Kruskal's Algorithm}\label{algo: kruskel's algorithm}
\end{algorithm}

The set thus formed clearly has no cycles by construction. Spanningness follows due to its maximality. Proving that it is a minimal spanning tree is quite easy in the case where all costs are distinct using the following property of any minimal spanning tree.

\begin{lemma}[Cut Property]
	Let $\emptyset\neq S\subsetneq V$. Let $e=vw$ be the minimal cost edge such that $v\in S$ and $w\in V\setminus S$. Then every minimal spanning tree of the graph must contain $e$.
\end{lemma}

To show that the required follows if we have the cut property, let $T$ be a minimal spanning tree and $T'$ the set output by the algorithm at some intermediate step.\\
Let $e=vw$ be the first edge added to $T'$ in the subsequent steps and $S$ be the neighbourhood of $v$ in $T'$. Since the algorithm was able to add $e$ to $T'$, there is no edge in $T'$ connecting any node in $S$ to any node in $V\setminus S$ (we do not create cycles). We already know that $e$ must be the lowest cost such edge. Then by the cut property, $e$ must be present in $T$!\\
The only issue arises when $v$ is not connected to any vertex in $T'$, but this case is easily resolved.\\
Therefore, $T$ must be minimum spanning.\\

Let us next show that $T$ is connected. Suppose otherwise. There must then be a non-empty $S$ such that no edge from $S$ to $V\setminus S$ is in $T$. However, as $G$ is connected, there is a minimum cost edge that connects $S$ and $V\setminus S$ and by the cut-property, this edge must be in $T$.

\begin{proof}[Proof of the cut-property]
	Suppose we have set $S$ and edge $e$ as in the cut-property. let $T$ be a spanning tree that does not contain $e$. Then adding $e$ to $T$ creates a cycle. Let $P$ be a path in $T$ that connects $v$ to $w$. Let $v'$ be the last vertex along this path in $S$ and $w'$ the first in $V\setminus S$. Let $e'=v'w'$.\\
	Defining $T' = T\setminus\{e'\}\cup\{e\}$, we see that $T'$ has lower cost than $T$, thus completing the proof. 
\end{proof}

Observe that the second algorithm given above is essentially Kruskal's algorithm in reverse.\\

Also note that the third algorithm given above aises quite naturally from the cut property -- it is also known as \textit{Prim's algorithm}.

\begin{algorithm}
	\DontPrintSemicolon
	\SetNoFillComment
	\KwIn{A connected graph $G=(V,E)$ with $|V|=n$, $|E|=m$, and a cost function $c:E\to\Zp$}
	\KwOut{A minimal spanning subgraph of $G$}
	$T\gets\emptyset, S\gets\{v\}$ \tcp*{$v$ is an arbitrary node}
	\While{$|S|<n-1$}{
		Compute $E_s = \{vw\in E:v\in S,w\not\in S\}$\;
		$\tilde{e}\gets\argmin_{e\in E_s}c(e)$\;
		$S\gets S\cup\{w\}$ \tcp*{$w$ is the vertex of $\tilde{e}$ that is in $V\setminus S$}
		$T\gets T\cup\{\tilde{e}\}$\;
	}
	\Return{$T$}
	\caption{Prim's Algorithm}\label{algo: prim's algorithm}
\end{algorithm}

Proving optimality of the set output by the above is easily shown using the cut-property.\\

As a slight detour from what we have done thus far, how would one \textit{implement} Prim's algorithm? We are given an undirected graph $G=(V,E)$ with edge costs given in an adjacency list and a source vertex $s\in V$.\\
We use a method similar to Djikstra's algorithm. Add an arbitary start vertex to the queue with key value $0$ and let all other key values to be $\infty$. At each step, we use a priority queue to extract node with the minimum key value from the queue. Explore the neighbourhood of the node, updating the key value. Here, the key value is the cost of the smallest cost edge leading to a node. The only change here is the update step, everything else is as in Djikstra's.\\
The time complexity of this is the same as Djikstra's, namely $\mathcal{O}((m+n)\log n)$.

\begin{exercise}
	Consider the following modified interval scheduling problem. There are $n$ jobs, each with a start and finish time $(s(i),f(i))$ and in addition, they have a (positive) weight $w(i)$. The problem is to maximize the total weight of the jobs scheduled on a (single) machine. For the sake of brevity, we denote the set of jobs $J$ by a set of tuples with the $i$th tuple being $(s(i),f(i),w(i))$.
	\begin{enumerate}[(a)]
		\item Show that the algorithm for the usual interval scheduling need not give an optimal solution for this problem.
		
		\item Suppose a greedy algorithm picks the largest weight job with the earliest finishing time among the available jobs at each step. Prove/disprove that this strategy works.
		
		\item We say that jobs $J$ and $J'$ \textit{overlap}, denoted $J\| J'$, if $J\cap J'\neq\emptyset$. For a job $J_i$, define $O_i = \sum_{i':J_{i'}\cap J_i} w(i')$, called the \textit{overlap-weight} of $J_i$. Consider a greedy algorithm that schedules a job with the smallest overlap-weight among the available jobs at each step. Prove/disprove that this algorithm gives the optimal solution.
	\end{enumerate}
\end{exercise}

\begin{solution*}
	\begin{enumerate}[(a)]
		\item Consider the set of jobs $J=\{(1,2,5),(1,3,10)\}$. Then the algorithm chooses only the first job, while the optimal solution picks the second.
		
		\item Consider the set of jobs $J=\{(1,2,5),(2,3,5),(1,3,8)\}$. Then this algorithm chooses only the third job, whereas the optimal solution picks the first two.
		
		\item The algorithm is incorrect. Consider the counterexample
		\[ J = \{(0,1,1),(1,2,1),(0,2,3)\}. \]
		Then the algorithm chooses jobs $1$ and $2$, whereas the (unique) optimal solution picks job $3$.\\
		The reader might be tempted to think that the algorithm would work if in the definition of $O_i$, we only consider those $i'\neq i$. However, this doesn't work either. Indeed, consider
		\[ J = \{(1,3,5),(1,2,1),(2,4,5),(3,5,5),(4,5,1)\}. \]
		The algorithm chooses jobs $2$, $3$, and $5$, whereas the (unique) optimal solution chooses jobs $1$ and $4$.\\
		This idea is made more natural on rewriting the problem as finding $\mathcal{A}\subseteq[n]$ that minimizes $w\left(\bigcup_{i\in\mathcal{A}} O(i)\right)$,	where $O(i) = \{i'\in[n] : i'\neq i\text{ and }J_{i'}\|J_i\}$ and $w(S)=\sum_{s\in S}w(s)$ for any $S\subseteq[n]$. The issue arises because a single $j$ might appear in multiple $O(i)$. It is worth noting that there is a dynamic programming algorithm to solve this problem (which we shall study later).
	\end{enumerate}
\end{solution*}


\begin{exercise}
	Suppose we have a computer and some $n$ jobs. For the $i$th job, there are two parts with durations $t_{1}(i),t_{2}(i)>0$. The part of duration $t_{1}(i)$ must be performed on the computer (only one such part can be scheduled at a time) whereas the part of duration $t_{2}(i)$ can be performed at any point after the first part of the same job is done (multiple such parts can be scheduled simultaneously). Give an algorithm that designs an ordering for the jobs to be sent to the computer such that the overall time taken to complete all jobs is minimized.
\end{exercise}
\begin{solution*}
	The problem can be stated alternatively as: given $n$ and two functions $t_1,t_2:[n]\to\Rp$, find a permutation $\sigma$ of $[n]$ such that
	\[ Q_\sigma = \max_{k\in[n]} \left(t_2(\sigma(k)) + \sum_{1\leq i\leq k} t_1(\sigma(i))\right) \]
	is minimized. ($\sigma(i)$ denotes the position that is at the $i$th index after permuting)

	Assume without loss of generality that the jobs are ordered in non-increasing order of $t_2$. We claim that then, the identity permutation suffices. Let $Q$ be the value of $Q_\sigma$ for the identity permutation and $k\in[n]$ attain the maximum involved. Let $\sigma$ be any permutation of $[n]$.

	\textbf{Claim.} If $Q_\sigma\leq Q$, then for every $1\leq i\leq k$, $1\leq\sigma(i)\leq k$. This implies that the first $k$ positions permute among themselves in any optimal solution.\\
	Suppose otherwise and let $r=\max\{j\in[n] : j=\sigma(i)\text{ for some }1\leq i\leq k\}> k$. Then since each of the $(f(i)-s(i))$ are positive,
	\[ Q_\sigma \geq t_2(\sigma(r)) + \sum_{1\leq i\leq r} t_1(\sigma(i)) > t_2(k) + \sum_{1\leq i\leq k}t_1(i) = Q, \]
	thus proving the claim.

	Now, let $\sigma$ be a permutation such that $Q_\sigma \leq Q$. Then by the above claim and since the $t_2$ are in non-increasing order, $t_2(\sigma(k))\geq t_2(k)$. Then
	\[ Q_\sigma \geq t_2(\sigma(k)) + \sum_{1\leq i\leq k} t_1(\sigma(i)) = t_2(\sigma(k)) + \sum_{1\leq i\leq k} t_1(i) \geq Q, \]
	thus implying that the identity permutation is optimal. 
\end{solution*}

\begin{exercise}
	Given a set of $n$ intervals, design a greedy algorithm to find the smallest subset of intervals such that every interval not in the subset overlaps with at least one interval in the subset.
\end{exercise}
\begin{solution*}
	Given a set $X$ of $n$ intervals, consider the graph $G$ whose vertices are elements of $X$ and with an edge between intervals that intersect. Now, recall that a subset of vertices of a graph is a vertex cover if and only if its complement is an independent set. In the context of this problem, an independent set is an interval scheduling whereas a vertex cover is precisely what this problem asks for. Therefore, it suffices to solve the interval scheduling problem on $X$ (in this context, it just corresponds to a maximum independent set) to get a set $\mathcal{A}$ of intervals, and return $X\setminus\mathcal{A}$.
\end{solution*}

% \begin{exercise}
% 	Given a set of $n$ intervals, design a greedy algorithm to find the smallest subset of intervals such that every interval is contained in the union of the intervals of the subset.
% \end{exercise}