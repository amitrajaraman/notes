\section{Different Types of Algorithms}

\subsection{Asymptotic Notation}

\subsubsection{Big-\texorpdfstring{$\mathcal{O}$}{O} Notation}

Consider the following basic algorithm we use to check primality (checking if any $2\leq i\leq \sqrt{n}$ divides $n$):

\begin{algorithm}
	\DontPrintSemicolon
	\KwIn{A non-negative integer $n$}
	\KwOut{If $n$ is prime, output $\tr$  else $\fa$}
	$\var{flag}\gets\tr$\;
	\For{$i=2$ \KwTo $\sqrt{n}$} {
		\If{$i$ divides $n$} {
			$\var{flag}\gets\fa$
		}
	}
	\Return{$\var{flag}$}
	\caption{Algorithm to check if a number is prime}
\end{algorithm}

Is the above a polynomial time algorithm?

No! It is polynomial in $n$, but \textit{not} polynomial in the input size $\log n$.\footnote{A polynomial time algorithm was described in \cite{AKS-primality}.}\\
While analyzing algorithms in general, it is important to look at the input size, the number of bits that constitute the input.

\begin{definition}[Big-$\mathcal{O}$ notation]
$T(n)$ is said to be $\mathcal{O}(f(n))$ if there is some $c\geq 0$ and $N\in\N$ such that for all $n>N$, $T(n)\leq cf(n)$.
\end{definition}

This notation is often abused to say, for example, that $\sqrt{n} = \mathcal{O}(n)$ (instead of the correct $\sqrt{n}\in\mathcal{O}(n)$).

\begin{definition}[Big-$\Omega$ notation]
$T(n)$ is said to be $\Omega(f(n))$ if there is some $c\geq 0$ and $N\in\N$ such that for all $n>N$, $T(n)\geq cf(n)$.
\end{definition}

Finally, we have

\begin{definition}[$\Theta$ notation]
$T(n)$ is said to be $\Theta(f(n))$ if there are some $c_1,c_2\geq 0$ and $N\in\N$ such that for all $n>N$, $c_1 f(n) \leq T(n)\leq c_2 f(n)$.\\
\end{definition}

Equivalently, $T(n) \in \Theta(f(n))$ if and only if $T(n) \in \mathcal{O}(f(n))$ and $T(n) \in \Omega(f(n))$.

\subsubsection{A Few Examples}

We give a few examples with the aim of hopefully making the above notation more clear.

\begin{enumerate}
	\item $\mathcal{O}(n)$. Given an array $\var{A}$ containing $n$ integers ($0$-indexed), find the value of the maximum element in the array.\\
	Consider \Cref{algo: maximum element in array} that takes $\mathcal{O}(n)$ time.

	\begin{algorythm}
		\DontPrintSemicolon
		\KwIn{An array $\var{A}$ containing $n$ integers}
		\KwOut{The maximum element in $\var{A}$}
		$\var{max}\gets\var{A[0]}$\;
		\For{$i=1$ \KwTo $n-1$} {
			\If{$\var{A[i]}>\var{max}$} {
				$\var{max}\gets\var{A[i]}$
			}
		}
		\Return{$\var{max}$}
		\caption{Algorithm to find the maximum element in an array}\label{algo: maximum element in array}
	\end{algorythm}
	
	The bits used to represent each $\var{A}[i]$ is $\log m$. We are assuming that comparison takes constant time. Technically the following algorithm is $\mathcal{O}(n \log m)$, but we often blur the details slightly. To be completely correct, we should say that the algorithm performs $\mathcal{O}(n)$ \textit{comparisons}.

	\item $\mathcal{O}(n \log n)$. Given an array $\var{A}$ containing $n$ elements ($0$-indexed), sort the array.\\
	The merge sort algorithm performs $\mathcal{O}(n\log n)$ comparisons. We do not explicitly write out the algorithm.

	\item $\mathcal{O}(n^2)$. Given $n$ points $p_1=(x_1,y_1),\ldots,p_n=(x_n,y_n)$ in the plane, output a pair $i,j$ such that the distance between $p_i$ and $p_j$ is minimum.

	The algorithm is described in \Cref{algo: closest pair of points}.

	\begin{algorythm}
		\DontPrintSemicolon
		\KwIn{$n\geq 2$ points $p_1=(x_1,y_1),\ldots,p_n=(x_n,y_n)$}
		\KwOut{$i,j\in[n]$ such that the distance between $p_i$ and $p_j$ is minimum}
		$\var{i_1},\var{i_2}\gets 0,1$\;
		$\var{min}\gets (x_1-x_2)^2+(y_1-y_2)^2$\;
		\For{$i=1$ \KwTo $n$} {
			\For{$j=i+1$ \KwTo $n$} {
				$\var{d}\gets (x_i-x_j)^2+(y_i-y_j)^2$\;
				\If{$\var{d}<\var{min}$} {
					$\var{min}\gets\var{d}$\;
					$i_1,j_1\gets i,j$\;
				}
			}
		}
		\Return{$\var{i_1},\var{i_2}$}
		\caption{Algorithm to find a closest pair of points}\label{algo: closest pair of points}
	\end{algorythm}
	This problem can in fact be solved in $\mathcal{O}(n\log n)$ time, which we shall see later (you can try thinking about it now).

	\item $\mathcal{O}(n^k)$. Given a graph $G=(V,E)$, find a $S\subseteq V$ such that $|S|=k$ and there is no edge between any two nodes in $S$.\\
	The above is known as the ``independent set problem''. The counterpart with an edge between any two nodes is known as the ``clique problem''.\\
	This is easily done by just checking every subset of size $k$, of which there are $\binom{n}{k}\mathcal{O}(k^2)=\mathcal{O}(n^k k^2)$ (there are $\mathcal{O}(k^2)$ comparisons for each subset). Since $k$ is constant, this is just $\mathcal{O}(n^k)$.

	So for constant $k$, this \textit{is} technically polynomial, but for reasonably large $k$, this is terrible. In fact, if we want to find the largest clique, then a polynomial time algorithm (in $n$) is not known. Indeed, this is an ``NP-hard'' problem, which we shall read more about later.

\end{enumerate}


\subsection{Greedy Algorithms}

Greed is good, and sometimes, it's even optimal.\\

What is a greedy algorithm? It essentially builds a solution in tiny steps, performing some sort of \textit{local} optimization, which ends up optimizing the problem requirement \textit{globally}.\\
It's quite clear that greedy algorithms needn't always work, we have to pick a local criterion that fits our requirements.\\

\subsubsection{Interval Scheduling}

Consider the ``interval-scheduling problem''. We have a supercomputer on which jobs need to be scheduled. We are given $n$ jobs specified by their start and finish times. That is, for $i\in[n]$, we are given $J_i = (s(i),f(i))$. We wish to schedule as many jobs as possible on the computer. That is, given $J = \{J_i = (s(i),f(i)) : i\in[n]\}$, find the largest subset $S\subseteq J$ such that no two intervals in $S$ overlap.\\
We want maximality in terms of \textit{cardinality} of $S$ (the number of jobs scheduled), not the total time covered.

Consider the following greedy algorithms.

\begin{enumerate}
	\item Choose a job with the earliest starting time. This basically says that we never want to leave the computer idle. It is easily seen that this need not work, since the job that starts soonest can take a very long time, obviously resulting in non-optimality.\\
	More concretely, let $J = \{(0,3),(1,2),(2,3)\}$.

	\item Choose the smallest available job. This needn't work either, since the smallest job could overshadow multiple longer jobs that intersect it.\\
	For example, let $J=\{(1,5),(4,6),(6,10)\}$.

	\item Choose the job with the earliest ending time. This \textit{does} work. The idea behind this is that it tries to keep as many resources free as possible. One can try playing around with a few examples to see that it does work.\\
	But how would we prove that it works?
\end{enumerate}

Let $\mathcal{A}$ be the set selected by the (last) greedy algorithm above and $\mathsf{OPT}$ denote an optimal solution. We wish to show that $|\mathcal{A}|=|\mathsf{OPT}|$.\\
Let $\mathcal{A}=\{a_1,\ldots,a_k\}$ and $\mathsf{OPT}=\{b_1,\ldots,b_m\}$.

\begin{lemma}
For $r\in[k]$, $f(a_r) \leq f(b_r)$.
\end{lemma}
\begin{proof}
This is easily shown via induction. For $r=1$, it holds by the definition of $\mathcal{A}$. For $r>1$, we have (by induction) $f(a_{r-1}) \leq f(b_{r-1}) \leq s(b_r)$. Then since $b_r$ itself can be chosen by the algorithm, we must have that $f(a_r) \leq f(b_r)$.
\end{proof}

This also implies that $\mathcal{A}$ is optimal since at each step, the job corresponding to $\mathsf{OPT}$ can be picked by $\mathcal{A}$. Try showing why this implies $|\mathcal{A}|=|\mathsf{OPT}|$.\\
What is the running time of this algorithm? We first sort the jobs according to their finish times, which takes $\mathcal{O}(n\log n)$ time. We then have to scan through all the jobs in this sorted list, which takes $\mathcal{O}(n)$ times. The total running time is $\mathcal{O}(n\log n)$.

\subsubsection{Minimal Spanning Subgraph}

Now, we consider the ``Minimal Spanning Subgraph'' problem. Given an undirected connected graph $G=(V,E)$ and a cost function $c:E\to\Zp$, find a subset $T\subseteq E$ such that $T$ spans all the vertices, $T$ is connected, and it is the set with the least such cost.

It is easy to show that this $T$ must be a tree. Suppose it is connected, spanning and has a cycle $C$. If we delete the costliest edge $e$ on $C$, then on removing $e$ from $T$, $T$ remains connected and spanning, but the weight goes strictly down (since $c$ maps into $\Zp$).

The brute force approach is to just iterate over all distinct spanning trees, but this is obviously quite terrible (exponential).

As it turns out, nearly no matter what greedy strategy we choose, the optimal solution is attained. We give four such algorithms.
\begin{enumerate}[(i)]
	\item Choose an edge $\alpha$ such that $c(\alpha)$ is minimal. Each subsequent edge is chosen from the cheapest remaining edges of $G$ ensuring that we never form any cycles.
	
	\item At each step, delete a costliest edge that does not destroy the connectedness of the graph.
	
	\item Pick a vertex $x_1$ of $G$. Having found vertices $x_1,\ldots,x_k$ an an edge $x_ix_j$, $i<j$, for each vertex $j$ with $j\leq k$, select a cheapest edge of the form $x_ix$, say $x_i x_{k+1}$, where $1\leq i\leq k$ and $x_{k+1}\not\in\{x_1,\ldots,x_k\}$. The process terminates after we have selected $n-1$ edges.
	
	\item This only works if all the edge costs are distinct. First, for each vertex, select the cheapest edge. After this, repeatedly select a cheapest edge between two distinct connected components until the graph becomes connected.
\end{enumerate}

The first algorithm is also known as Kruskal's algorithm. More concretely, what it does is 

\begin{algorithm}
	\DontPrintSemicolon
	\SetNoFillComment
	\KwIn{A connected graph $G=(V,E)$ with $|V|=n$, $|E|=m$, and a cost function $c:E\to\Zp$}
	\KwOut{A minimal spanning subgraph of $G$}
	\SetKwFunction{Sort}{sort}
	\Sort{$E$,$c$} \tcp*{sort the elements of $E$ in non-decreasing cost}
	$T\gets\emptyset$\;
	\For{$1\leq i\leq m$} {
		\If{$T\cup\{e_i\}$ doesn't have a cycle} {
			$T\gets T\cup\{e_i\}$
		}
	}
	\Return{$T$}
	\caption{Kruskal's Algorithm}\label{algo: kruskel's algorithm}
\end{algorithm}

The set thus formed clearly has no cycles by construction. Spanningness follows due to its maximality. Proving that it is a minimal spanning tree is quite easy in the case where all costs are distinct using the following property of any minimal spanning tree.

\begin{lemma}[Cut Property]
	Let $\emptyset\neq S\subsetneq V$. Let $e=vw$ be the minimal cost edge such that $v\in S$ and $w\in V\setminus S$. Then every minimal spanning tree of the graph must contain $e$.
\end{lemma}

To show that the required follows if we have the cut property, let $T$ be a minimal spanning tree and $T'$ the set output by the algorithm at some intermediate step.\\
Let $e=vw$ be the first edge added to $T'$ in the subsequent steps and $S$ be the neighbourhood of $v$ in $T'$. Since the algorithm was able to add $e$ to $T'$, there is no edge in $T'$ connecting any node in $S$ to any node in $V\setminus S$ (we do not create cycles). We already know that $e$ must be the lowest cost such edge. Then by the cut property, $e$ must be present in $T$!\\
The only issue arises when $v$ is not connected to any vertex in $T'$, but this case is easily resolved.\\
Therefore, $T$ must be minimum spanning.\\

Let us next show that $T$ is connected. Suppose otherwise. There must then be a non-empty $S$ such that no edge from $S$ to $V\setminus S$ is in $T$. However, as $G$ is connected, there is a minimum cost edge that connects $S$ and $V\setminus S$ and by the cut-property, this edge must be in $T$.

\begin{proof}[Proof of the cut-property]
	Suppose we have set $S$ and edge $e$ as in the cut-property. let $T$ be a spanning tree that does not contain $e$. Then adding $e$ to $T$ creates a cycle. Let $P$ be a path in $T$ that connects $v$ to $w$. Let $v'$ be the last vertex along this path in $S$ and $w'$ the first in $V\setminus S$. Let $e'=v'w'$.\\
	Defining $T' = T\setminus\{e'\}\cup\{e\}$, we see that $T'$ has lower cost than $T$, thus completing the proof. 
\end{proof}

Observe that the second algorithm given above is essentially Kruskal's algorithm in reverse.\\

Also note that the third algorithm given above aises quite naturally from the cut property -- it is also known as \textit{Prim's algorithm}.

\begin{algorithm}
	\DontPrintSemicolon
	\SetNoFillComment
	\KwIn{A connected graph $G=(V,E)$ with $|V|=n$, $|E|=m$, and a cost function $c:E\to\Zp$}
	\KwOut{A minimal spanning subgraph of $G$}
	$T\gets\emptyset, S\gets\{v\}$ \tcp*{$v$ is an arbitrary node}
	\While{$|S|<n$}{
		Compute $E_s = \{vw\in E:v\in S,w\not\in S\}$\;
		$\tilde{e}\gets\argmin_{e\in E_s}c(e)$\;
		$S\gets S\cup\{w\}$ \tcp*{$w$ is the vertex of $\tilde{e}$ that is in $V\setminus S$}
		$T\gets T\cup\{\tilde{e}\}$\;
	}
	\Return{$T$}
	\caption{Prim's Algorithm}\label{algo: prim's algorithm}
\end{algorithm}

Proving optimality of the set output by the above is easily shown using the cut-property.\\

As a slight detour from what we have done thus far, how would one \textit{implement} Prim's algorithm? We are given an undirected graph $G=(V,E)$ with edge costs given in an adjacency list and a source vertex $s\in V$.\\
We use a method similar to Djikstra's algorithm. Add an arbitary start vertex to the queue with key value $0$ and let all other key values to be $\infty$. At each step, we use a priority queue to extract node with the minimum key value from the queue. Explore the neighbourhood of the node, updating the key value. Here, the key value is the cost of the smallest cost edge leading to a node. The only change here is the update step, everything else is as in Djikstra's.\\
The time complexity of this is the same as Djikstra's, namely $\mathcal{O}((m+n)\log n)$.

\subsubsection{Huffman Coding}

Given a file with data from an alphabet, convert it to a binary alphabet using as few bits as possible while keeping it uniquely decodable. For example, if we had $\Sigma=\{a,b,c,d,e\}$, using $3$ bits would definitely get the job done since $\lceil \log_2(5)\rceil = 3$.\\
Suppose $a,b,c,d,e$ occur with frequencies $100,3,5,45,20$ respectively. Then using $3$ bits for each, it would result in $519$ bits.\\
What if instead, we use fewer bits for letters that occur frequently? For example, what if we encode it as
\[ \{a\mapsto 0),b\mapsto 100,c\mapsto 010,d\mapsto 1,e\mapsto 01\}? \] This would only use $209$ bits, which is obviously a huge improvement. However, this is \textit{not} uniquely decodable (consider $0101$).\\

\paragraph{Prefix-free encoding.} It turns out that we must encode the letters such that each has a unique prefix (to ensure unique decoding). Consider
\[ \{a\mapsto 1, b\mapsto 0000, c\mapsto 0001, d\mapsto 01, e\mapsto 001\}. \]
This enables unique decoding and uses $282$ bits, which is obviously far better than $519$. It uses fewer bits for low frequency letters, but a quite high number of bits for low frequency letters. Is it optimal? It is important to note that we desire optimality for this specific set of frequencies.

\paragraph{Greedy Strategy I.} Sort the frequencies in non-increasing order ($f_1\geq\cdots\geq f_n$). Use an $i$-length prefix-free string for $f_i$. It is quite easy to see that this fails spectacularly if all the frequencies are equal -- in this case, the na\"ive  encoding where each character takes $\log n$ bits is optimal.

\paragraph{Greedy Strategy II.} This is a top-down strategy. Divide the letters into two sets of almost equal frequencies. Recursively code the two sets, giving a shorter prefix to the more frequent of the two sets. It turns out that this isn't optimal either.\\ % *** GIVE A COUNTEREXAMPLE. ***
This algorithm essentially creates a tree such that the nodes are subsets of the alphabet, leaves are single letters, and the depth of a leaf is the length of the code word associated with it. Specifying the tree specifies the code generated by this strategy (Why?).\\
The issue with this strategy is that we don't completely specify what ``almost equal frequencies'' mean, it is underspecified.

\paragraph{Greedy Strategy III.} Since the top-down strategy doesn't work, perhaps we can try a bottom-up strategy. To construct the tree mentioned in the previous paragraph, start with the two least frequently occurring elements, say $f,f'$. Assign a string (suffix) $0$ to one and $1$ to the other. Combine the two frequencies to create a new letter with frequency $f+f'$. Repeat until all letters are assigned strings.\\
This strategy is optimal.

\begin{algorithm*}
	\DontPrintSemicolon
	\SetNoFillComment
	\KwIn{An alphabet $\Sigma=\{a_1,\ldots,a_n\}$ and a frequency $f_i$ for each $a_i$.}
	\KwOut{A binary encoding of the letters for optimal compression}
	\For{$1\leq i\leq n$}{
		Create a leaf node for $a_i$\;
		Insert it into the min-heap $H$ with $f_i$ as the key\;
	}
	\While{$H$ has $>1$ elements} {
		Extract the two nodes with lowest key value from $H$, say $u,v$ with keys $f_u,f_v$\;
		Create a new internal node $w$ and $f_w\gets f_u+f_v$\;
		Let $u$ be $w$'s left child and $v$ its right child\;
		Insert $w$ into $H$ with key $f_w$.
	}
	\Return{$H$}
	\caption{Huffman Coding}\label{algo: huffman coding}
\end{algorithm*}

How do we prove correctness? For a tree $T$ corresponding to a prefix-free encoding of $\Sigma$, we can associate it to a cost 
\[ c(T) = \sum_{1\leq i\leq n} f_i d_T(a_i), \]
where $d_T(a_i)$ is the depth of $a_i$ in $T$.\\
Observe that any optimal tree must be a full binary tree (any internal node has $2$ children) -- if not, we can shift leaves lower down to a higher position and decrease the cost.\\

\begin{lemma}
	Consider the two letters $x$ and $y$ with the smallest frequencies. There is an optimal code tree $T^*$ in which these tw letters are sibling leaves at the lowest level.
\end{lemma}

\begin{proof}
	Let $a$ and $b$ be two letters at the lowest level of an optimal tree $T$ that are siblings of each other. Let their frequencies be $f_a$ and $f_b$. Assume $f_x\leq f_y$ and $f_a\leq f_b$.\\
	Let $T'$ be a tree formed from $T$ by exchanging $a$ and $x$. We know that $d_T(a)\geq d_T(x)$ and $d_T(b)\geq d_T(y)$. By definition, we have $c(T)\leq c(T')$. This implies that
	\[ c(T) \leq c(T) - \left(d_T(x)f_x + d_T(a)f_a - d_{T'}(x)f_x - d_{T'}(a)f_a\right) = c(T) - \left(d_T(x)f_x (f_x-f_a) + d_T(a)(f_a-f_x)\right), \]
	thus proving the claim.
\end{proof}

We shall next show that if a step in the algorithm is correct, the next step is also correct (the above lemma says that the first step is correct). That is,

\begin{lemma}
	Let $T$ be a tree corresponding to an optimal encoding of $\Sigma$. Let $x,y$ be sibling leaves of $T$ and $z$ their parent in $T$. Let $f_z=f_x+f_y$, $T'=T\setminus\{x,y\}$, and $\Sigma'=\Sigma\setminus\{x,y\}\cup\{z\}$. Then $T'$ corresponds to an optimal encoding of $\Sigma'$.
\end{lemma}
\begin{proof}
	We have
	\[ c(T') = c(T) - d_T(x)(f_x+f_y) + (d_T(x)-1)f_z = c(T) - f_z. \]
	Non-optimality of $T'$ is seen to result in non-optimality of $T$ (if it is not optimal, then substituting $z$ back with $x$ and $y$ in an optimal solution results in an encoding of $\Sigma$ with cost strictly less than that of $T$), thus proving the claim.
\end{proof}

Why does correctness follow? At each step, we are restricting ourselves to a smaller set while maintaining the presence of an optimal solution with the current (partial) mapping. At the end, the current mapping is a complete mapping, thus implying optimality.

\subsection{Divide and Conquer Algorithms}

% \subsubsection{The Idea}

The basic paradigm is as follows:
\begin{itemize}
	\item A task needs to be solved on an instance of size $n$.
	\item Divide it into, say, $k$ parts of size $n/k$ each.
	\item Invoke recursion to solve each of these sub-problems.
	\item Combine the smaller answers to get the larger answer.
\end{itemize}

We \textit{divide}, \textit{delegate}, and \textit{combine}. Once we have the answers, we get the time complexity as
\[ T(n) = k T(n+k) + \text{time to combine}. \]

This can be unrolled using the Master Theorem:

\begin{theorem}[Master Theorem]
	Let $T(n)=a T(n/b) + \theta(n^c)$, where $a,b,c\in\N$, $a\geq 1$, $b>1$, and $c\geq 0$. Then
	\begin{itemize}
		\item $T(n)=\Theta(n^c)$ if $a<b^c$,
		\item $T(n)=\Theta(n^c\log n)$ if $a=b^c$, and
		\item $T(n)=\Theta(n^{\log_b a})$ if $a>b^c$.
	\end{itemize}
	More generally, if $T(n)=a T(n/b) + f(n)$, where $a,b\in\N$, $a\geq 1$, and $b>1$, then
	\begin{itemize}
		\item $T(n)=\Theta(f(n))$ if $f(n) = \Omega(n^{\log_b a + \varepsilon})$ for all $\varepsilon>0$,
		\item $T(n)=\Theta(n^{\log_b a}\log n)$ if $f(n) = \Theta(n^{\log_b a})$, and
		\item $T(n)=\Theta(n^{\log_b a})$ if $f(n) = \mathcal{O}(n^{\log_b a - \varepsilon})$ for all $\varepsilon>0$.
	\end{itemize}
\end{theorem}

A popular example of the divide and conquer paradigm is merge-sort. To sort an array, we divide it into two halves, sort the two halves separately, and merge the two (sorted) halves in sorted order. The overall time complexity is $\mathcal{O}(n\log n)$. \\

While in greedy algorithms the proof mainly comprised of showing that they return the correct answer, here the main issue is in showing that the algorithm actually runs -- that the recombination is justified at each step.

\subsubsection{Integer Multiplication}

The input is two $n$-digit (non-negative) numbers $x$ and $y$ in decimal notation and we need to compute $x\times y$.\\

The basic algorithm studied in grade school is quite simple. What is the time complexity of this algorithm? It is \textit{quadratic} in the number of digits ($\mathcal{O}(n^2)$). Here, the assumption is that adding two single digit numbers, multiplying two single digit numbers, or inserting a zero at the end of a number each take $\mathcal{O}(1)$ time. We stick with this assumption for the remainder of this section.\\

Is it possible to do better? The main algorithm we shall see in this section is known as \textit{Karatsuba's Algorithm}.\\
Motivated by the divide-and-conquer paradigm, perhaps we could split each of the $n$-digit numbers into two numbers, each of $(n/2)$ digits. So for example, we split $1234$ as $12$ and $34$.\\
In general, suppose we split $x$ and $y$ as $a,b$ and $c,d$ respectively. We compute $X=a\times c$ and $Y=b\times d$. We then compute $Z=(a+b)\cdot(c+d)$ and $W=Z-X-Y$. Finally, return $10^n\cdot X + 10^{n/2}\cdot W + Y$. Indeed,

\begin{align*}
	x\times y &= (10^{n/2}a+b)\times(10^{n/2}c+d) \\
		&= 10^n (a\times c) + 10^{n/2} (a\times d+b\times c) + bd \\
		&= 10^n\cdot X + 10^{n/2}\cdot W + Y,
\end{align*}
so the algorithm is correct (we must use an inductive argument to conclude since calculating each of $X, Z, Y$ use the same algorithm).

Is this an improvement over the na\"ive grade school algorithm? To perform the task for size $n$, we perform the task of size $(n/2)$ $3$ times (for $X, Y, Z$) so
\[ T(n) = 3 T(n/2) + \mathcal{O}(n). \]
We have assumed that addition/subtraction take $\mathcal{O}(n)$.\\
Using the master theorem, it is easy to conclude that $T(n)=\mathcal{O}(n^{\log 3}) = \mathcal{O}(n^{1.584})$.\\ Note that if we instead calculate $a\times d$ and $b\times c$ separately (instead of calculating $Z$), there are $4$ tasks of size $n/2$ at each step which results in an overall time of $\mathcal{O}(n^2)$.\\

Later, Andrei Toom broke the numbers into $3$ parts and showed that $5$ multiplications are enough. This gives $T(n)=5T(n/3)+\mathcal{O}(n)$, resulting in a time complexity of $\mathcal{O}(n^{\log_3 5}) = \mathcal{O}(n^{1.465})$.\\

Stephen Cook attempted to generalize this idea even further. Breaking it into $r$ parts reduces it to $\mathcal{O}(C(r)n^{\log_r(2r-1)})$, where $C(r)$ is some constant depending on $r$.\\
Later, Sch\"onhage and Strassen managed to breach the $n^{1+\varepsilon}$ barrier and got it down to $\mathcal{O}(n \log n \log\log n)$. They also conjectured that $\mathcal{O}(n\log n)$ is the best possible running time.\\
Martin F\"urer got it down to $\mathcal{O}(n2^{\log^* n}\log n )$ in 2007.\\
In 2019, Harvey and Hoeven managed to get the ``perfect'' method which takes $\mathcal{O}(n\log n)$. Soon after, a team at Aarhus University managed to prove that given the (unproven) Valiant's conjecture, this is indeed the best we can do.

\subsubsection{Closest points in a plane}

Suppose we have $n$ points $p_1=(x_1,y_1),\ldots,p_n=(x_n,y_n)$. We want to find $i,j$ such that the distance between $p_i$ and $p_j$ is minimum.\\

There is obviously a na\"ive $\mathcal{O}(n^2)$ algorithm which does pairwise comparisons. Can we do better?\\

In the one-dimensional case, we can sort the points in $\mathcal{O}(n\log n)$ and then iterate through the points in $\mathcal{O}(n)$ to find the closest pair, which takes an overall time of $\mathcal{O}(n\log n)$.\\
How would we extend this to two dimensions? There is no ordering for the points in $\R^2$, so a similar idea will not work.\\

Divide the points into two halves. Recursively find the points in two halves. Recursively find the closest pair in each half. Finally, combine. The problematic part here is the recombination, which should take $\mathcal{O}(n)$.\\
If we just compare every pair across halves, it still takes $\Omega(n^2)$.\\

Suppose we split it into two halves based on their $x$-coordinates. Recursively compute the closest pair in each of the two halves. Let the minimum of these two distances is $d$. If the first division does not separate the closest pair, then $d$ is the answer.

How many ``cross-pairs'' need to be checked though? Observe that we don't need to check anything for the points that are farther than $d$ from the central separating line (Why?). But in the worst case, we might still need to check several points.

\begin{lemma}
	Let $S_y=(q_1,\ldots,q_m)$ be the points in the distance $d$ region sorted in decreasing order of their $y$ coordinates. If the distance between $q_i$ and $q_j$ is less than $d$, then $j-i\leq 11$.
\end{lemma}
\begin{proof}
	Consider the distance $d$ region split into (closed) squares of side length $(d/2)$. In each row, there are $4$ squares. Note that in each box, there is at most $1$ point (if there was more than one, the minimum distance in one of the halves would be less than $d$). Suppose that there are more than $11$ indices between $i$ and $j$. Then at least $2$ full rows separate the boxes containing $q_i$ and $q_j$. However, this would imply that the distance between $q_i$ and $q_j$ is $\geq d$, thus yielding a contradiction and proving the claim.
\end{proof}

It is worth noting that the constant can be better than $11$, but this is just a constant factor change and doesn't change the actual running time of the algorithm.

This then gives an $\mathcal{O}(n\log n)$ algorithm. Computing the first half of $P_x$ (or $P_y$) takes $\mathcal{O}(n)$, and computing the answers by comparing the distances with the middle band takes $\mathcal{O}(n)$. Therefore, 
\[ t(n) \leq 2t(n/2) + \mathcal{O}(n)\text{ and the running time is }T(n)=t(n)+\mathcal{O}(n\log n), \]
where the extra $\mathcal{O}(n\log n)$ factors comes from having to sort the points based on $x$ and $y$ coordinate ahead of time.

\subsubsection{Univariate polynomial multiplication}

The input is two univariate polynomials $A,B$ of degree $n-1$, which are described by coefficient vectors $(a_i)$ and $(b_i)$ of length $n$. The desired output is $C(x)=A(x)B(x)$ as a coefficient vector $(c_i)$ of length $2n-1$.\\
This problem is also known as the ``convolution problem''.

It is seen that
\[ c_k = \sum_{\substack{i,j<n \\ i+j=k}} a_i b_j, \]
so a na\"ive $\mathcal{O}(n^2)$ algorithm is easy to come up with. Note that here, we sweep the time taken to actually multiply integers under the rug, assuming that they can be done in constant time.

This problem feels quite similar in spirit to the integer polynomial multiplication algorithm that we have already seen. While something faintly similar to Karatsuba's would work, we need to make several significant changes to modify it into this (slightly more general) context.\\
Inspired by what we got in Karatsuba's, can we get an $\mathcal{O}(n\log n)$ algorithm?\\

Rather than multiplying $A$ and $B$ symbolically, consider evaluation of these polynomials. First, choose $2n$ values $\alpha_1,\ldots,\alpha_{2n}$, evaluate $A$ and $B$ at these points and then compute $C(\alpha_j)=A(\alpha_j)B(\alpha_j)$ for each $j$. Next, recover $C(x)$ from the evaluations by interpolating.\\
Obviously, we have skimmed over most of the details here.

Since we have assumed that integer multiplication takes constant time, computing all the $C(\alpha_j)$ given $A(\alpha_j)$ and $B(\alpha_j)$ takes $\mathcal{O}(n\log n)$ time.\\
However, evaluating a polynomial at a single value takes $\Omega(n)$ time and we need to do so $\mathcal{O}(n\log n)$ time! We can't just separately evaluate each of the $A(\alpha_j)$ and $B(\alpha_j)$ hoping for it to work.

\subsection{Exercises}


\begin{exercise}
Let $\var{A}$ be an array of $n$ distinct numbers. A number at location $1<i<n$ is said to be a maxima in the array if $\var{A}[i-1]<\var{A}[i]$ and $\var{A}[i]>\var{A}[i+1]$. Also, $\var{A}[1]$ is a maxima if $\var{A}[2]<\var{A}[1]$ and $\var{A}[n]$ is a maxima if $\var{A}[n]>\var{A}[n-1]$. Find a maxima in the array in time $\mathcal{O}(\log n)$.
\end{exercise}
\begin{solution*}
	The basic idea behind this algorithm is to, at each step, greedily check the half of the array that contains the greater element among the two neighbours of the current element. It is described more precisely in \Cref{algo: find maxima}. We encourage the reader to prove the correctness of this algorithm.
\end{solution*}
\begin{algorithm}
	\DontPrintSemicolon
	\KwIn{An array $\var{A}$ containing $n$ elements ($1$-indexed)}
	\KwOut{A maxima in $\var{A}$}
	\SetKwProg{Fn}{}{}{}
	\SetKwFunction{FRecurs}{greedyStep}
	\SetKwFunction{Size}{size}
	\Fn{\FRecurs{$\var{B}$}}{
		\If{$\var{size(B)}=1$} {
			\Return{$\var{B}[0]$}
		}
		$\var{mid}\gets\Size{$\var{B}$}/2$\;
		\If{$\var{B}[\var{mid}]>\var{B}[\var{mid}+1]$ and $\var{B}[\var{mid}]>\var{B}[\var{mid}-1]$} {
			\Return{$\var{B}[\var{mid}]$}\;
		}
		\ElseIf{$\var{B}[\var{mid}]\leq\var{B}[\var{mid}-1]$} {
			\Return{$\FRecurs(\var{B}[1:\var{mid}/2])$}
		}
		\ElseIf{$\var{B}[\var{mid}]\leq\var{B}[\var{mid}+1]$} {
			\Return{$\FRecurs(\var{B}[1+\var{mid}/2:\Size{$\var{B}$}])$}
		}
	}
	\Return{\FRecurs{$\var{A}$}}
	\caption{Solution 1.1}\label{algo: find maxima}
\end{algorithm}

\begin{exercise}
	Let $G=(V,E)$ be an undirected graph. Consider the following greedy algorithm:
	\begin{algorithm*}
		\DontPrintSemicolon
		\SetNoFillComment
		\KwIn{A connected graph $G=(V,E)$ with $|V|=n$, $|E|=m$, and a cost function $c:E\to\Zp$}
		% \KwOut{A minimal spanning subgraph of $G$}
		$M\gets\emptyset$. Let $V(M)$ be the set of vertices in $M$.\;
		\ForEach{$uv\in E$}{
			\If{$V(M)\cap \{u,v\}=\emptyset$}{
				$M\gets M\cup\{uv\}$\;
			}
		}
		\Return{$M$}
		\caption{Exercise 1.2}\label{algo: ex 1.2}
	\end{algorithm*}

	\begin{enumerate}[(a)]
		\item Give a graph $G$ such that the above algorithm may not output a perfect matching in $G$ even if it has a perfect matching.
		\item Give a graph that has a perfect matching $M$ and a maximal matching $M'$ such that $M'\neq M$.
		\item Prove that the above algorithm finds a maximal matching.
	\end{enumerate}
\end{exercise}
\begin{solution*}
	For parts (a) and (b), consider the graph $G=(V,E)$ with $V=\{1,2,3,4\}$ and $E=\{\{1,2\},\{2,3\},\{3,4\},\{4,1\},\{1,3\}\}$. Then $\{\{1,3\}\}$ is a maximal matching but not a perfect matching and $\{\{1,2\},\{3,4\}\}$ is a maximal matching. The former may be output by the algorithm depending on which edge is chosen.\\
	Part (c) is easily shown. If the output is not a maximal matching, then there is some edge $e=vw$ such that $E\cap\{v,w\}=\emptyset$ (by definition). However, this contradicts the termination of the algorithm, thus proving the required.
\end{solution*}

\begin{exercise}
	Consider the following modified interval scheduling problem. There are $n$ jobs, each with a start and finish time $(s(i),f(i))$ and in addition, they have a (positive) weight $w(i)$. The problem is to maximize the total weight of the jobs scheduled on a (single) machine. For the sake of brevity, we denote the set of jobs $J$ by a set of tuples with the $i$th tuple being $(s(i),f(i),w(i))$.
	\begin{enumerate}[(a)]
		\item Show that the algorithm for the usual interval scheduling need not give an optimal solution for this problem.
		
		\item Suppose a greedy algorithm picks the largest weight job with the earliest finishing time among the available jobs at each step. Prove/disprove that this strategy works.
		
		\item We say that jobs $J$ and $J'$ \textit{overlap}, denoted $J\| J'$, if $J\cap J'\neq\emptyset$. For a job $J_i$, define $O_i = \sum_{i':J_{i'}\cap J_i} w(i')$, called the \textit{overlap-weight} of $J_i$. Consider a greedy algorithm that schedules a job with the smallest overlap-weight among the available jobs at each step. Prove/disprove that this algorithm gives the optimal solution.
	\end{enumerate}
\end{exercise}

\begin{solution*}
	\begin{enumerate}[(a)]
		\item Consider the set of jobs $J=\{(1,2,5),(1,3,10)\}$. Then the algorithm chooses only the first job, while the optimal solution picks the second.
		
		\item Consider the set of jobs $J=\{(1,2,5),(2,3,5),(1,3,8)\}$. Then this algorithm chooses only the third job, whereas the optimal solution picks the first two.
		
		\item The algorithm is incorrect. Consider the counterexample
		\[ J = \{(0,1,1),(1,2,1),(0,2,3)\}. \]
		Then the algorithm chooses jobs $1$ and $2$, whereas the (unique) optimal solution picks job $3$.\\
		The reader might be tempted to think that the algorithm would work if in the definition of $O_i$, we only consider those $i'\neq i$. However, this doesn't work either. Indeed, consider
		\[ J = \{(1,3,5),(1,2,1),(2,4,5),(3,5,5),(4,5,1)\}. \]
		The algorithm chooses jobs $2$, $3$, and $5$, whereas the (unique) optimal solution chooses jobs $1$ and $4$.\\
		This idea is made more natural on rewriting the problem as finding $\mathcal{A}\subseteq[n]$ that minimizes $w\left(\bigcup_{i\in\mathcal{A}} O(i)\right)$,	where $O(i) = \{i'\in[n] : i'\neq i\text{ and }J_{i'}\|J_i\}$ and $w(S)=\sum_{s\in S}w(s)$ for any $S\subseteq[n]$. The issue arises because a single $j$ might appear in multiple $O(i)$. It is worth noting that there is a dynamic programming algorithm to solve this problem (which we shall study later).
	\end{enumerate}
\end{solution*}

\begin{exercise}
	You are given a set $S$ of $n$ pairs of numbers $(\ell_1,c_1),(\ell_2,c_2),\ldots,(\ell_n,c_n)$ and some $C$ (all positive). You are required to find a subset $T$ of $[n]$ such that $\sum_{i\in T} c_i \leq C$ while maximizing the sum $\sum_{i\in T}\ell_i$. Consider the algorithm to do the same that picks jobs with the largest values of $\ell_i/c_i$. This is described in \Cref{algo: ex 1.4}.
	\begin{enumerate}[(a)]
		\item Show that this algorithm need not find the optimal subset.
		\item Can you suggest any other greedy strategy to come up with the optimal subset in the above setting?
		\item Suppose that a ``fractional'' part of each of the $n$ pairs can be taken. Choosing a fraction $f_i$ of the item $c_i$ adds cost $c_i f_i$. Give an optimal algorithm in this case. That is, we want to maximize $\sum_{i\in [n]} f_i \ell_i$ constrained by $\sum_{i\in[n]} f_i c_i \leq C$.
	\end{enumerate}
	The above problem is more often known as the ``knapsack problem''.
\end{exercise}

\begin{algorithm}
	\DontPrintSemicolon
	\SetNoFillComment
	\KwIn{A set $S$ of $n$ tuples $(\ell_i,c_i)$ (each positive) and some $C>0$}
	\KwOut{$T\subseteq[n]$ that maximizes $\sum_{i\in S}\ell_i$ subject to the constraint $\sum_{i\in S}c_i\leq C$}
	\SetKwFunction{Sort}{merge-sort}
	\Sort{$S$, $\ell_i/c_i$} \tcp*{Arrange in non-increasing order of $(\ell_i/c_i)$}
	$S\gets\emptyset$, $\var{cost}\gets 0$\;
	\For{$1\leq i\leq n$}{
		\If{$\var{cost}+c_i\leq C$}{
			$S\gets S\cup\{i\}$\;
			$\var{cost}\gets\var{cost}+c_i$\;
		}
	}
	\Return{$S$}
	\caption{Exercise 1.4}\label{algo: ex 1.4}
\end{algorithm}

\begin{solution*}
	% \phantom{pog}
	\begin{enumerate}[(a)]
		\item Consider the set of tuples $\{(6,2),(9,3),(6,3),(8,4)\}$ with $C=7$. The algorithm returns the set $\{(6,2),(9,3)\}$ with $\sum_i \ell_i=15$ whereas the optimal solution is $\{(9,3),(8,4)\}$ with $\sum_i \ell_i = 17$.

		\item No, not yet.

		\item In the context of part (b), this just means that instead of choosing an $x_i\in\{0,1\}$ for each $i$, we choose $f_i\in[0,1]$ for each $i$. The algorithm is very similar in spirit to that given in the problem statement (of (a)). It is described explicitly in \Cref{algo: sol 1.4(c)}. Correctness is easily proved.

	\end{enumerate}
\end{solution*}


\begin{algorithm}[H]
	\DontPrintSemicolon
	\SetNoFillComment
	\KwIn{An set $S$ of $n$ tuples $(\ell_i,c_i)$ (each positive) and some $C>0$}
	\KwOut{An $f_i\in[0,1]$ for each $i$ that maximizes $\sum_{i\in S} f_i \ell_i$ subject to the constraint $\sum_{i\in S} f_i c_i\leq C$}
	\SetKwFunction{Sort}{merge-sort}
	\Sort{$S$, $\ell_i/c_i$}\;
	$S\gets\emptyset$, $\var{cost}\gets 0$\;
	$f_i\gets 0$ for each $i$\;
	\For{$1\leq i\leq n$}{
		\eIf {$\var{cost}+c_i\leq C$} {
			$f_i\gets 1$\;
			$\var{cost}\gets\var{cost}+c_i$\;
		} {
			$f_i \gets (C - \var{cost})/c_i$\;
			break\;
		}
	}
	\Return{$(f_i)$}
	\caption{Solution 1.4(c)}\label{algo: sol 1.4(c)}
\end{algorithm}

\begin{exercise}
	(Dis)prove that when all the edges in an undirected graph have distinct costs, the minimum spanning tree in the graph is unique.
\end{exercise}
\begin{solution*}
	Suppose instead there are two distinct MSTs $T, T'$. Let $e$ be the least costly edge that is in exactly one of the trees. Suppose it is in $T$. $T'\cup\{e\}$ must contain an edge $e'$ that is not in $T$ which is in the newly formed cycle. Then by the MST nature of $T'$ and the fact that all edges have distinct costs, $c(e)>c(e')$ (otherwise, $T'\cup\{e\}\setminus\{e'\}$ is a strictly cheaper tree). We similarly get $c(e')>c(e)$, thus resulting in a contradiction and proving the required.
\end{solution*}

\begin{exercise}
	Suppose we have a computer and some $n$ jobs. For the $i$th job, there are two parts with durations $t_{1}(i),t_{2}(i)>0$. The part of duration $t_{1}(i)$ must be performed on the computer (only one such part can be scheduled at a time) whereas the part of duration $t_{2}(i)$ can be performed at any point after the first part of the same job is done (multiple such parts can be scheduled simultaneously). Give an algorithm that designs an ordering for the jobs to be sent to the computer such that the overall time taken to complete all jobs is minimized.
\end{exercise}
\begin{solution*}
	The problem can be stated alternatively as: given $n$ and two functions $t_1,t_2:[n]\to\Rp$, find a permutation $\sigma$ of $[n]$ such that
	\[ Q_\sigma = \max_{k\in[n]} \left(t_2(\sigma(k)) + \sum_{1\leq i\leq k} t_1(\sigma(i))\right) \]
	is minimized. ($\sigma(i)$ denotes the position that is at the $i$th index after permuting)

	Assume without loss of generality that the jobs are ordered in non-increasing order of $t_2$. We claim that then, the identity permutation suffices. Let $Q$ be the value of $Q_\sigma$ for the identity permutation and $k\in[n]$ attain the maximum involved. Let $\sigma$ be any permutation of $[n]$.

	\textbf{Claim.} If $Q_\sigma\leq Q$, then for every $1\leq i\leq k$, $1\leq\sigma(i)\leq k$. This implies that the first $k$ positions permute among themselves in any optimal solution.\\
	Suppose otherwise and let $r=\max\{j\in[n] : \sigma(j)=i\text{ for some }1\leq i\leq k\}> k$. Then,
	\[ Q_\sigma \geq t_2(\sigma(r)) + \sum_{1\leq i\leq r} t_1(\sigma(i)) > t_2(k) + \sum_{1\leq i\leq k}t_1(i) = Q, \]
	thus proving the claim.

	Now, let $\sigma$ be a permutation such that $Q_\sigma \leq Q$. Then by the above claim and since the $t_2$ are in non-increasing order, $t_2(\sigma(k))\geq t_2(k)$. Then
	\[ Q_\sigma \geq t_2(\sigma(k)) + \sum_{1\leq i\leq k} t_1(\sigma(i)) = t_2(\sigma(k)) + \sum_{1\leq i\leq k} t_1(i) \geq Q, \]
	thus implying that the identity permutation is optimal. 
\end{solution*}


% \begin{exercise}
% 	Given a set of $n$ intervals, design a greedy algorithm to find the largest subset of intervals such that every interval in the subset overlaps with at most $k$ other intervals in the subset.
% \end{exercise}
% \begin{solution*}
% 	Observe that this question is the same as the usual interval scheduling problem except that we have $k+1$ machines instead of just one.\\
% 	First, arrange the jobs in increasing order of finishing time. Iterate through the jobs and at each step, choose the job if it is valid. Observe that in the case $k=0$, this is just the standard algorithm of choosing the job with the earliest finish time which we know solves the interval scheduling problem.\\
% 	To prove optimality, we shall show that if the set chosen by the algorithm (so far) when we are at index $i$ is $S_i$, then for any $i$, there is an optimal solution $\mathsf{OPT}_i$ such that $S_i\subseteq\mathsf{OPT}_i$. For $i=0$, we have $S_i=\emptyset$ so the required is trivial. Suppose it is true for $r-1\geq 0$. Then for $i=r$, if the algorithm does not choose the job, we can take $\mathsf{OPT}_r=\mathsf{OPT}_{r-1}$. Otherwise, we take two cases.
% 	\begin{itemize}
% 		\item $r\in\mathsf{OPT}_{r-1}$. We can again take $\mathsf{OPT}_r=\mathsf{OPT}_{r-1}$.
% 		\item $r\not\in\mathsf{OPT}_{r-1}$. This means there is some interval $j\in\mathsf{OPT}_{r-1}$ already intersecting $k$ jobs that intersects the $r$th job. Also, $j>r$ (because $r$ is chosen by the algorithm, there cannot be an invalid intersection for a lower index). Further, there are at most $k$ such values of $j$ (Why?). Fixing $j$ to be any of these jobs, $\mathsf{OPT}_{r-1}\setminus \{j\}\cup\{r\}$ is an optimal solution that contains $S_r$, thus proving the claim.
% 	\end{itemize}
% 	This completes the proof (the greedy nature of the algorithm together with the fact that $S_n\subseteq\mathsf{OPT}_n$ implies correctness).
% \end{solution*}

\begin{exercise}
	Given a set of $n$ intervals, design a greedy algorithm to find the smallest subset of intervals such that every interval is contained in the union of the intervals of the subset.
\end{exercise}
\begin{solution*}
	First, we order the intervals as $\{(a_1,b_1),(a_2,b_2),\ldots,(a_n,b_n)\}$ such that if $j>i$, either $a_j>a_i$ or $a_j=a_i$ and $b_j<b_i$ (not $>$). We denote $(a_i,b_i)$ by $A_i$.\\
	The algorithm performs a linear scan of the $A_i$. At the first step, choose $A_1$. At some step of the algorithm, let the last interval chosen be $A_k$. Among the intervals $A_i$ with $a_i<b_k$ and $b_i>b_k$, choose that interval with the largest $b_i$. In case there is no such interval, then just choose the first interval encountered with $a_i\geq a_k$.\\
	It is easy to see that any interval is contained in the union of these intervals.\\

	To show correctness, it suffices to consider the first case of the algorithm alone (Why? It splits into smaller disjoint problems). Let $S_i$ be the set of indices chosen by the algorithm so far when we are at index $i$. We shall show that for any $i$, there is an optimal solution $\mathsf{OPT}_i$ such that $S_i\subseteq\mathsf{OPT}_i$. For $i=0$, it is trivial since $\emptyset$ is a subset of any optimal solution $\mathsf{OPT}$. Suppose it is true for some $r-1\geq 0$. If the algorithm does not choose $r$, then $\mathsf{OPT}_r=\mathsf{OPT}_{r-1}$ will work. Now, let $r$ be chosen and $k$ be the most recently chosen index. It is easily shown that $\mathsf{OPT}_{r-1}$ must contain some $j$ such that $a_j<b_k<b_j$ (How?). We can then set $\mathsf{OPT}_r=\mathsf{OPT}_{r-1}\setminus \{j\}\cup\{k\}$, thus proving the claim.\\
	The required follows by the above claim (because the output of the algorithm is a valid solution).
\end{solution*}

% \begin{exercise}
% 	Given a set of $n$ intervals, design a greedy algorithm to find the smallest subset of intervals such that every interval not in the subset overlaps with at least one interval in the subset.
% \end{exercise}
% \begin{solution*}
% 	Consider the graph $G=(V,E)$ where $V$ is the set of intervals and there is an edge between any two intervals intersect. This problem merely asks for the smallest dominating set of $G$.\footnotemark{} Further, the interval scheduling problem gives the largest independent set of $G$. Recall from graph theory that a minimal dominating set of a graph is equal to a maximal independent set. Therefore, it suffices to just give the same output as the interval scheduling problem.
% \end{solution*}
% \footnotetext{A dominating set of a graph $G=(V,E)$ is a set $S\subseteq V$ such that the neighbourhood of $S$ is $V$.}