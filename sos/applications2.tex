%!TEX root = ./main.tex

\clearpage

\section{Higher degree sum-of-squares}

\subsection{Approximating conductance}

	In \Cref{subsec:max-cut}, we looked at the \textsf{NP}-hard problem of max-cut. Before moving to the rest of this section where we look at conductance, let us look at the far simpler min-cut problem. The reader might know that this problem can be solved in polynomial time using the Ford-Fulkerson max-flow algorithm. Before moving to the main problem of this subsection, let us overkill min-cut by giving a randomized algorithm via SoS.\\
	Min-cut and conductance will be our first example of degree $4$ sum-of-squares. The main reason for the degree $4$ requirement here is the following.

	\begin{ftheo}[Squared Triangle Inequality]
		For indeterminates $a,b,c \in \{-1,1\}$,
		\[ (a-c)^2 \le (a-b)^2 + (b-c)^2 \]
		has a degree $4$ sum-of-squares certificate.
	\end{ftheo}
	\begin{proof}
		We have
		\begin{align*}
			\frac{1}{2}\left((a-b)^2 + (b-c)^2 - (a-c)^2\right) &= b^2 - ab - bc + ac \\
				&= (1-bc)(1-ab) \\
				&= \frac{1}{4} (b^2+c^2-2bc)(a^2+b^2-2ab) = \left(\frac{(b-c)(a-b)}{2}\right)^2. \qedhere
		\end{align*}
	\end{proof}

	While we will not use it, we also state the following, which can be proved by considering a Gaussian with the same first two moments (like in \Cref{sec:quad-optim}) and using the fact that the $L^2$ metric (on $\R$) is a metric.

	\begin{theorem}
		For any degree $2$ pseudodistribution $\mu$,
		\[ \sqrt{\pE_\mu (x_i - x_k)^2} \le \sqrt{\pE_\mu (x_i - x_j)^2} + \sqrt{\pE_\mu (x_j - x_k)^2}. \]
	\end{theorem}

	For the remainder of this subsection, let $G$ be a graph with vertex set $[n]$, and let $\pE$ be any degree $4$ pseudodistribution. We continue to denote by $f_G$ the function in \cref{eqn: fg-def}. Our proof strategy for the min-cut problem will be to give a distribution $\mu'$ such that $\E_{\mu'} f_G \le \pE_{\mu} f_G$. For any $i,j \in [n]$, denote
	\[ D(i,j) = \pE_{\mu} \frac{1}{4} (x_i - x_j)^2 \]
	and for non-empty $A \subseteq [n]$, $D(A,i) = D(i,A) = \min_{j \in A} D(i,j)$. Because $\mu$ is a degree $2$ pseudodistribution (degree $4$ in fact), $D(i,A) \ge 0$ for any $i,A$ and it is bounded from above by $1$. Consider the ``line embedding'' of the vertices $[n]$ on the interval $[0,1]$ defined by $i \mapsto D(i,1)$. $\mu'$ is defined by uniformly randomly choosing $t \in [0,1]$, and outputting the cut $\{i \in [n] : D(i,1) \le t\}$. An edge $\{i,j\}$ is cut with probability
	\[ |D(j,1) - D(i,1)| = \frac{1}{4} \left|\pE_{\mu} (x_i - x_1)^2 - \pE_{\mu} (x_j - x_1)^2 \right| \le \frac{1}{4} \pE_\mu (x_i - x_j)^2 = D(i,j), \]
	where we have used the fact that $\mu$ is degree $4$ for the squared triangle inequality.
	Summing over all edges,
	\[ \E_{\mu'} f_G \le \sum_{ij \in E} D(i,j) = \pE_{\mu} f_G. \]

	Note that this proof works out more generally in the scenario where we have $D(i,A)$ for some set $A$ instead.
	To ensure non-triviality, we choose $t \in [0,\max_j D(1,j)]$ instead of $t \in [0,1]$.\\
	% While this proof may seem complete, it is not -- we require that the cut output by $\mu'$ is (almost surely) not $\emptyset$ or $[n]$. Since $D(j,A) = 0$ for any $j \in A$, the cut is never $\emptyset$. For the cut to be almost surely unequal to $[n]$, we require the existence of some $i \in [n]$ such that $D(i,A) \ne 0$. 

	Let us now get to a more non-trivial problem.

	\begin{fdef}
		Given a $d$-regular graph $G = (V,E)$ with $n$ vertices and a non-empty subset $S \subsetneq V$, the \emph{conductance} or \emph{normalized cut} of $S$ is
		\[ \Phi_G(S) = \frac{E(S,V \setminus S)}{(d/n) |S| |V \setminus S|}. \] 
	\end{fdef}
	The denominator can be thought of as the expected number of edges between $S$ and $S^c$ if the graph is a Erd\H{o}s-R\'{e}nyi random graph with edge probability $d/n$.

	\begin{fdef}
		Given a graph $G = (V,E)$ with $n$ vertices, the \emph{conductance} or \emph{expansion} of $G$ is
		\[ \Phi_G = \min_{\emptyset \ne S \subsetneq V} \Phi_G(S). \]
	\end{fdef}
	Conductance is very closely related to the rate of convergence of the standard random walk on the graph -- a low conductance implies that there is a ``tight bottleneck'' somewhere in the graph where the walk can get stuck. Indeed, if we remove the $d/n$ in the denominator, the conductance of a set is just the probability of exiting the set if we start a random walk at a uniformly random point in it.\\

	The problem of determining the conductance of a graph (and possibly a cut that attains it) is called the uniform sparsest cut problem. It is not too difficult to show that this is \textsf{NP}-hard using a reduction from max-cut. In fact, \cite{cond-appx-hard-ugc} show that the Unique Games Conjecture implies that any constant-factor approximation for $\Phi_G$ is $\mathsf{NP}$-hard! It is quite interesting how min-cut is in \textsf{P}, max-cut is \textsf{NP}-hard but we can get a good constant-factor approximation, but even that is not possible for sparsest-cut (assuming the UGC).

	It is quite easy to see that
	\[ \Phi_G = \min_{x \in \{-1,1\}^n} \frac{f_G(x)}{(d/4n) \sum_{i,j} (x_i - x_j)^2} = \min_{x \in \{-1,1\}^n} \frac{f_G(x)}{(d/n) f_{K_n}(x)}. \]
	Minimizing this directly seems impossible using the sum-of-squares method since we are looking at a rational function that is not a polynomial. So, we instead try to get a ``large'' $\alpha$ such that
	\[ f_G(x) - \alpha \frac{d}{n} f_{K_n}(x) \]
	has a sum-of-squares certificate.
	
	\begin{ftheo}[Cheeger's Inequality]
		There is a degree $2$ sum-of-squares certificate for
		\[ f_G(x) - \frac{\Phi_G^2}{2} \cdot \frac{d}{n} f_{K_n}(x). \]
	\end{ftheo}
	Furthermore, this value of $\alpha$ is tight up to constants for degree $2$ SoS -- with the example used being the cycle again.\\
	We omit the proof of this (standard) inequality; the reader may consult 
	% given a deg 2 pd \mu with \pE_\mu f_G - C \pE_\mu ... \ge 0, can find a set S with expansion \varphi_G(S) = O(\sqrt{C}).

	In [cite Leighton-Rao 88], Leighton-Rao gave an $O(\log n)$-approximation using linear programming. In the setting where $\Phi_G = O(1/\log n)$, this improves on Cheeger's inequality. We shall study the Arora-Rao-Vazirani (ARV) algorithm [cite ARV04], which gives an $O(\sqrt{\log n})$-approximation, based on degree $4$ sum-of-squares. \\
	In \Cref{sec:quad-optim}, our analysis was completely local in the sense that we analyzed what happens to each $(x_i - x_j)^2$ term completely independent of the others. It might not be too ambitious to hope that some sort of global analysis is possible, wherein we show that if one term is small, then others must be large, thus forcing the entire summation to be large. Of course, our remarks before \Cref{prop: gw-reparametr} make this unlikely, at least in the setting of max-cut.\\
	On the other hand, we shall use global analysis in the ARV algorithm. Even before ARV, it has helped in better approximations to vertex cover, graph coloring, max-cut gain etc.

	\begin{ftheo}[ARV]
		Let $G$ be a $d$-regular graph with $n$ vertices. There is a degree $4$ sum-of-squares certificate for
		\[ f_G(x) - \frac{\varphi_G}{\Theta(\sqrt{\log n})} \cdot \frac{d}{n} f_{K_n}(x). \]
		Furthermore, given any degree $4$ pseudodistribution $\mu$ on $\{-1,1\}^n$, it is possible to find in polynomial time a set $S \subseteq V$ such that
		\[ \varphi_G(S) \le O(\sqrt{\log n}) \cdot \frac{\pE_\mu f_G}{(d/n) \pE_\mu f_{K_n}}. \]
	\end{ftheo}
	Like in prior proofs, we shall show that given any degree $4$ pseudodistribution $\mu$, there exists a distribution $\mu'$ such that
	\[ \frac{\E_{\mu'} f_G}{\E_{\mu'} f_{K_n}} \le O(\sqrt{\log n}) \cdot \frac{\pE_\mu f_G}{\pE_\mu f_{K_n}}. \]
	% $D(i,j) = \frac{1}{4} \pE_\mu (x_i - x_j)^2$ is in some sense the ``pseudoprobability'' that $i,j$ are separated by the cut. Because $\mu$ is a degree $2$ pseudodistribution (a degree $4$ one, in fact), this is non-negative. By the squared triangle inequality, we have that $D(i,k) \le D(i,j) + D(j,k)$ for any $i,j,k$ has a degree $4$ sum-of-squares certificate. \\
	We have a distribution $\mu'$ such that $\E_{\mu'} f_G \le \pE_{\mu} f_G$, but we would also like to show that $\E_{\mu'} f_{K_n} \ge C \cdot \pE_\mu f_{K_n}$. Denoting by $\indic(\hat{x})$ the set of vertices where $\hat{x} = 1$, we have that $\E_{\mu'} f_{K_n} = |\indic(\hat{x})|(n - |\indic(\hat{x})|)$.

	\begin{fdef}
		$A,B \subseteq V$ are said to be \emph{large $\Delta$-separated sets} if for all $i \in A, j \in B$, $D(i,j) \ge \Delta$ and $|A||B| = \Omega(n^2)$.
	\end{fdef}

	If we manage to find large $\Delta$-separated sets, we can do the rounding (similar to the min-cut procedure) using $D(j,A)$, so
	\[ \E_{\mu'} f_{K_n} = \sum_{i,j} \E_{\mu'} \frac{1}{4} (x_i - x_j)^2 \ge \sum_{i\in A,j\in B} \E_{\mu'} \frac{1}{4} D(j,A) \ge \frac{\Delta}{4} |A||B| = \Omega(\Delta n^2) \ge \Omega(\Delta) \pE_\mu f_{K_n}. \]
	Therefore, if we manage to find large $\Delta$-separated sets, we immediately get an $O(1/\Delta)$-approximation algorithm.

	\begin{ftheo}[Global Structure Theorem]
		\label{arv-glob-struct}
		Let $G$ be a $d$-regular graph and $\mu$ a degree $4$ pseudodistribution. Suppose that $\sum_{i,j} D(i,j) = \Omega(n^2)$.
		Then, there exist $A,B$ that can be found in polynomial time that are $\Omega(1/\sqrt{\log n})$-separated.
	\end{ftheo}
	Let us first give a proof of the Leighton-Rao $O(\log n)$-approximation algorithm, which proves the above for $\Omega(1/\log n)$-separation instead. Their original Leighton-Rao proof was based on linear programming, but we give one based on the above sum-of-squares idea.
	\begin{proof}[Proof of weaker \nameref{arv-glob-struct}]
		Like before, assume wlog that $\pE_\mu x = 0$, and let $g \sim \mathcal{N}(0,\pE_\mu xx^\top)$. Let
		\[ A^{(0)} = \{i : g_i \le -1\} \text{ and } B^{(0)} = \{i : g_i \ge 1\}. \]
		Because $\E g_i^2 = 1$ and $\E g_i = 0$, these two sets have size $\Omega(n)$ in expectation.\\
		However, this is not enough, because it is possible that the probability that \emph{both} sets are large is small. To fix this, we can show without much difficulty that $\Pr[g_i \le -1 \text{ and } g_j \ge 1] \ge C D(i,j)$ for some constant $C$ using arguments from earlier sections. Since $\sum_{i,j} D(i,j) = \Omega(n^2)$, we can use linearity of expectation to conclude that $\E |A^{(0)}||B^{(0)}| = \Omega(n^2)$.\\
		Now, while $A^{(0)}$ and $B^{(0)}$ are well-separated for the draw of the Gaussian that is used to define them, we care about the separation in expectation. That is, while $g_j - g_i \ge 2$ for $i \in A^{(0)}, j \in B^{(0)}$, we want that $\E (g_i - g_j)^2 \ge \Delta$ for $i \in A, j \in B$. We would like to show that if some coordinates appear in $A^{(0)},B^{(0)}$, then with good probability they are ``good'' pairs in the sense of also being $\Delta$-separated. \\
		If we have for some pair $i,j$ that $\E (g_i - g_j)^2 \le \Delta$, then standard concentration inequalities show that $\Pr[g_j - g_i \ge 2] \le e^{-\Omega(1/\Delta)}$. When $\Delta = O(1/\log n)$, we can use a union bound argument to extend this to all pairs $i,j$.
	\end{proof}

	Consider the graph $H$ on vertex set $[n]$ with $ij$ adjacent iff $D(i,j) \le \Delta$. Our goal is to find $\Omega(n)$-sized sets $A,B$ such that there are no edges between $A$ and $B$ in $H$. Such pairs are typically called \emph{vertex separators}.\\
	The $A^{(0)}$ and $B^{(0)}$ we have now are random sets such that $(g_j - g_i) \ge 2$ for $i \in A^{(0)}, j \in B^{(0)}$ and $\E |A^{(0)}| |B^{(0)}| = \Omega(n^2)$. The issue that might require us to throw away vertices from $A^{(0)}$ and $B^{(0)}$ is when for some two vertices, $\E (g_j - g_i)^2 \le \Delta$. One cause for this might be if in our draw, there is some vertex $j$ whose value $g_j$ is abnormally large, which results in it contributing to many ``bad'' edges. To account for this, in the ARV result, we remove such vertices only once using a matching.  \\
	Before the algorithm begins, deterministically fix an ordering on the vertices. Greedily find a maximal matching $M$ in $E(H) \cap (A^{(0)} \times B^{(0)})$, and then set $A = A^{(0)} \setminus V(M)$ and $B = B^{(0)} \setminus V(M)$. Note that this $A$ and $B$ are indeed $\Delta$-separated -- were they not, we would be able to make the matching larger by adding an edge. For ease of notation, direct all edges in $M$ from $A^{(0)}$ to $B^{(0)}$.\\
	% If $ij \in M$, then $g_j - g_i \ge 2$ but $\E (g_j - g_i)^2 \le \Delta$.
	We would like to show that $M$ is small, so not too many vertices are removed from $A^{(0)}$ and $B^{(0)}$ to get $A,B$.
	% Next, we claim that if $|A||B| = o(n^2)$, then $\E |M| \ge \Omega(n)$.
	To prove this, we shall show that
	\begin{flem}
		\label{lem: arv-matching-size}
		It holds that
		\[ O(\sqrt{\log n}) \ge \E \max_{i,j \in [n]} (g_j - g_i) \ge \frac{\Omega(1)}{\Delta} \cdot \left( \frac{\E |M|}{n} \right)^3. \]
	\end{flem}
	In particular, on setting $\Delta = \Theta(1/\sqrt{\log n})$, $|M|$ is $O(n)$, so $|A||B| = \Omega(n^2)$, completing the proof of the \nameref{arv-glob-struct}.
	\begin{proof}%[Proof of \cref{eqn: arv-matching-size}]
		The left-hand-side is proved rather simply, using the fact that if $(Z_1,Z_2,\ldots,Z_t)$ are jointly Gaussian, then
		\begin{equation}
			\label{eqn: var-max-gauss}
			\Var \max_{i \le t} Z_i \le O(1) \max_{i \le t} \Var Z_i.
		\end{equation}
		The variable $\max Z_i$ is actually a subgaussian distribution around its mean, which is around $\sqrt{\log t}$ (this may be proved using a Hoeffding bound argument). In our context, we have $n^2$ Gaussians $(g_j - g_i)$, and this is around $O(\sqrt{\log n})$ with good probability.\\

		The more difficult part of the inequality is the lower bound. Let $H^k(i)$ be the vertices at most $k$ steps away of $i$ in the graph $H$, and $\gamma_i^k = \max_{j \in H^k(i)} g_j - g_i$. The idea is that we shall ``chain'' together edges in the matching, each of which increases the difference $g_j - g_i$ of the terminal edges in the path by $2$, such that the final difference is quite large. Let $\phi_k = \sum_i \E \gamma_i^k$. We shall try to show $\phi_\cdot$ becomes quite large. More precisely, we claim that
		\begin{equation}
			\label{eqn: chaining}
			\phi_{k+1} - \phi_k \ge \E |M| - O(n) \max_{\substack{i \in [n] \\ j \in H^{k+1}(i)}} (\E (g_j - g_i)^2)^{1/2}.
		\end{equation}
		Given this, let us prove the lower bound. Note that for any vertices $i,j$ which are $k$ steps apart in $H$, we have $\E (g_i - g_j)^2 \le k\Delta$ by the squared triangle inequality. Consequently, using \eqref{eqn: chaining},
		\[ \phi_{k+1} - \phi_k \ge \E |M| - O(n) \sqrt{k\Delta}. \]
		Set $k_0 = \frac{c}{\Delta} \left(\frac{\E|M|}{n}\right)^2$, for some constant $c$ to be fixed later. For $k \le k_0$, we have that
		\[ \phi_{k+1} - \phi_k \ge 2 \E |M| - O(n) \sqrt{\Delta \cdot \frac{c}{\Delta} \left(\frac{\E|M|}{n}\right)^2} \ge \E |M|, \]
		where we set $c$ appropriately to get the second inequality. In particular, $\phi_{k_0} \ge k_0 \E|M|$. Noting that $\max_{i,j \in [n]} (g_j - g_i) \ge \phi_{k_0}/n$, we get that
		\[ \max_{i,j \in [n]} (g_j - g_i) \ge \Omega\left(\frac{k_0}{n}\E |M|\right) = \frac{\Omega(1)}{\Delta} \left(\frac{\E|M|}{n}\right)^3 \]
		as desired.\\

		Now, to conclude the proof, let us prove \cref{eqn: chaining}. First off, we have that if $ij \in E(H)$, $H_k(j) \subseteq H_{k+1}(i)$ so
		\[ \gamma_j^{k+1} \ge \gamma_i^k + (g_j - g_i). \]
		In particular, if $ij$ is an edge in the matching $M$, then by definition, $\gamma_j^{k+1} \ge \gamma_i^k + 2$.
		Now, for each vertex $i$, set
		\[ L_i = \begin{cases} 1, & \text{$i$ has an outgoing edge in $M$,} \\ 0, & \text{$i$ has an incoming edge in $M$,} \\ 1/2, & \text{$i$ is unmatched in $M$} \end{cases} \]
		and $R_i = (1-L_i)$. Because $H^{k+1}(\cdot) \supseteq H^k(\cdot)$, we have that $\gamma_{\cdot}^{k+1} \ge \gamma_{\cdot}^k$ and so,
		\[ \sum_i L_i \gamma_i^{k+1} \ge \sum_i R_i \gamma_i^k + 2|M| \]
		and
		\begin{equation}
			\label{eqn: arv-chaining-penult}
			\sum_i \E L_i \gamma_i^{k+1} \ge \sum_i \E R_i \gamma_i^k + 2 \E |M|.
		\end{equation}
		Note that for a specific draw $g$ of the gaussians, the matching $M(g)$ has all the edges in $M(-g)$ but reversed (due to the greedy nature when defining $M$). Therefore, the probability that a vertex has an incoming edge in $M$ is the same as the probability that a vertex has an outgoing edge in the perfect matching. This implies that $\E L_i = \E R_i = (1/2)$. \\
		We are almost done, barring the issue that the expectation of the product above is not equal to the product of the expectation. The difference between the two is bounded using Cauchy-Schwarz without too much difficulty as
		\begin{align*}
			\left| \E L_i \gamma_i^{k+1} - \E L_i \E \gamma_i^{k+1} \right| &= \left| \E (L_i - \E L_i)(\gamma_i^{k+1} - \E \gamma_i^{k+1}) \right| \\
				&\le \sqrt{\E (L_i - \E L_i)^2} \sqrt{\E(\gamma_i^{k+1} - \E \gamma_i^{k+1})^2} \\
				&\le \sqrt{\Var \gamma_i^{k+1}} \\
				&\stackrel{\eqref{eqn: var-max-gauss}}{\le} O(1) \sqrt{\max_{j \in H^{k+1}(i)} \E (g_j - g_i)^2}, \\
		\end{align*}
		with a similar inequality for $R_i$ instead of $L_i$.
		Substituting this back in \Cref{eqn: arv-chaining-penult}, we get that
		\begin{align*}
			\phi^{k+1} &= \sum_i \E \gamma_i^{k+1} \\
				&= 2 \sum_i \E L_i \E \gamma_i^{k+1} \\
				&\ge 2 \sum_i \E L_i \gamma_i^{k+1} - O(1) \sum_i \sqrt{\max_{j \in H^{k+1}(i)} \E (g_j - g_i)^2} \\
				&\ge 2 \sum_i \E R_i \gamma_i^k + 2 \E |M| - O(1) \sum_i \max_{j \in H^{k+1}(i)} \sqrt{\E (g_j - g_i)^2} \\
				&\ge 2 \sum_i \E R_i \E \gamma_i^k + 2 \E |M| - O(1) \sum_i \max_{j \in H^{k+1}(i)} \sqrt{ \E (g_j - g_i)^2} \\
				&\ge \phi^{k} + 2 \E |M| - O(n) \max_{\substack{i \in [n] \\ j \in H^{k+1}(i)}} \sqrt{\E (g_j - g_i)^2}. \qedhere
		\end{align*}
		% completing the proof.
	\end{proof}

\subsection{The Unique Games Conjecture}
\label{subsec:ugc}

	\subsubsection{Introduction}

		\begin{fdef}[$2$-Constraint Satisfaction Problem]
			In a \emph{$2$-CSP}, we have $n$ variables $(x_i)_{i=1}^n$, which take values in a finite alphabet $[q]$ and $m$ constraints $( (C_i,S_i) )_{i=1}^m$, where each $C_i$ is a pair of variables $(x_{i_1},x_{i_2})$, and $S_i \subseteq [q]^2$. A certain constraint $(C_i,S_i)$ is said to be \emph{satisfied} by a certain assignment of the variables if the corresponding pair is contained in $S_i$. The algorithmic goal of a $2$-CSP is to find an assignment that maximizes the number of satisfied assignments.
		\end{fdef}
		In our setting, $q$ is typically a constant. An example of a (boolean) $2$-CSP is the max-cut problem, where for each edge $ij$ in the graph, we have the constraint $(x_i,x_j) \in \{(0,1),(1,0)\}$. An example of a non-boolean $2$-CSP is max $3$-coloring, where we try to find a coloring of the vertices that maximizes the number of ``good'' edges which have end-vertices of different colours -- try to encode this as a CSP!

		\begin{fdef}[Promise Problem]
			For $0 \le s \le c \le 1$, the $(c,s)$ promise problem takes as input a $2$-CSP instance, and the goal is to decide whether
			\begin{enumerate}
				\item there exists an assignment that satisfies a $\ge c$ fraction of constraints, or
				\item every assignment satisfies a $\le s$ fraction of the constraints.
			\end{enumerate}
			The quantities $c$ and $s$ are typically referred to as \emph{completeness} and \emph{soundness.}
		\end{fdef}
		When the input is a general $2$-CSP, we refer to this problem as $(c,s)$-2CSP. We use similar notation throughout, for example in restricted classes of $2$-CSPs.\\
		It is clear that if we have a $(s/c)$-approximation algorithm for a CSP, then the $(c,s)$-version of this problem is easy.\\

		For example, a $(1,1-1/m)$-2CSP could be used to check the satisfiability of a CNF. For $q \ge 3$, $(1,1-1/m)$-2CSP is \textsf{NP}-hard.

		\begin{fdef}[Unique $2$-CSP]
			A $q$-sized alphabet $2$-variable constraint $(C,S)$ is said to be \emph{unique} if for every possible assignment to one of the two variables, there is exactly one satisfying assignment to the other variable. A $2$-CSP is said to be a \emph{unique $2$-CSP} or \emph{unique game} if all its constraints are unique. 
		\end{fdef}
		A unique constraint is essentially just a permutation $\pi$ of $[q]$, where the set $S$ is just $\{(i,\pi(i)) : i \in [q]\}$.\\
		Max-Cut is an example of a boolean unique game, while MAX2SAT is not. A non-boolean unique game is MAX2LIN, where $q$ is a prime and the constraints are linear equations like $x_i \pm x_j \equiv a_{ij} \pmod{q}$.\\

		Given that $(1,1-1/m)$-2CSP is NP-hard for $q \ge 3$, the following may be slightly surprising.
		\begin{ftheo}[Propagation]
			There is a polynomial time algorithm for $(1,1-1/m)$-UG.
		\end{ftheo}
		\begin{proof}
			Let $G$ be the graph of all pairs that appear in constraints. Assume wlog that $G$ is connected; if it is not, we can apply the following argument separately on connected components. Start an arbitrary vertex $u$, and give it the assignment $1$. Due to uniqueness, this single assignment propagates to an assignment on all vertices in $u$. If we get conflicting assignments at some point, then there is no satisfying assignment which assigns $1$ to $u$. We can then repeat this by varying the assignment for $u$ to determine if there is a satisfying assignment. 
		\end{proof}

		Given the previous, the following may be very surprising.
		\begin{fcon}[Unique Games Conjecture]
			For any $\epsilon > 0$, for sufficiently large $q_{\epsilon}$, $(1-\epsilon,\epsilon)$-UG is \textsf{NP}-hard.
		\end{fcon}
		The conjecture was born when trying to show ``hardness of approximation'' results, which claim that even getting a good approximation of certain quantities is \textsf{NP}-hard. For example, recall how in \Cref{subsec:max-cut}, we had said that the Unique Games Conjecture would imply that getting a $(\aGW+\epsilon)$-approximation of the max-cut is \textsf{NP}-hard. The real advent of these results was with the PCP Theorem, an implication of which is the following.

		\begin{theorem}
			For some constant $c > 0$, $(1,1-c)$-3SAT is \textsf{NP}-hard.
		\end{theorem}
		In the original proof of the above, the constant $c$ was incredibly tiny. Later, this was improved by Ran Raz \cite{raz-parallel-reptn} in the parallel repetition theorem. Finally, H\r{a}stad \cite{hastad-appx-3sat} proved the best possible result, showing that for any $\epsilon > 0$, $(1,7/8 + \epsilon)$-3SAT is \textsf{NP}-hard; there is a simple (but clever) $7/8$-approximation algorithm. In the same paper, H\r{a}stad also proved that $(1,1/2+\epsilon)$-3LIN is \textsf{NP}-hard. This provided a more general system for proving hardness of approximation results, including many other CSPs. However, questions regarding the hardness of approximating problems like MAX-CUT and MAX2LIN remained elusive. Recall that for the former, \Cref{prop: gw-reparametr} implies that $(1-\epsilon,1-\sqrt{\epsilon})$-CUT is in \textsf{P}. VERTEX-COVER was another problem that seemed difficult to approximate -- a simple algorithm gives a $2$-approximation, but we were unable to do better on the algorithmic or hardness fronts.

	\subsubsection{A brief history of unique games}

		The Unique Games Conjecture was proposed by Subhash Khot \cite{ugc-og} in 2002, and connected it to one of the three problems mentioned above.

		\begin{theorem}[Khot]
			If the Unique Games Conjecture is true, for all $\epsilon > 0, 1 \ge t \ge (1/2)$, $(1-\epsilon,1-\epsilon^t)$-2LIN is \textsf{NP}-hard.
		\end{theorem}

		In 2003, Khot-Regev \cite{khot-regev-ugc-vtxcov} connected it to VERTEX-COVER.

		\begin{theorem}[Khot-Regev]
			If the Unique Games Conjecture is true, for all $\epsilon > 0$, $(2-\epsilon)$-VERTEXCOVER is \textsf{NP}-hard.
		\end{theorem}
		An alternate way of phrasing this is as follows. Supposing the Unique Games Conjecture is true and someone manages to synthesize a $(2-\epsilon)$-approximation algorithm for VERTEX-COVER, then $\mathsf{P} = \mathsf{NP}$, so we can in fact solve VERTEX-COVER exactly!

		In 2004, Khot-Kindler-Mossell-O'Donnell \cite{max-cut-ugc} connected it to MAX-CUT, as we mentioned in \Cref{subsec:max-cut}.

		\begin{theorem}[Khot-Kindler-Mossell-O'Donnell]
			For $\epsilon > 0$, $(1-\rho_\text{GW},\aGW+\epsilon)$-CUT is \textsf{NP}-hard.
		\end{theorem}
		This paper gave some very surprising connections between Gaussian rounding of CSPs and $2$-CSPs! A bit later, O'Donnell-Wu gave a strengthening of this, to the analogue of \Cref{prop: gw-reparametr}, showing we cannot do better than RPR$^2$ rounding either! Finally, in 2008, the following was showed by Raghavendra \cite{raghavendra-punchline}.

		\begin{ftheo}[Raghavendra]
			For every CSP, there exists a natural SDP (the analogue of degree $2$ sum-of-squares) and a natural rounding (the analogue of RPR$^2$) that, assuming the Unique Games Conjecture, is optimal.
		\end{ftheo}

		% take a graph G exhibiting the integrality gap \alpha for deg 2 sos (for max-cut say). then you can reduce UG to Max-Cut with \alpha factor hardness. gadget reduction with this graph being the gadget.

		Now, setting implications aside, is the Unique Games Conjecture true?\\

		In Khot's original paper \cite{ugc-og}, an $(1/q)$-approximation algorithm was given. More precisely, they showed that $(1-\epsilon,1 - O(q^2\epsilon^{1/5}\sqrt{\log 1/\epsilon}))$-UG is solvable in polynomial time; this is very good in the small $\epsilon$ regime. There is a long line of work improving on this, and today, the best known algorithm is due to Charikar-Makarychev-Makarychev in 2007 \cite{ug-optimal-algo}, where they gave an optimal polynomial time algorithm for $(1-\epsilon,1-O(\sqrt{\epsilon \log q}))$-UG. This algorithm is optimal in the sense that if the Unique Games Conjecture is true, we cannot do any better! More concretely, \cite{max-cut-ugc} also showed that if the Unique Games Conjecture is true, then for any $\epsilon > 0$, $(1-\epsilon,1 - \sqrt{\frac{2}{\pi}}\sqrt{\epsilon \log q} + \epsilon)$-UG is \textsf{NP}-hard! Improving the \cite{ug-optimal-algo} algorithm just a little bit would disprove the Unique Games Conjecture.\\
		All the above results are algorithms using degree $2$ sum-of-squares (or rather, an analogue of what we have seen so far in a non-boolean setting).\\

		On the other hand, a proof of the Unique Games Conjecture also seems within reach!
		In 2010, Arora-Barak-Steurer \cite{subexp-ug} showed that for all $q,\epsilon$, there exists a $2^{q^2n^{O(\epsilon^{1/3})}}$-time algorithm to find a $(1/2)$-satisfying assignment for any $(1-\epsilon)$-satisfiable instance of UG. That is, $(1-\epsilon,1/2)$-UG has a $2^{q^2 n^{O(\epsilon^{1/3})}}$-time algorithm. This is better than the na\"{i}ve exponential time $2^{O(n)}$ algorithm.\\
		Consider the following, proposed by Impagliazzo-Kabanets-Wigderson \cite{eth} in 2002. This can be thought of as a stronger version of $\mathsf{P} \ne \mathsf{NP}$.

		\begin{fcon}[Exponential Time Hypothesis]
			$3$-SAT does not have a $2^{o(n)}$-algorithm.
		\end{fcon}
		The ETH is easily seen to imply that $3$-coloring doesn't have a $2^{O(n)}$ algorithm. If it was able to get a $2^{n^{o(1)}}$ algorithm for $(1-\epsilon,1/2)$-UG, assuming the ETH would imply that the UGC is false. Indeed, if the UGC were true instead, there is a $\poly(n)$ algorithm that reduces a 3\textsf{SAT} instance to an instance of UG. Using the $2^{n^{o(1)}}$ algorithm on top of this yields a $2^{n^{o(1)}}$ algorithm for 3\textsf{SAT}, contradicting ETH.\\
		We also have that assuming the ETH and the UGC, the Arora-Barak-Steurer algorithm shows that no reduction from 3\textsf{SAT} to UGC can have an instance size blow-up of less than $O(n^{\epsilon^{-1/3}})$.

		Recently in 2018, Dinur-Khot-Kindler-Minzer-Safra \cite{2-2-gc} settled the $2$-to-$2$ Games Conjecture, which is a weaker version of the Unique Games Conjecture, wherein instead of ``unique'' constraints where fixing a variable leaves precisely \emph{one} choice for the other variable, there are now \emph{two} possibilities. A consequence of this is that $(1/2-\epsilon,\epsilon)$-UG is \textsf{NP}-hard.\\

		% UGC SAYS THAT YOU CANNOT BEAT THE INTEGRALITY GAP OF DEG 2 SOS

\subsection{Global correlation rounding}

	We have extensively looked at Gaussian rounding, which works for degree $2$ pseudodistributions. Later, we combined this with the squared triangle inequality to get something for degree $4$ pseudodistributions. In this section, we shall study a more general scheme called global correlation rounding that works for higher degree pseudodistributions. This is also the idea behind the Arora-Barak-Steurer proof we mentioned in the previous section.\\
	Our running example over this section will be the max-cut problem over a certain restricted class of graphs.\\

	Recall the normalized adjacency matrix (random walk matrix) $A$ of a $d$-regular graph $G$ on $[n]$. Let $1 = \lambda_1 \ge \cdots \ge \lambda_n \ge -1$ be the eigenvalues of $A$. Recall that $G$ is said to be a combinatorial expander if the conductance $\Phi_G \ge \delta$ for some constant $\delta$, and a spectral expander if $\lambda_2 \le 1-\delta$. It may be shown that these two notions are equivalent, with $\Phi_G \ge \delta$ implying that $\lambda_2 \le 1 - O(\delta^2)$.\\
	An example of an expander is a random $d$-regular graph. We also have more explicit examples such as \emph{configuration models}, where $\lambda_2 = \frac{2\sqrt{d-1}}{d} + o(1)$.

	\begin{fdef}
		Let $G$ be a $d$-regular graph with eigenvalues $\lambda_1 \ge \cdots \ge \lambda_n$. The \emph{$\rho$-threshold rank} of $G$ $\rank_\rho(G)$ is the number of eigenvalues that are at least $\rho$.
	\end{fdef}
	In particular, on expanders, $\rank_{1-\epsilon}(G) = 1$ for sufficiently small $\epsilon$. The threshold rank ends up being related to some notion of graph expansion, but we shall not study this.\\
	
	\begin{ftheo}
		Let $G$ be a $d$-regular graph on $[n]$, $r = \rank_\delta(G)$. Let $r = \rank_{\delta}(G)/\delta^4$. Given a degree $r+2$ pseudodistribution $\mu$, we can find in polynomial time a distribution $\mu'$ such that
		\[ \E_{\mu'} f_G \ge \pE_{\mu} f_G - m\cdot O(\sqrt{\delta}). \]
	\end{ftheo}
	The ``restricted class of graphs'' we mentioned is that of graphs with bounded threshold rank. This gives us a $n^{O(r)}$ time algorithm to compute an additive $O(\sqrt{\delta})$-approximation of the max-cut. Alternatively, this gives us an algorithm to find an $1-O(\sqrt{\delta})$ multiplicative approximation of the max-cut -- this is a consequence of the fact that the max-cut is at least $m/2$. In particular, this gives us a polynomial time algorithm for max-cut on expanders.

	\begin{fdef}[Marginal pseudodistribution]
		Let $\mu$ be a pseudodistribution on $\{-1,1\}^n$. For any $S \subseteq [n]$, the marginal pseudodistribution $\restr{\mu}{S} : \{-1,1\}^S \to \R$ is defined by
		\[ \restr{\mu}{S}(y) = \sum_{x : \restr{x}{S} = y} \mu(x). \]
	\end{fdef}

	The following observations near-immediately follow from the definition.
	\begin{fprop}
		\label{prop:marginal-facts}
		Let $\mu$ be a degree $r$ pseudodistribution on $\{-1,1\}^n$. Then,
		\begin{enumerate}[label=(\alph*)]
			\item For any $U \subseteq S$, $\pE_\mu x_U = \pE_{\restr{\mu}{S}} x_U$.
			\item For any function $f$ of degree $r$ that depends only on variables in $S$, $\pE_\mu f = \pE_{\restr{\mu}{S}} f$.
			\item Let $\mu$ be a degree $r$ pseudodistribution on $\{-1,1\}^n$. Let $S \subseteq [n]$ with $|S| \le r/2$. Then, $\restr{\mu}{S}$ is an actual probability distribution. For such $S$, we call this distribution the \emph{local distribution} of $\mu$ on $S$.
		\end{enumerate}
	\end{fprop}
	(c) above is essentially a consequence of \Cref{prop: deg-2n-sos}. 

	\begin{fdef}[Reweighting]
		Let $\mu$ be a degree $r$ pseudodistribution. Suppose $p$ is polynomial of degree $r' \le r$ that has a sum-of-squares certificate, such that $\pE_\mu p > 0$ (without this assumption, we only have non-negativity). Then, $\mu'$, the \emph{reweighting} of $\mu$ by $p$, is defined by
		\[ \mu'(x) = \frac{\mu(x) p(x)}{\pE_\mu p}. \]
	\end{fdef}

	Defining $\mu'$ as above, we have for any other polynomial $f$ that
	\[ \pE_{\mu'} f = \frac{\pE_\mu pf}{\pE_\mu p}. \]

	\begin{fprop}
		Let $\mu$ be a degree $r$ pseudodistribution, $p$ a sum-of-squares polynomial of degree at most $r'$ with $\pE_\mu p > 0$. Then, $\mu'$, the reweighting of $\mu$ by $p$, is a degree $(r-r')$ pseudodistribution.
	\end{fprop}
	\begin{proof}
		We clearly have $\pE_{\mu'} 1 = 1$. For any function $f$ of degree $\le (r-r')/2$,
		\[ \pE_{\mu'} f^2 = \frac{\pE_{\mu} pf^2}{\pE_\mu p} \ge 0 \]
		since $pf^2$ is an SoS polynomial of degree at most $r$.
	\end{proof}

	We can also more generally define conditioning on events.

	\begin{fdef}[Conditioning]
		Let $\mu$ be a pseudodistribution of degree $r$, and let $S \subseteq [n]$ with $|S| \le r/2$. Let $\alpha \in \{-1,1\}^S$. Then, $\restr{\mu}{x_S = \alpha}$ is the reweighting of $\mu$ by the polynomial $f_\alpha^2$, where
		\[ f_\alpha(x) = \begin{cases} 1, & \restr{x}{S} = \alpha, \\ 0, & \text{otherwise.} \end{cases} \]
	\end{fdef}

	How do we round high-degree pseudodistributions?

	\begin{fdef}[Independent rounding]
		Let $\mu$ be a pseudodistribution. The independent rounding of $\mu$ is the distribution of the random variable $x'$ which independently sets
		\[ x_i' = \begin{cases} 1, & \text{with probability $1/2 + (1/2)\pE_\mu x_i$} \\ -1, & \text{with probability $1/2 - (1/2)\pE_\mu x_i$.} \end{cases} \]
	\end{fdef}
	That is, the different coordinates are independent and $\E x_i' = \pE_\mu x_i'$.

	\begin{fprop}
		Let $\mu$ be a pseudodistribution (of degree $\ge 2$) and $x'$ the independent rounding of $x$ as defined above. Then,
		\[ \E f_G(x') = \pE_\mu f_G(x) + \sum_{ij \in E} \frac{1}{2} \pCov_\mu(x_i,x_j), \]
		where $\pCov_\mu (x_i,x_j) \defeq \pE_\mu (x_i - \pE_\mu x_i)(x_j - \pE_\mu x_j)$.
	\end{fprop}
	This intuitively makes sense. In the scenario where the various coordinates according to $\mu$ are independent, we do not incur any loss when doing independent rounding. A loss gets introduced when there is a negative correlation. This makes sense in the context of cuts, since here a negative correlation between two adjacent vertices corresponds to the two vertices being likely to be on opposite sides of the cut; when we make it independent, this information gets lost.
	\begin{proof}
		Due to linearity of expectation, we can look at a single edge $ij \in E$. On the left, independence implies that
		\[ \E (x_i' - x_j')^2 = \E x_i'^2 + \E x_j'^2 - 2 \E x_i' \E x_j' = 2 - 2 \pE_\mu x_i \pE_\mu x_j. \]
		and
		\[ \pE_\mu (x_i - x_j)^2 = 2 - 2\pE_\mu x_i x_j. \]
		The desideratum follows on subtracting the two, since $\pCov_\mu(x_i,x_j) = \pE_\mu x_i x_j - \pE_\mu x_i \pE_\mu x_j$.
	\end{proof}

	The main insight of global correlation rounding is the following. We start with a high-degree pseudodistribution $\mu$, and produce a low average covariance pseudodistribution $\mu_1$ but conditioning it, while ensuring that $\pE f_G$ does not change by much. Following this, we just independently round $\mu_1$ to $\mu'$ and are done.\\

	\begin{fdef}[Local correlation]
		Let $\mu$ be a pseudodistribution on $\{-1,1\}^n$ and $G$ a graph on $[n]$. The \emph{local correlation} of $\mu$ under $G$ is
		\[ \LC_G(\mu) = \E_{ij \in E} \left|\pCov(x_i,x_j).\right| \]
	\end{fdef}
	We drop the $G$ subscript if it is clear from context.\\
	If the local correlation were less than $\delta$ (i.e., it is used to represent the ``low average covariance'' condition we described in the previous paragraph), we get that
	\[ \E f_G(x') \ge \pE_{\mu_1} f_G - m\delta \approx \pE_\mu f_G - m\delta. \]

	If independent rounding fails to give an additive $\delta$ approximation, then we must have that the local correlation is at least $2\delta$.\\
	Unfortunately, we cannot expect to find a pseudodistribution of low local correlation for arbitrary $G$, since we cannot approximate max-cut to an arbitrarily small error.\\ % there is no PTAS for max-cut.

	Recall that the \emph{entropy} of a random variable $X$ on $[q]$ is defined by
	\[ H(X) \defeq \sum_{i \in [q]} \Pr[X = i] \log \Pr[X = i]. \]
	Also recall the \emph{mutual information} between a pair of random variables, which is a measure of how independent the two are, defined by
	\[ \I(X;Y) \defeq \sum_{i,j \in [q]} \Pr[X=i,Y=j] \log \frac{\Pr[X=i,Y=j]}{\Pr[X=i]\Pr[Y=j]}. \]
	In particular, the mutual information of two independent random variables is $0$. A consequence of these definitions is that defining the conditional entropy of two random variables by
	\[ H(X\mid Y) = \sum_{i \in [q]} \Pr[Y = i] H(X \mid Y = i), \]
	we have
	\[ \I(X;Y) = H(X) - H(X\mid Y) = H(Y) - H(Y\mid X). \]

	\begin{fprop}
		\label{prop:cov-mutinfo}
		Let $X,Y$ be $[q]$-valued random variables. Then,
		\[ |\Cov(X,Y)| = O(\sqrt{\I(X;Y)}). \]
	\end{fprop}

	\begin{fdef}[Global correlation]
		Let $\mu$ be a pseudodistribution of degree at least $4$. The \emph{global correlation} of $\mu$ is defined by
		\[ \GC(\mu) = \E_{i,j \in [n]} \I(X_i;X_j). \]
	\end{fdef}
	Note here that we are using $\I$ despite $\mu$ being a pseudodistribution because \Cref{prop:marginal-facts}(c) implies that the marginal $\restr{\mu}{\{i,j\}}$ is indeed a distribution.\\
	Unlike local correlation, we can always find a pseudodistribution with low global correlation.

	\begin{flem}
		Let $\mu$ be a pseudodistribution on $\{-1,1\}^n$ of degree $\ell + 2/\eta$. Then, there is a reweighting $\mu'$ (of degree at least $\ell$) of $\mu$ by a degree $2/\eta$ sum-of-squares polynomial such that $\GC(\mu') \le \eta$.
	\end{flem}
	\begin{proof}
		We shall build a sequence of reweightings $\mu = \mu_0, \mu_1, \ldots, \mu_{t-1}, \ldots$, with each $\mu_t$ being a degree $2$ reweighting of $\mu_{t-1}$. We shall do this $\eta$ times, then show that one of these $\eta$ reweightings must satisfy the required.\\
		Sample a uniformly random $1/\eta$-tuple of variables from $[n]$, say $i_1,\ldots,i_{1/\eta}$. For $1 \le t \le 1/\eta$, sample $\alpha_{t}$ from $\restr{\mu_{t-1}}{i_t}$. Finally, get $\mu_t$ from $\mu_{t-1}$ by conditioning on $x_{i_t} = \alpha_t$.\\
		We are obtaining $\mu_t$ from $\mu_{t-1}$ by reweighting with $\indic_{x_{i_t} = \alpha_t}$, which is a degree $1$ polynomial, so $\mu_t$ has degree $2$ less than $\mu_{t-1}$. Now, how do we show that one of the $\mu_i$s work?\\

		Now, suppose instead that for all $t \le 1/\eta$, $\E_{i,j \in [n]} \I_{\mu_t}(x_i;x_j) > \eta$. So, for some $i$, $\E_{j} \I_{\mu_t}(x_i,x_j) > \eta$. Conditioning on $x_i = \alpha_i$ for some such $i$,
		\[ \E_j H_{\mu_{t-1}}(x_j) - H_{\mu_t}(x_j) = \E_j H_{\mu_{t-1}}(x_j) - H_{\mu_{t-1}}(x_j \mid x_i) = \E_j \I_{\mu_{t-1}}(x_i;x_j) \]
		is large. Considering the potential function $\E_i H_{\mu_t} (x_i)$, this means that if the chosen $i$ is a typical $i$, the potential increases by $\eta$ at each step. However, this potential function cannot go over $1$, so we are done since we have $1/\eta$ pseudodistributions.\\
		This gives us an $n^{O(1/\eta)}$ algorithm to also find this reweighting, by brute-forcing over all possible $i_1,\ldots,i_{1/\eta}$ -- this algorithm also uses the fact that global correlations can easily be computed for a given pseudodistribution.
	\end{proof}

	Now, we would like to connect global correlation and local correlation. This is where our assumption of bounded threshold rank enters the picture.

	\begin{flem}
		Let $G$ be a graph on $[n]$ and $\mu$ a pseudodistribution on $\{-1,1\}^n$. If $\LC_G(\mu) \ge \delta$, then for any $\rho$,
		\[ \GC(\mu) \ge \left( \frac{\delta-\rho}{\rank_\rho(G)} \right)^2. \]
		In particular,
		\[ \GC(\mu) \ge \left(\frac{\delta}{2\rank_{\delta/2}(G)}\right)^2. \]
	\end{flem}
	If $\GC(\mu) < \eta$, then we have that $\LC_G(\mu) < \delta$ for any $\delta$ such that
	\[ \delta^2 \le 4 \rank_{\delta/2}(G)^2 \eta. \]
	This tells us that we should choose some $\eta$ that is much less than $1/\rank_{\delta/2}(G)^2$, that is, we should start with a degree $\Omega(\rank_{\delta/2}(G)^2)$ pseudodistribution. 

	\begin{proof}
		The following claim implies the desideratum. If $M$ is an $n \times n$ PSD matrix such that $\Tr(M) \le n$ and $|M_{ij}| \le 1$ for all $i,j$, then if $\E_{ij \in E} M_{ij} \ge \delta$, then $\E_{i,j \in [n]} M_{ij}^2 \ge \left(\frac{\delta-\rho}{\rank_\rho(G)}\right)^2$.\\
		Let us see why this gives the required. For ease of notation, denote $\overline{x}_i = x_i - \pE_\mu x_i$. The local correlation condition says that $\E_{ij \in E} |\pE_\mu \overline{x}_i \overline{x}_j| \ge \delta$.
		Set $M_{ij} = \pE_\mu \overline{x}_i \overline{x}_j$. $M = \pE_\mu \overline{x} \overline{x}^top$ is clearly PSD, and the above condition gives that $\sum_{ij \in E} M_{ij} \ge \delta$. Using the claim, we get that
		\[ \E_{i,j \in [n]} \pCov_\mu(x_i,x_j)^2 \ge \left( \frac{\delta-\rho}{\rank_\rho(G)} \right)^2. \] 
		\Cref{prop:cov-mutinfo} then yields that
		\[ \GC(\mu) = \E_{i,j \in [n]} \I(X_i;X_j) \ge \Omega\left( \left(\frac{\delta-\rho}{\rank_\rho(G)}\right)^2 \right) \]
		as desired.\\

		Let us now prove the claim. Let $A$ be the transition matrix of $G$, with eigenvalues $\lambda_i$ and corresponding eigenvectors $v_i$. Using the PSDness of $M$ with the fact that the largest eigenvalue of $A$ is $1$, we have
		\begin{align*}
			\delta &\le \E_{ij \in E} M_{ij} \\
				&= \frac{1}{n} \sum_{i,j \in [n]} A_{ij} M_{ij} \\
				&= \frac{1}{n} \sum_{i} \lambda_i v_i^\top M v_i \\
				&\le \frac{1}{n} \left(\sum_{i : \lambda_i \ge \rho} \lambda_i v_i^\top M v_i + \rho \sum_{i} v_i^\top M v_i \right) \\
				&\le \frac{1}{n} \left(\sum_{i : \lambda_i \ge \rho} v_i^\top M v_i + \rho \Tr(M) \right) \\
			(\delta-\rho) &\le \frac{1}{n} \sum_{i : \lambda_i \ge \rho} v_i^\top M v_i.
		\end{align*}
		It follows that for some $v_i$,
		\[ \frac{1}{n} v_i^\top M v_i \ge \frac{\delta-\rho}{\rank_\rho(G)}. \]
		That is,
		\[ \frac{\delta-\rho}{\rank_\rho(G)} \le \frac{1}{n} \|M_2\| \le \frac{1}{n} \|M\|_F = \sqrt{\E_{i,j \in [n]} M_{ij}^2}	, \]
		completing the proof.
	\end{proof}