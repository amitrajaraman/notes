%!TEX root = ./main.tex

\clearpage

\section{More applications}

\subsection{Approximating conductance}

	In \Cref{subsec:max-cut}, we looked at the \textsf{NP}-hard problem of max-cut. Before moving to the rest of this section where we look at conductance, let us look at the far simpler min-cut problem. The reader might know that this problem can be solved in polynomial time using the Ford-Fulkerson max-flow algorithm. Before moving to the main problem of this subsection, let us overkill min-cut by giving a randomized algorithm via SoS.\\
	Min-cut and conductance will be our first example of degree $4$ sum-of-squares. The main reason for the degree $4$ requirement here is the following.

	\begin{ftheo}[Squared Triangle Inequality]
		For indeterminates $a,b,c \in \{-1,1\}$,
		\[ (a-c)^2 \le (a-b)^2 + (b-c)^2 \]
		has a degree $4$ sum-of-squares certificate.
	\end{ftheo}
	\begin{proof}
		We have
		\begin{align*}
			\frac{1}{2}\left((a-b)^2 + (b-c)^2 - (a-c)^2\right) &= b^2 - ab - bc + ac \\
				&= (1-bc)(1-ab) \\
				&= \frac{1}{4} (b^2+c^2-2bc)(a^2+b^2-2ab) = \left(\frac{(b-c)(a-b)}{2}\right)^2. \qedhere
		\end{align*}
	\end{proof}

	While we will not use it, we also state the following, which can be proved by considering a Gaussian with the same first two moments (like in \Cref{sec:quad-optim}) and using the fact that the $L^2$ metric (on $\R$) is a metric.

	\begin{theorem}
		For any degree $2$ pseudodistribution $\mu$,
		\[ \sqrt{\pE_\mu (x_i - x_k)^2} \le \sqrt{\pE_\mu (x_i - x_j)^2} + \sqrt{\pE_\mu (x_j - x_k)^2}. \]
	\end{theorem}

	For the remainder of this subsection, let $G$ be a graph with vertex set $[n]$, and let $\pE$ be any degree $4$ pseudodistribution. We continue to denote by $f_G$ the function in \cref{eqn: fg-def}. Our proof strategy for the min-cut problem will be to give a distribution $\mu'$ such that $\E_{\mu'} f_G \le \pE_{\mu} f_G$. For any $i,j \in [n]$, denote
	\[ D(i,j) = \pE_{\mu} \frac{1}{4} (x_i - x_j)^2 \]
	and for non-empty $A \subseteq [n]$, $D(A,i) = D(i,A) = \min_{j \in A} D(i,j)$. Because $\mu$ is a degree $2$ pseudodistribution (degree $4$ in fact), $D(i,A) \ge 0$ for any $i,A$ and it is bounded from above by $1$. Consider the ``line embedding'' of the vertices $[n]$ on the interval $[0,1]$ defined by $i \mapsto D(i,1)$. $\mu'$ is defined by uniformly randomly choosing $t \in [0,1]$, and outputting the cut $\{i \in [n] : D(i,1) \le t\}$. An edge $\{i,j\}$ is cut with probability
	\[ |D(j,1) - D(i,1)| = \frac{1}{4} \left|\pE_{\mu} (x_i - x_1)^2 - \pE_{\mu} (x_j - x_1)^2 \right| \le \frac{1}{4} \pE_\mu (x_i - x_j)^2 = D(i,j), \]
	where we have used the fact that $\mu$ is degree $4$ for the squared triangle inequality.
	Summing over all edges,
	\[ \E_{\mu'} f_G \le \sum_{ij \in E} D(i,j) = \pE_{\mu} f_G. \]

	Note that this proof works out more generally in the scenario where we have $D(i,A)$ for some set $A$ instead.
	% While this proof may seem complete, it is not -- we require that the cut output by $\mu'$ is (almost surely) not $\emptyset$ or $[n]$. Since $D(j,A) = 0$ for any $j \in A$, the cut is never $\emptyset$. For the cut to be almost surely unequal to $[n]$, we require the existence of some $i \in [n]$ such that $D(i,A) \ne 0$. 

	Let us now get to a more non-trivial problem.

	\begin{fdef}
		Given a $d$-regular graph $G = (V,E)$ with $n$ vertices and a non-empty subset $S \subsetneq V$, the \emph{conductance} or \emph{normalized cut} of $S$ is
		\[ \Phi_G(S) = \frac{E(S,V \setminus S)}{(d/n) |S| |V \setminus S|}. \] 
	\end{fdef}
	The denominator can be thought of as the expected number of edges between $S$ and $S^c$ if the graph is a Erd\H{o}s-R\'{e}nyi random graph with edge probability $d/n$.

	\begin{fdef}
		Given a graph $G = (V,E)$ with $n$ vertices, the \emph{conductance} or \emph{expansion} of $G$ is
		\[ \Phi_G = \min_{\emptyset \ne S \subsetneq V} \Phi_G(S). \]
	\end{fdef}
	Conductance is very closely related to the rate of convergence of the standard random walk on the graph -- a low conductance implies that there is a ``tight bottleneck'' somewhere in the graph where the walk can get stuck. Indeed, if we remove the $d/n$ in the denominator, the conductance of a set is just the probability of exiting the set if we start a random walk at a uniformly random point in it.\\

	The problem of determining the conductance of a graph (and possibly a cut that attains it) is called the uniform sparsest cut problem. It is not too difficult to show that this is \textsf{NP}-hard using a reduction from max-cut. In fact, \cite{cond-appx-hard-ugc} show that the Unique Games Conjecture implies that any constant-factor approximation for $\Phi_G$ is $\mathsf{NP}$-hard! It is quite interesting how min-cut is in \textsf{P}, max-cut is \textsf{NP}-hard but we can get a good constant-factor approximation, but even that is not possible for sparsest-cut (assuming the UGC).

	It is quite easy to see that
	\[ \Phi_G = \min_{x \in \{-1,1\}^n} \frac{f_G(x)}{(d/4n) \sum_{i,j} (x_i - x_j)^2} = \min_{x \in \{-1,1\}^n} \frac{f_G(x)}{(d/n) f_{K_n}(x)}. \]
	Minimizing this directly seems impossible using the sum-of-squares method since we are looking at a rational function that is not a polynomial. So, we instead try to get a ``large'' $\alpha$ such that
	\[ f_G(x) - \alpha \frac{d}{n} f_{K_n}(x) \]
	has a sum-of-squares certificate.
	
	\begin{ftheo}[Cheeger's Inequality]
		There is a degree $2$ sum-of-squares certificate for
		\[ f_G(x) - \frac{\Phi_G^2}{2} \cdot \frac{d}{n} f_{K_n}(x). \]
	\end{ftheo}
	Furthermore, this value of $\alpha$ is tight up to constants for degree $2$ SoS -- with the example used being the cycle again.\\
	We omit the proof of this (standard) inequality; the reader may consult 
	% given a deg 2 pd \mu with \pE_\mu f_G - C \pE_\mu ... \ge 0, can find a set S with expansion \varphi_G(S) = O(\sqrt{C}).

	In [cite Leighton-Rao 88], Leighton-Rao gave an $O(\log n)$-approximation using linear programming. In the setting where $\Phi_G = O(1/\log n)$, this improves on Cheeger's inequality. We shall study the Arora-Rao-Vazirani (ARV) algorithm [cite ARV04], which gives an $O(\sqrt{\log n})$-approximation, based on degree $4$ sum-of-squares. \\
	In \Cref{sec:quad-optim}, our analysis was completely local in the sense that we analyzed what happens to each $(x_i - x_j)^2$ term completely independent of the others. It might not be too ambitious to hope that some sort of global analysis is possible, wherein we show that if one term is small, then others must be large, thus forcing the entire summation to be large. Of course, our remarks before \Cref{prop: gw-reparametr} make this unlikely, at least in the setting of max-cut.\\
	On the other hand, we shall use global analysis in the ARV algorithm. Even before ARV, it has helped in better approximations to vertex cover, graph coloring, max-cut gain etc.

	\begin{ftheo}
		Let $G$ be a $d$-regular graph with $n$ vertices. There is a degree $4$ sum-of-squares certificate for
		\[ f_G(x) - \frac{\varphi_G}{\Theta(\sqrt{\log n})} \cdot \frac{d}{n} f_{K_n}(x). \]
		Furthermore, given any degree $4$ pseudodistribution $\mu$ on $\{-1,1\}^n$, it is possible to find in polynomial time a set $S \subseteq V$ such that
		\[ \varphi_G(S) \le O(\sqrt{\log n}) \cdot \frac{\pE_\mu f_G}{(d/n) \pE_\mu f_{K_n}}. \]
	\end{ftheo}
	\begin{proof}
		Like in prior proofs, we shall show that given any degree $4$ pseudodistribution $\mu$, there exists a distribution $\mu'$ such that
		\[ \frac{\E_{\mu'} f_G}{\E_{\mu'} f_{K_n}} \le O(\sqrt{\log n}) \cdot \frac{\pE_\mu f_G}{\pE_\mu f_{K_n}}. \]
		$D(i,j) = \frac{1}{4} \pE_\mu (x_i - x_j)^2$ is in some sense the ``pseudoprobability'' that $i,j$ are separated by the cut. Because $\mu$ is a degree $2$ pseudodistribution (a degree $4$ one, in fact), this is non-negative. By the squared triangle inequality, we have that $D(i,k) \le D(i,j) + D(j,k)$ for any $i,j,k$ has a degree $4$ sum-of-squares certificate. \\
		% OVERKILL
		Now, given a (nonempty) set $A$ and vertex $i$, define $D(i,A) = \min_{j \in A} D(i,j)$. In the min-cut procedure above, we created the distribution by arranging all the $D(j,1)$ on a line. In fact, the same analysis works if we use $D(j,A)$ instead. To make this meaningful however, I need to ensure that the cut is non-trivial. So, I choose $t \in [0,\max_j D(1,j)]$ instead of $t \in [0,1]$.\\
		Getting back to approximating expansion, we have a distribution $\mu'$ such that $\E_{\mu'} f_G \le \pE_{\mu} f_G$, but we would also like to show that $\E_{\mu'} f_{K_n} \ge C \cdot \pE_\mu f_{K_n}$. Denoting by $\indic(\hat{x})$ the set of vertices where $\hat{x} = 1$, we have that $\E_{\mu'} f_{K_n} = |\indic(\hat{x})|(n - |\indic(\hat{x})|)$.

		% large separated sets: A, B are large \Delta-separated if for all i \in A, j \in B, D(i,j) \ge \Delta and |A|,|B| = \Omega(n).
		% If we round according to D(j,A), then \sum_{i,j} \E (x_i - x_j)^2 \ge \Delta |B||A| = \Omega(\Delta n^2). Therefore, \pE_\mu f_G(x) \ge \Omega(\Delta n^2), so we can round with approximation ratio 1/\Theta(\Delta).
	\end{proof}

	\begin{ftheo}[Global Structure Theorem]
		\label{arv-glob-struct}
		Let $G$ be a $d$-regular graph and $\mu$ a degree $4$ pseudodistribution. Suppose that
		\[ \frac{1}{n^2} \sum_{i,j} D(i,j) \ge 0.1. \]
		Then, there exist $A,B$ that can be found in polynomial time that are $\Omega(1/\sqrt{\log n})$-separated.
	\end{ftheo}
	Let us first give a proof of the Leighton-Rao $O(\log n)$-approximation algorithm, which proves the above for $\Omega(1/\log n)$-separation instead.
	\begin{proof}[Proof of weaker \nameref{arv-glob-struct}]
		Like before, assume wlog that $\pE_\mu x = 0$, and let $g \sim \mathcal{N}(0,\pE_\mu xx^\top)$. Let $A^{(0)} = \{i : g_i \le -1\}$ and $B^{(0)} = \{i : g_i \ge 1\}$. Because $\E g_i^2 = 1$ and $\E g_i = 0$, we expect these two sets to have $\Omega(n)$ size separately. However, this is not enough, because we might have that $B^{(0)}$ is small when we condition on $A^{(0)}$ being large. So, we shall show that $\E |A^{(0)}||B^{(0)}| \ge \Omega(n^2)$. To do so, we can pretty easily show that $\Pr[g_i \le -1 \text{ and } g_j \ge 1] \ge C D(i,j)$ for some constant $C$ using arguments from earlier sections. Since $\sum_{i,j} D(i,j) = \Omega(n^2)$, we can use linearity of expectation to conclude that $\E |A^{(0)}||B^{(0)}| \ge \Omega(n^2)$.\\
		Note that $A^{(0)}$ and $B^{(0)}$ are random variables depending on the draw of the Gaussian. While we have that for $i \in A^{(0)}, j \in B^{(0)}$, $g_j - g_i \ge 2$, we would like some fixed non-random sets $A,B$ such that for any $i \in A, j \in B$, $\E (g_i - g_j)^2 \ge \Delta$. We would like to show that if some coordinates appear in $A^{(0)},B^{(0)}$, then with good probability they are ``good'' pairs in the sense of also being $\Delta$-separated (which is a statement about the expectation, not a single draw). If we have for some pair $i,j$ that $\E (g_i - g_j)^2 \le \Delta$, then standard concentration inequalities show that $\Pr[g_j - g_i \ge 2] \le e^{-\Omega(1/\Delta)}$. However, this is still for a \emph{single} pair $i,j$, but we need to somehow deal with $\Omega(n^2)$ such pairs simultaneously; when $\Delta = O(1/\log n)$, we can use a union bound to do so.
	\end{proof}

	The original Leighton-Rao proof was based on linear programming. Consider the graph $H$ with vertex set $[n]$ and edges between pairs $ij$ iff $D(i,j) \le \Delta$. Our goal is to find $\Omega(n)$ sets $A,B$ such that there are no edges between $A$ and $B$ in $H$. Such pairs are typically called \emph{vertex separators}.\\
	Now, $A^{(0)}$ and $B^{(0)}$ are still not necessarily vertex separators, since some ``bad'' pairs might have crept in. So, greedily\footnote{non-randomly fix a vertex order beforehand to do this.} find a maximal matching $M$ in $E(H) \cap (A^{(0)} \times B^{(0)})$, and then set $A = A^{(0)} \setminus V(M)$ and $B = B^{(0)} \setminus V(M)$. Note again that $M$ is a random variable comprised of directed edges. If $i,j \in M$, then $g_j - g_i \ge 2$ but $\E (g_j - g_i)^2 \le \Delta$. Also note that $M(g)$ has all the edges in $M(-g)$ but reversed. Therefore, the probability that a vertex has an incoming edge in $M$ is the same as the probability that a vertex has an outgoing edge in the perfect matching.\\
	Next, we claim that if $|A||B| = o(n^2)$, then $\E |M| \ge \Omega(n)$. We already saw that for each $(i,j) \in M$, because $\E (g_i - g_j)^2 \le \Delta$ and $g_j - g_i \ge 2$, the matching is expected to be small generally. Furthermore, one might think that we can blame the smallness of $|A||B|$ on a single outlier vertex whose Gaussian value is too large, but the fact that we are using a matching rules this out. To prove the claim, we shall use the right inequality in the following.
	\[ O(\sqrt{\log n}) \ge \E \max_{i,j \in [n]} (g_j - g_i) \ge \frac{\Omega(1)}{\Delta} \cdot \left( \frac{\E |M|}{n} \right)^3. \]
	The left-hand-side is proved rather simply, using the fact that if $(Z_1,Z_2,\ldots,Z_t)$ are jointly Gaussian, then
	\[ \Var \max_{i \le t} Z_i \le O(1) \max_{i \le t} \Var Z_i. \]

	Let $H^k(i)$ be the vertices within distance $k$ of $i$ in the graph $H$, and $\gamma_i^k = \max_{j \in H^k(i)} g_j - g_i$.


	% cheeger's inequality:
	% the thing above has a deg 2 SoS for C = 1/2 \varphi^2_G. also tight up to constants for deg 2 (cycle example again)
	% when phi_G large, this is pretty good -- called expander graph. it becomes worse the smaller o(1) phi_G is.
	% Leighton-Rao 88 gave a LP O(log(n)) approximation. improves on Cheeger when phi = o(1/log n)
	
	% Arora-Rao-Vazirani 04 (ARV) gave O(sqrt{log n}) approximation. based on deg 4 SoS, which is what we shall do.
	% in earlier secs, we did some sort of local analysis where we just comapred \pE_\mu (x_i - x_j)^2 and \E_{\mu'} (x_i - x_j)^2. here, we shall do global analysis instead. this sort of global analysis is a pretty incredible idea, and before ARV (? unsure) it helped in vertex cover, coloring graphs, max-cut gain etc

	% first, let us give a local analysis which gives Leighton's O(log n).
