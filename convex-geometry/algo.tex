\section{Computing Volume in High Dimensions}

A very popular problem in high-dimensional convex geometry is that of determining the volume of an arbitrary body.\\

For a fixed dimension $n$, this problem isn't too difficult if we want to measure it up to some precision $\varepsilon$. We could assume that the body $K$ is enclosed in a box $B=\bigtimes_{i\leq n}[a_i,b_i]$, subdivide this box up to precision $\varepsilon$, and count how many subdivided boxes have non-empty intersection with $K$. This (after normalizing appropriately) can be considered efficient in a fixed dimension, where polynomiality is measure in $\frac{1}{\varepsilon}$. If we measure it in $n$ on the other hand, this method is useless.\\
We are looking for algorithms that are efficient (polynomial) in the \textit{dimension} $n$.\\

There are a few issues that arise when we even want to formulate this problem.
\begin{itemize}
	\item What does it mean when we say that a convex body $K$ is ``given"? In what form is it given?
	\item What does ``efficient" exactly mean?
\end{itemize}

We have already answered the second question above -- we are looking for algorithms that are \textit{polynomial} in the dimension $n$. We either want the exact volume or an approximation up to some small \textit{relative error} $\varepsilon>0$. If it is the latter, we would also like the algorithm to be polynomial in $\frac{1}{\varepsilon}$.\footnote{Note that $\varepsilon$ takes only $\log(1/\varepsilon)$ bits, so this is a relaxation in some sense.}

\subsection{Sandwiching and Deterministic Algorithms}
\label{subsec: sandwiching and deterministic algorithms}

Let us answer the first question that we mentioned -- how is an arbitrary body $K$ represented? What information do we have access to?

\subsubsection{Oracles}

We represent the body using an \textit{oracle}. We explain the different types of oracles one may consider over the course of this section.

\begin{definition}
A body $K$ is said to be \textit{well-guaranteed} if it contains $r B_2^n$ and is contained in $R B_2^n$ for some $r,R>0$.
\end{definition}

We restrict ourselves to well-guaranteed bodies since otherwise, we may ask any (finite) number of questions about the body (say of the form ``is $x\in K$") and receive a \texttt{no} every time. This doesn't allow us to make any useful inferences about $\vol(K)$. The fact that $K$ contains $r B_2^n$ ensures that it isn't too small and the containment in $R B_2^n$ ensures that it isn't ``at infinity".\\

A body $K$ is given by an \textit{oracle} $K$ if we know nothing about it other than the fact that it is well-guaranteed with $r$ and $R$ and we may ask questions about $K$, and receive answers to said questions. Depending on the questions and answers, we get different types of oracles.\\
We primarily use \textit{weak separation oracles} and \textit{strong membership oracles}.

\begin{definition}[Strong Membership Oracle]
For a fixed convex body $K$, the \textit{strong membership oracle} (correctly) answers questions of the form ``Is $x\in K$?".
\end{definition}

Now, for $x\not\in K$, we know that there is a hyperplane separating them by \Cref{hyperplane separation theorem}. This gives rise to the strong separation oracle.

\begin{definition}[Strong Separation Oracle]
For a fixed convex body $K$, the \textit{strong separation oracle} (correctly) answers questions of the form ``Is $x\in K$?". If the answer is \texttt{no}, it also returns a hyperplane $S$ separating $x$ from $K$.
\end{definition}
This hyperplane is returned as a vector $s\in\R^n$ with $\norm{s}_\infty=1$ such that $\langle s,x\rangle > 1$ and $\langle s,y\rangle\leq 1$ for any $y\in K$.\\
This leads to the weak separation oracle.

\begin{definition}[Weak Separation Oracle]
For a fixed convex body $K$, we can fix an $\varepsilon>0$ and ask the \textit{weak separation oracle} questions of the form ``Is $x\in K$ for the positive number $\varepsilon$?". However, in this case, the precision of the answer is $\varepsilon$ in the sense that
\begin{enumerate}[(i)]
	\item If $d(x,\partial K)<\varepsilon$, we can get any answer.
	\item If $B(x,\varepsilon)\subseteq K$, we get the correct answer (\texttt{yes}).
	\item If $d(x,K)\geq\varepsilon$, we get the correct answer (\texttt{no}) and a vector $s$ normalized by $\norm{s}_\infty=1$ for which $\langle s,y\rangle < \langle s,x\rangle+\varepsilon$ for every $y\in K$.
\end{enumerate}
\end{definition}

A \textit{weak membership oracle} is similar, where if it is within $\varepsilon$ of $\partial K$, it may return either answer and if it is farther than $\varepsilon$, it returns the correct answer.\\

The complexity of the algorithm is measured in the number of calls to the oracle, since this is usually the most expensive step.

\subsubsection{Sandwiching}

We earlier mentioned that we only consider well-guaranteed bodies. As might be expected, the ratio $R/r$ is quite important. Defining it slightly more concretely,

\begin{definition}
Given a convex body $K$, let $\mathcal{E}$ an ellipsoid centered at $0$ such that $\mathcal{E}\subseteq K\subseteq d\mathcal{E}$ for some $d\geq 1$. We are then said to have a \textit{sandwiching} of $K$ with \textit{sandwiching ratio} $d$.
\end{definition}

We are given a sandwiching of sandwiching ratio $R/r$ initially. It is natural to want to obtain a sandwiching that has a lower ratio to make whatever algorithm we use more efficient.\\
Further, note that by \Cref{fritz john banach mazur distance}, the minimum possible sandwiching ratio of (an affine transformation of) a body is at most $n$.\\

The information given to us initially ($r$ and $R$) are not even necessarily useful all the time. For example, one could have a very ``pencil-like" body in $\R^n$ such that the inscribed ball is far far smaller than the circumscribed one. Thus, before we even begin our algorithm, we would want to do some preliminary sandwiching -- perform an affine transformation to get a sandwiching with a more manageable sandwiching ratio.\\

Lov\'asz showed in \cite{Lovsz1986AlgorithmicTO} that it is possible to compute an affine transformation $\tilde{K}$ of $K$ in polynomial time such that
\begin{equation}
\label{eqn weak lowner john ellipsoid}
	B_2^n\subseteq \tilde{K} \subseteq (n+1)\sqrt{n} B_2^n.
\end{equation}

We first introduce the following common tool.

\begin{lemma}[Basic ellipsoid method]
For a convex body $K\subseteq\R^n$ along with some $R>0$ such that $K\subseteq RB_2^n$ and a weak separation oracle, it is possible to find a point in $K$ in polynomial time.
\end{lemma}
We prove it for the case where we have a \textit{strong} separation oracle. The algorithm basically works by cutting down our search space until we find a point.
\begin{proof}
We construct a sequence $\mathcal{E}_0,\ldots,\mathcal{E}_k$ of ellipsoids with $\mathcal{E}_0 = R B_2^n$. Given $\mathcal{E}_r$, check if its center $x_r$ is contained in $K$. Otherwise, we have a half-space $H_r$ such that $K\subseteq H_r$. We set $\mathcal{E}_{r+1}$ to be the ellipsoid of minimal volume that contains $K\cap H_r$. The sequence terminates when the center of an ellipsoid is contained in $K$.

It may be shown\footnote{We explicitly give the update formula without proof on the next page. See \href{https://web.stanford.edu/class/ee364b/lectures/ellipsoid_method_notes.pdf}{this}.} that
\[ \vol(\mathcal{E}_{r+1}) = \left(\frac{n}{n+1}\right)^{(n+1)/2}\left(\frac{n}{n-1}\right)^{(n-1)/2} \vol(\mathcal{E}_{r}). \]
Rewriting it more suggestively,
\begin{align*}
	\vol(\mathcal{E}_{r+1}) &= \left(\frac{n^2}{n^2-1}\right)^{(n-1)/2} \frac{n}{n+1} \vol(\mathcal{E}_{r}) \\
	&< \left(1+\frac{1}{n^2}\right)^{(n-1)/2} \vol(\mathcal{E}_r) < e^{-1/2n} \vol(\mathcal{E}_r).
\end{align*}
The thing to note here is that $e^{-1/2n}$ is independent of the ellipsoids involved. Since we have $K\subseteq \mathcal{E}_k$,
\[ \vol (K)\leq \vol(\mathcal{E}_k) \leq e^{-k/2n}(2R)^n.  \]
That is,
\[ k \leq 2n^2 \log(2R) - 2n \log(\vol (K)) \]
so there is a polynomial upper bound on the number of steps.
\end{proof}

If each ellipsoid is given by
\[ \mathcal{E}_r = \left\{x\in\R^n : (x-x_k)^\top A_k^{-1} (x-x_k) \leq 1 \right\}, \]
$c_k$ is the vector returned by the separation oracle and $g_k = \frac{1}{\sqrt{c_k^\top A_k c_k}} c_k$, then
\begin{align*}
	x_{k+1} &= x_k - \frac{1}{n+1}A_k g_k\text{ and } \\
	A_{k+1} &= \frac{n^2}{n^2-1} \left( A_k - \frac{2}{n+1}A_k g_k g_k^\top A_k. \right)
\end{align*}
Since there is rounding anyway (irrationals might become involved due to the $\sqrt{\cdot}$), it turns out that it suffices to have a weak separation oracle.\\

A pair of ellipsoids like that in \Cref{eqn weak lowner john ellipsoid} is often known as a \textit{weak L\"owner-John pair} for $K$ (the sandwiching ratio must be $(n+1)\sqrt n$). 

\begin{ftheo}
\label{lovasz pre-sandwich}
Let $K\subseteq\R^n$ be a convex body given by a weak separation oracle. Then a weak L\"owner-John pair for $K$ can be computed in polynomial time. 
\end{ftheo}
Again, we prove it for the case where we have a strong separation oracle instead. This algorithm is nearly identical to that of basic ellipsoid method, but at each step we perform a little extra computation to check if the corresponding ellipsoid scaled down by a factor of $(n+1)\sqrt{n}$ is contained in $K$.
\begin{proof}
We construct a sequence $\mathcal{E}_0,\ldots,\mathcal{E}_k$ of ellipsoids with $\mathcal{E}_0=R B_2^n$. Given $\mathcal{E}_r$, first check if its center $x_r$ is contained in $K$. if it is not, then use the basic ellipsoid method to get an ellipsoid that does; we abuse notation and refer to this as $\mathcal{E}_r$ as well.\\
Next, let the endpoints of the axes of the ellipsoid be given by $x_r \pm a_i$ (for $1\leq i\leq n$). Check if the $2n$ points $x_r \pm \frac{1}{n+1} a_i$ are in $K$ for each $i$. If they all are, then we are done, since this implies that the convex hull of these points is contained in $K$ as well, and the maximal ellipsoid contained in the convex hull is just $\mathcal{E}_r$ scaled down by a factor of $(n+1)\sqrt{n}$.\\
Otherwise, suppose that $x_r+\frac{1}{n+1}a_1$ is not in $K$ and $H_r$ is the half-space returned by the oracle that contains $K$. Similar to the basic ellipsoid method, find the minimal ellipsoid $\mathcal{E}_{r+1}$ that contains $\mathcal{E}_r\cap H_r$.\\
The sequence terminates when we have found a weak L\"owner-John pair.
\end{proof}

% It may be shown that this sandwiching algorithm can be run in $\tilde{\mathcal{O}}(n^4)$.\footnote{the $\tilde{O}$ notation means that we suppress powers of $\log n$.}\\

One might be tempted to increase the $\frac{1}{n+1}$ factor we use to get something even better, but it is worth noting the reason for both this algorithm working in the first place is that the volume of the ellipsoid decreases at each step.\\

It is also notable that for certain types of special convex bodies, we can improve the bound beyond $(n+1)\sqrt{n}$.\\
In particular, if $K$ is symmetric, we can attain a factor of $n$, if $K$ is a polytope given as the convex hull of a set of vectors, we can attain $n+1$, and if $K$ is a symmetric polytope given as above, we can attain $\sqrt{n+1}$.\\

Typically, we assume that after performing sandwiching, we perform a linear transformation such that $B_2^n$ becomes the maximal ellipsoid of the transformed body. That is, the problem boils down computing the volume of a body $K$ with
\[ B_2^n \subseteq K \subseteq (n+1)\sqrt{n} B_2^n. \]

\subsubsection{The Problem and Deterministic Attempts}

Our problem is to find for some given convex body $K$, some quantities $\uvol(K)$ and $\ovol(K)$ such that
\[ \uvol(K) \leq \vol(K) \leq \ovol(K) \]
while minimizing $\frac{\ovol(K)}{\uvol(K)}$.

\Cref{lovasz pre-sandwich} produces estimates (equal to the volumes of the ellipsoids) with $\frac{\ovol(K)}{\uvol(K)} = n^n (n+1)^{n/2}$. This may seem ludicrously bad, but as it turns out, any deterministic attempts in general are destined to fail. Indeed, Elekes proved in \cite{Elekes1986} that for any positive $\varepsilon<2$, there exists no deterministic polynomial time algorithm that returns
\begin{equation}
\label{eqn elekes deterministic SUCKS}
	\frac{\ovol(K)}{\uvol(K)} \leq (2-\varepsilon)^n
\end{equation}
for every convex body $K$. The reason for this is that the convex hull of polynomially many points in $B_2^n$ is always bound to be far smaller than $B_2^n$ itself -- we've already seen this all the way back in \Cref{approximating sphere to polytope}. Let us now prove \Cref{eqn elekes deterministic SUCKS}.

\begin{lemma}[Elekes' Theorem]
Every deterministic algorithm to estimate the volume of an arbitrary convex body $K\subseteq\R^n$ that uses $q$ oracle queries has $\frac{\ovol(K)}{\uvol(K)} \geq \frac{2^n}{q}$ for some $K$ given by a well-guaranteed weak separation oracle.
\end{lemma}

What exactly do we mean by a deterministic algorithm? Roughly, it means that if we pass the same body into the algorithm twice, we will get the exact same result. More specifically, if we pass two bodies $K_1$ and $K_2$ such that $(x_i)$ and $(y_i)$ are the queried points respectively, the first point where they differ, say $x_i\neq y_i$, must be such that $x_{i-1}=y_{i-1}$ is in $K_1\triangle K_2$. We abuse this fact.

\begin{proof}
Let $\mathcal{A}$ be some deterministic algorithm to estimate the volume of a convex body. Fix $\varepsilon=2^n$ for the separation oracle. When we run $\mathcal{A}$ on $B_2^n$, suppose that the points queried are $x_1,\ldots,x_q$. Let $C$ be the convex hull of these $q$ points. Now, note that if we run $\mathcal{A}$ on $C$, the same points $(x_i)$ will be queried and as a result, the volume estimates $\uvol$ and $\ovol$ that are returned are the same as well!\\
To conclude the argument, note that
\[ C \subseteq \bigcup_{i=1}^q B\left(\frac{x_i}{2},\frac{1}{2}\right). \]
Therefore,
\[ \frac{\vol(C)}{\vol(B_2^n)} \leq \frac{q}{2^n}. \]
We then have
\[ \frac{\ovol}{\uvol} \geq \frac{\vol(B_2^n)}{\vol(C)} \geq \frac{2^n}{q}. \]
\end{proof}

\subsubsection{The B\'ar\'any-F\"uredi Theorem}

We now give a stronger result known as the B\'ar\'any-F\"uredi Theorem (given in \cite{no-deterministic-algo-barany-furedi}), which shows that deterministic algorithms in general aren't much better than even the estimate with an $n^n$ error returned by basic sandwiching.

\begin{ftheo}[B\'ar\'any-F\"uredi Theorem]
There is no deterministic polynomial time algorithm that computes a lower bound $\uvol(K)$ and an upper bound $\ovol(K)$ for the volume of every convex body $K\subseteq\R^n$ given by some oracle such that for every convex body,
\[ \frac{\ovol(K)}{\uvol(K)} \leq \left(c \frac{n}{\log n}\right)^n \]
\end{ftheo}

The basic outline of the proof is as follows.\\

\begin{proof}
Rather than considering a simple separation oracle, we consider an even stronger oracle. First of all, we know beforehand that the convex body $K$ is such that $B_1^n\subseteq K\subseteq B_\infty^n$.\\
For $x\in\R^n$, denote $x^\circ = x/\norm{x}$, $H^+(x^\circ) = \{ z\in\R^d:\langle z,x^\circ \rangle \leq 1 \}$ and $H^-(x^\circ) = \{ z\in\R^d:\langle z,x^\circ \rangle \geq 1 \}$.\\

When we query $x\in\R^n$, in addition to the information given by the separation oracle, we also receive ``$x^\circ\in K$ and $-x^\circ\in K$ and $K\subseteq H^-(x^\circ)$ and $K\subseteq H^+(x^\circ)$". That is, if $x\not\in K$, in addition to a separating hyperplane, we also receive information as to whether the hyperplanes at $\pm x^\circ$ that are orthogonal to $x$ are tangential to $K$.\\

Now, for the main part of the proof, suppose we have some deterministic polyomial time algorithm $\mathcal{A}$ that returns a lower and upper bound $\uvol(K)$ and $\ovol(K)$ for any body $K$. The basic idea is roughly similar to that of Elekes' Theorem. Suppose we run $\mathcal{A}$ on $B_2^n$ until $m\leq n^a - n$ questions have been asked for some $a\geq 2$ (due to the polynomial nature of the algorithm) and $x_1,\ldots,x_m$ are the points queried. Define
\[ C = \conv(\pm e_1, \pm e_2, \ldots, \pm e_n, x_1^\circ, \ldots, x_m^\circ). \]
Now, consider the dual $C^*$ of $C$ (recall what a dual is from the proof of \Cref{fritz johns theorem part 1}). Observe that for any of the $x_i$, the output of the oracle on passing $x_i$ (or $\pm e_i$) must be the same whether we pass it with regards to $C$, $C^*$, or $B_2^n$.\footnote{if $x_i\in B_2^n$, we receive a \texttt{yes}. Otherwise, we receive a \texttt{no} along with the information that the hyperplanes at the $x_i^\circ$ are tangential.} Indeed, each $H^+(x_i^\circ)$ and $H^-(x_i^\circ)$ is a supporting hyperplane of all three bodies. This implies that the estimates returned by $\mathcal{A}$ are the same for all three bodies!\\
We then have
\[ \ovol(B_2^n) \geq \vol(C^*)\text{ and }\uvol(B_2^n) \leq \vol(C). \]
Therefore,
\[ \frac{\ovol(C)}{\uvol(C)} = \frac{\ovol(C^*)}{\uvol(C^*)} = \frac{\ovol(B_2^n)}{\uvol(B_2^n)} \geq \frac{\vol(C^*)}{\vol(C)}. \]
\end{proof}

Over the rest of this section, we show that there is some constant $c$ such that
\[ \frac{\vol(C^*)}{\vol(C)} \geq \left(c \frac{n}{\log n}\right)^n. \]
To do this, we introduce some more notation. Let
\[ V(n,m) = \sup\{\vol(K):K=\conv(\{v_1,\ldots,v_m\})\subseteq B_2^n\} \]
and
\[ S(n,m) = \inf\{\vol(\{x:|\langle x,v_i\rangle| \leq 1\text{ for each $i$}\}) : (v_i)_1^m\in\R^n\text{ such that for each }i, \norm{v_i}\leq 1\} \]

Clearly, it suffices to show that
\begin{equation}
\label{eqn: barany furedy}
	\frac{S(n,n^a)}{V(n,n^a)} \geq \left(c \frac{n}{\log n}\right)^n
\end{equation}
since $C^*$ and $C$ are of the above considered forms.\\

\subsubsection{Bounding \texorpdfstring{$V(n,m)$}{V(nm)} and \texorpdfstring{$S(n,m)$}{S(nm)}}

For $1\leq k\leq n$, define
\[
\rho(n,k) = 
\begin{cases}
1, & \text{if }k=0 \\
\sqrt{(n-k)/nk}, & \text{if }1\leq k\leq n-2 \\
1/n, & \text{if }k=n-1.
\end{cases}
\]

\begin{lemma}
\label{rho span bound}
Let $S=\conv(\{v_0,v_1,\ldots,v_n\})\subseteq B_2^n$ be an $n$-dimensional simplex and $x\in S$. Then for every $k$ such that $0\leq k\leq n-1$, $S$ has a $k$-dimensional face $S_k = \conv(\{v_{i_0},v_{i_1},\ldots,v_{i_k}\})$ and a point $x_k$ in the interior\footnote{it is a convex combination of the $(v_{i_j})$} of $S_k$ such that $(x-x_k) \perp \Span(S_k)$ and $\norm{x-x_k}\leq \rho(n,k)$.
\end{lemma}

\begin{proof}
The result for $k=n-1$ follows directly from the fact that the maximal ellipsoid in $S$ is at most $\frac{1}{n}B_2^n$.\\
For $1\leq k\leq n-2$, we use strong (backward) induction on $k$.\\
Let $x_n=x$ and for each $r:n>k>k$, let $x_r$ be such that $(x_{r+1}-x_r)\perp \Span(S_r)$ and $\norm{x_{r+1}-x_r} \leq \rho(n,r)$.\\
Note that $x_n - x_{n-1}, x_{n-1}-x_{n-2},\ldots,x_{k+1}-x_{k}$ are all orthogonal and $\norm{x_{r+1} - x_r}\leq\frac{1}{r}$ for each $r$. We then have
\begin{align*}
	\norm{x_n - x_k}^2 &= \sum_{r=k}^{n-1} \norm{x_{r+1}-x_r}^2 \\
	&\leq \sum_{r=k+1}^{n} \frac{1}{r^2} \\
	&\leq \sum_{r=k+1}^{n} \frac{1}{r(r-1)} \\
	&= \frac{1}{k} - \frac{1}{n}.
\end{align*}
Finally, the result for $k=0$ follows from the fact that the $(v_i)$ are contained in $B_2^n$.
\end{proof}

Observe that this bound is only tight when $k$ is $1$ or $n$. Putting this in a slightly more compact form, let $S\subseteq\R^n$ and $U=\Span(S)$. If we define
\[ S^\rho = S + \left(U^\perp + \rho B_2^n\right), \]
\Cref{rho span bound} just says that for some $S_k$, $x \in S_k^{\rho(n,k)}$.\\
It is also worth noting that if $S$ is convex and $\dim U = k$,
\begin{equation}
\label{eqn: rho convex body volume}
	\vol(S^\rho) = \vol_k(S) v_{n-k} \rho^{n-k}
\end{equation}

\begin{ftheo}
\label{upper bound on V n m}
There is a constant $c>0$ such that
\[ \frac{V(n,m)}{v_n} \leq \left(c \frac{1+\log(m/n)}{n}\right)^{n/2} \]
and so,
\[ V(n,m) \leq \left(\gamma \frac{\sqrt{1+\log(m/n)}}{n}\right)^{n} \]
where $\gamma=\sqrt{2\pi e c}$.
\end{ftheo}

If $m/n\to\infty$ and $n/\log(m/n)\to\infty$, then there is a constant $c'$ such that
\[ \frac{V(n,m)}{v_n} \leq \left(c' \frac{\log(m/n)}{n}\right)^{n/2} \]

\begin{proof}
It may be shown that if $K=\conv(\{v_1,\ldots,v_m\})\subseteq\R^n$, then $K$ is the union of its $n$-dimensional simplices. That is,
\[ K = \bigcup_{i_0 < \cdots < i_n} \conv(\{v_{i_0},\ldots,v_{i_n}\}). \]
This allows us to bound the volume of $K$. For any $1\leq k\leq n-1$, we can write
\[ K \subseteq \bigcup_{i_0 < \cdots < i_k} \left\{S^{\rho(n,k)} : S=\conv(\{v_{i_0},\ldots,v_{i_k}\})\right\}. \]
Bounding the volume,
\[ \vol(K) \leq \binom{m}{k+1} \max\left\{\vol\left(S^{\rho(n,k)}\right) : S=\conv(\{x_0,\ldots,x_k\})\subseteq B_2^n\right\}. \]
Using \Cref{eqn: rho convex body volume},
\[ \vol(K) \leq \binom{m}{k+1} \cdot v_{n-k} \rho(n,k)^{n-k} \cdot \max\left\{\vol_k(S) : S=\conv(\{x_0,\ldots,x_k\})\subseteq B_2^n\right\}. \]
The right-most quantity is maximum when the body is the $k$-dimensional regular solid simplex, whose volume is $(n+1)^{(n+1)/2}/n^{n/2}n!$. This is easily computed by using induction on dimension and the maximality was briefly mentioned at the end of \Cref{subsection: reverse isoperimetric problem 2.3}. So,
\begin{align*}
	\vol(K) &\leq \binom{m}{k+1} \cdot \frac{\pi^{(n-k)/2}}{\Gamma\left(\frac{n-k}{2}+1\right)} \left(\frac{n-k}{nk}\right)^{(n-k)/2} \frac{(k+1)^{(k+1)/2}}{k^{k/2}k!} \\
	&\leq \binom{m}{k+1} \cdot \left(\frac{2\pi e}{n-k}\right)^{(n-k)/2} \left(\frac{n-k}{nk}\right)^{(n-k)/2} \frac{(k+1)^{(k+1)/2}}{k^{k/2}k!} \\
	&= \binom{m}{k+1} \cdot \left(\frac{2\pi e}{nk}\right)^{(n-k)/2} \frac{(k+1)^{(k+1)/2}}{k^{k/2}k!} \\
	&\leq \frac{m^{k+1}}{(k+1)!} \cdot \left(\frac{2\pi e}{nk}\right)^{(n-k)/2} \frac{(k+1)^{(k+1)/2}}{k^{k/2}k!} \\
	&\leq \left(\frac{em}{k+1}\right)^{k+1} \cdot \left(\frac{2\pi e}{nk}\right)^{(n-k)/2} \left(\frac{e}{k}\right)^k.
\end{align*}
And therefore,
\[ \frac{\vol (K)}{v_n} \leq \left(\frac{em}{k+1}\right)^{k+1} n^{k/2} k^{-(n+k)/2}. \]
It remains to choose a suitable value of $k$. For the case when $m/n\to\infty$ and $n/\log(m/n)\to\infty$, we can let $k = \left\lceil \frac{n}{2\log(m/n)} \right\rceil$ to obtain
\[ \frac{\vol(K)}{v_n} \leq e^{o(n)} \left(\frac{2e\log(m/n)}{n}\right)^{n/2}. \]
\end{proof}

Note that the above again leads to an inference similar to that we made in \Cref{approximating sphere to polytope} -- the volume is comparable only when $m$ is exponentially large.\\

It remains to bound $S(n,m)$.\\

To do this, we use the second of the following beautiful results (we state the first for the sake of completeness).

\begin{theorem}[Blaschke-Santal\'o Inequality]
\label{santalo inequality}
Let $K$ be a convex body in $\R^n$ with dual $K^*$. Then
\[ \vol(K)\vol(K^*) \leq \vol(B_2^n)^2 \]
with equality when $K$ is an ellipsoid.
\end{theorem}

\begin{ftheo}[Inverse Santal\'o Inequality]
\label{inverse santalo inequality}
Let $K$ be a convex body in $\R^n$ with dual $K^*$. Then
\[ \vol(K)\vol(K^*) \geq \frac{4^n}{n!} \]
with equality when $K$ is the regular solid simplex.
\end{ftheo}

For our purposes, it suffices to know that there is some constant $c_2$ such that
\[ \vol(K)\vol(K^*) \geq \left(\frac{c_2}{n}\right)^n. \]
If we let $K=\{x\in\R^n:|\langle x,v_i\rangle| \leq 1\text{ for each $i$}\}$ for some $(v_i)_1^m\in\R^n$ such that $\norm{v_i}\leq 1$ for each $i$, then note that $K^* = \conv(\{v_1,\ldots,v_m\})$.\\
An upper bound then directly follows from \Cref{inverse santalo inequality} and \Cref{upper bound on V n m}. We get for some constants $c_2$ and $\gamma$,
\begin{equation}
\label{eqn: lower bound on S n m}
	S(n,m) \geq \left(\frac{c_2}{n}\right)^n \left(\frac{n}{\gamma\sqrt{ \log(m/n)+1}}\right)^n = \left(\frac{c'}{\sqrt{\log(m/n)+1}}\right)^n
\end{equation}
for some constant $c'$.\\

Finally, to show the bound mentioned in \Cref{eqn: barany furedy}, use \Cref{upper bound on V n m} and \Cref{eqn: lower bound on S n m} to get
\[ \frac{\ovol(C)}{\uvol(C)} \geq \frac{S(n,n^a)}{V(n,n^a)} \geq \left(\frac{c_1}{\gamma a}\frac{n}{\log n}\right)^n \]
which is exactly what we want.

\subsection{Rapidly Mixing Random Walks}

It has now clearly been established beyond doubt that deterministic algorithms will get us nowhere. What if instead, we consider randomized algorithms? That is, we are fine with some small probability, say $\eta$, of getting the incorrect answer? We can do far \textit{far} better in this case.

\subsubsection{An Issue with High Dimensions and the Solution}

Reformulating the problem in this context, we pass some $0<\eta<1$, some $\varepsilon>0$, and a well-guaranteed strong membership oracle\footnote{it is in general not too important which oracle we are given.} of a body $K$, and ask for an estimate $\evol(K)$ such that with probability at least $1-\eta$,
\[ (1-\varepsilon)\evol(K) \leq \vol(K) \leq (1+\varepsilon)\evol(K). \]

Henceforth, we assume that the reader has a basic understanding of Markov chains and stationary distributions thereof, at least in the discrete case. In case the reader does not, they can skip ahead to \Cref{subsec: measure theoretic markov chains}.\\

A simple method that might come to mind is a Monte Carlo algorithm. Find some box $Q$ in which $K$ is contained, uniformly randomly generate a large number of points in $Q$, and find the fraction of points generated that are in $K$ -- this is a good estimate of $\frac{\vol(K)}{\vol(Q)}$.\\
However, the issue is one that we emphasised very heavily on in the very first (and quantified to some extent in the previous) section: if we take $K=B_2^n$ and $Q=B_\infty^n$, then $\vol(K)/\vol(Q)$ is extremely (exponentially) small, so it will not work (in polynomial time) at all.\\

That is, the issue is that $\vol(K)$ is extremely small compared to a box it is contained in. To get around this, there is a surprisingly simple solution.\\

Rather than considering just $K$, consider some $m+1$ bodies $K_0\subseteq K_1\subseteq \cdots \subseteq K_m=K$ (for appropriately large $m$) and for each $i$, estimate $\Lambda_i \coloneqq \vol(K_i)/\vol(K_{i-1})$ (we can then estimate $\vol(K)$ as $\vol(K_0)\prod_i\Lambda_i$).\\
Usually, we take $K_i = K \cap 2^{i/n}B_2^n$. Note that because $K_{i}\subseteq 2^{i/n}K_{i-1}$ in this case,
\[ \Lambda_i = \frac{\vol(K_i)}{\vol(K_{i-1})} \leq 2 \]
is not large at all.\\
The value of $\vol(K_0)$ is already known. But how do we estimate $\Lambda_i$?\\
As observed earlier, since $1/\Lambda_i$ is not small, we can just stick with Monte Carlo methods, the basic idea being to somehow generate a uniform random distribution on $K_i$ and find the fraction of points generated within $K_{i-1}$. Here on out, our main interest is just to figure out a way of efficiently uniformly randomly generating points from $K_i$.\\

To do this, we synthesize a Markov chain whose stationary distribution is the uniform distribution on $K_i$. We run the chain for polynomially many steps, and take the resultant state as a point uniformly randomly generated from $K_i$.\\

Obviously, we want Markov chains that converge to the stationary distribution very rapidly (in polynomial time) since that is the main part of the algorithm that must be made efficient. To restrict ourselves to finding the uniform distribution \textit{within} $K_i$, we skip any move where the random walk attempts to leave $K_i$.\footnote{For the hit-and-run strategy we describe later, this is unimportant since it never even tries to leave the body.} At the same time, we count how often we are in $K_{i-1}\subseteq K_i$.\\

Succinctly, we use ``Multiphase Monte Carlo Markov Chain methods" for volume computation. We call the random walk ``rapidly mixing" if it gets sufficiently close to the stationary distribution in polynomially many steps.\\

Another small change is that we make the random walk \textit{lazy}. That is, if we are at $a\in K$,  we stay at $a$ with probability $\frac{1}{2}$, and with probability $\frac{1}{2}$ we choose a random direction $a+v$. If $a+v\in K$, we move there. Otherwise, we stay put. There are two reasons for doing this. The first is that sometimes parity issues arise due to which the stationary distribution might not be the uniform one. The second is that it turns the matrix describing the random walk into a positive semidefinite matrix, which is much easier to analyze.

\subsubsection{Random Walks on Graphs}

Before we move onto the general case, let us define random walks in graphs and study them for a bit. Let $G=(V,E)$ be a connected $d$-regular simple graph with $V=\{1,\ldots,n\}$. A \textit{simple random walk on $G$ with initial state $X_0$} is given by
\[
\Pr[X_{t+1}=j\mid X_t=i] = 
\begin{cases}
\frac{1}{2}, & i=j, \\
\frac{1}{2d}, & ij\in E, \\
0, & \text{otherwise.}
\end{cases}
\]

It may be shown that irrespective of $X_0$, $\lim_{t\to\infty}\Pr[X_t=i] = \frac{1}{n}$ for any $i$. But to see whether the walk is rapidly mixing, we need to know how fast it converges. Let
\[ e_{i,t} = \Pr[X_t=i] - \frac{1}{n} \]
be the ``excess" probability at time $t$ on vertex $i$. Also, denote $\Pr[X_t=i]$ as $p_i^{(t)}$ for the sake of brevity. Denoting the neighbourhood of $i$ by $\Gamma(i)$,
\begin{align}
	e_{i,t+1} &= p_i^{(t+1)} - \frac{1}{n} \nonumber \\
	&= \left(\frac{1}{2}p_i^{(t)} + \frac{1}{2d}\sum_{j\in\Gamma(i)}p_j^{(t)}\right) - \frac{1}{n} \nonumber \\
	&= \frac{1}{2}e_{i,t} + \frac{1}{2d}\sum_{j\in\Gamma(i)}e_{j,t} \nonumber \\
	&= \frac{1}{2d} \sum_{j\in\Gamma(i)}(e_{i,t}+e_{j,t}) \label{eqn: alternate expression for future error}
\end{align}
To be able to quantify our closeness to the stationary distribution, define
\[d_1(t)=d_1(\tilde{X},t) = \sum_{i}|e_{i,t}|\]
and
\[d_2(t)=d_2(\tilde{X},t) = \sum_{i}e_{i,t}^2.\]

We call a walk $\tilde{X}$ on $G$ \textit{rapidly mixing} if there exists a polynomial $f$ such that for any $0<\varepsilon<\frac{1}{3}$ and $t\geq f(\log n)\log(1/\varepsilon)$, $d_1(t)\leq\varepsilon$. However, this doesn't completely make sense right now since if we only have a single graph, $n$ is constant.

\begin{fdef}[Rapidly Mixing Random Walks]
\label{def: rapidly mixing random walks}
	Let $(G_i)_{i\in\N}$ be a sequence of graphs where $G_i$ has $n_i$ vertices and $n_i\to\infty$. We say that the simple random walks on $G_1,G_2,\ldots$ are \textit{randomly mixing} if there is a polynomial $f$ (depending only on the sequence $(G_i)$) such that if $0<\varepsilon<\frac{1}{3}$ and $t\geq f(\log n_i)\log(1/\varepsilon)$, then $d(\tilde{X_i},t)\leq\varepsilon$ whenever $\tilde{X_i}$ is a simple random walk on $G_i$.
\end{fdef}

There are some issues that arise when we want to synthesize a rapidly mixing walk. For example, suppose we have a random walk on $[-1,1]^n$ and we somehow find ourselves near one of the corners. Then the probability of leaving the corner is extremely low (of the order of $2^{-n}$) at each step, which would greatly hinder the speed of convergence.

\subsubsection{Conductance and Bounding the Speed of Convergence}

In a graph, the analogous event is that we get stuck within some subset of vertices that is highly connected within itself, but not very well-connected to its complement. With this in mind, let us define the conductance of a graph. Let $G=(V,E)$ be a graph and $U\subseteq V$ be non-empty. Then define
\[ \Phi_G(U) = \frac{e(U,V\setminus U)}{d|U|} \]
where $e(U,V\setminus U)$ is the number of edges between $U$ and $V$.\\
$\Phi_G(U)$ gives a measure of the ``difficulty" we mentioned earlier. The lower it is, the more difficult it is to leave $U$. We might as well consider only sets $U$ with $|U|\leq\frac{n}{2}$. Thus, we define the \textit{conductance} of $G$ by
\[ \Phi_G = \min_{1\leq|U|\leq n/2}\Phi_G(U). \]
In graph theoretic contexts, this quantity is more often known as the \textit{Cheeger's constant} of a graph or its \textit{isoperimetric number}.\\

For graphs in general, denote $\vol(U)=\sum_{u\in U}d(u)$. Then its conductance is
\[ \Phi_G = \min_{\substack{ U\subsetneq V \\ U\neq\emptyset}} \frac{e(U,V\setminus U)}{\min(\vol(U),\vol(V\setminus U))}. \]
Obviously, $0\leq\Phi_G\leq 1$ for any graph $G$. The upper bound is only attained when $G$ is the graph containing a single vertex, a single edge, or a triangle. The lower bound is attained only when $G$ is disconnected. If $G$ is large, then the best we can hope for is that $\Phi_G$ is not too much lower than $\frac{1}{2}$.\\

It is intuitively clear that if a graph has high conductance, then any simple walk will converge quite rapidly. This is stated quantitatively in the following.

\begin{ftheo}
\label{random walk speed of convergence conductance}
Every simple random walk on a connected $d$-regular graph $G$ satisfies
\[ d_2(t+1) \leq \left(1-\frac{1}{4}\Phi_G^2\right) d_2(t). \]
In particular,
\[ d_2(t) \leq \left(1-\frac{1}{4}\Phi_G^2\right)^t d_2(0) \leq 2\left(1-\frac{1}{4}\Phi_G^2\right)^t. \]
\end{ftheo}

We prove this using two other lemmas.

\begin{lemma}
\label{random walk speed of convergence conductance lemma 1}
	For any simple random walk on a connected $d$-regular simple graph on $G$,
\[ d_2(t+1) \leq d_2(t) - \frac{1}{2d}\sum_{ij\in E}(e_{i,t}-e_{j,t})^2. \]
\end{lemma}
\begin{proof}
Using \Cref{eqn: alternate expression for future error} along with the Cauchy-Schwarz inequality,
\begin{align*}
	d_2(t+1) &= \frac{1}{4d^2} \sum_{i=1}^n \left( \sum_{j\in\Gamma(i)} e_{i,t}+e_{j,t}\right)^2 \\
	&\leq \frac{1}{4d} \sum_{i=1}^n \sum_{j\in\Gamma(i)} (e_{i,t}+e_{j,t})^2 \\
	&= \frac{1}{2d} \sum_{ij\in E} (e_{i,t}+e_{j,t})^2 \\
	&= \frac{1}{d} \sum_{ij\in E} (e_{i,t}^2 + e_{j,t}^2) - \frac{1}{2d} \sum_{ij\in E}(e_{i,t}-e_{j,t})^2 \\
	&= d_2(t) - \frac{1}{2d} \sum_{ij\in E} (e_{i,t}-e_{j,t})^2
\end{align*}
\end{proof}

\begin{lemma}
Suppose weights $x_i$ are assigned to the elements of the vertex set $V=[n]$ satisfying $\sum_i x_i = 0$. Then
\[ \sum_{ij\in E} (x_i-x_j)^2 \geq \frac{d}{2}\Phi_G^2 \sum_{i=1}^n x_i^2. \]
\end{lemma}

Observe that setting $x_i=e_{i,t}$ for each $i$ and substituting the above in \Cref{random walk speed of convergence conductance lemma 1} directly gives \Cref{random walk speed of convergence conductance}.

\begin{proof}
We may assume without loss of generality that $x_1\geq x_2\geq \cdots\geq x_n$. Fix $m=\lceil n/2\rceil$ and for each $i$, let $y_i = x_i-x_m$. Note that it suffices (and is in fact stronger) to prove the inequality for the $(y_i)$ instead of the $(x_i)$ since
\[ \frac{d}{2}\Phi_G^2\sum_{i=1}^n (x_i-x_m)^2 = \frac{d}{2}\Phi_G^2\sum_{i=1}^n x_i^2 + \frac{nd}{2}\Phi_G^2 x_m^2. \]
Also, let
\[
u_i =
\begin{cases}
y_i, & i\leq m, \\
0, & \text{otherwise},
\end{cases}
v_i =
\begin{cases}
0, & i\leq m, \\
y_i, & \text{otherwise}.
\end{cases}
\]
Obviously, it suffices to prove the inequality for the $(u_i)$ and $(v_i)$ since \[(y_i-y_j)^2 = (u_i-u_j + v_i-v_j)^2\geq (u_i-u_j)^2 + (v_i-v_j)^2\]
and $\sum_i x_i^2 = \sum_i u_i^2 + \sum_i v_i^2$.\\
We prove it only for the $(u_i)$. Using the Cauchy-Schwarz inequality,
\begin{align}
	2d\sum_{i=1}^n u_i^2 \sum_{ij\in E} (u_i-u_j)^2 &= \sum_{ij\in E} 2(u_i^2 + u_j^2) \sum_{ij\in E}(u_i-u_j)^2 \nonumber \\
	&\geq \sum_{ij\in E} (u_i + u_j)^2 \sum_{ij\in E}(u_i-u_j)^2 \nonumber \\
	&\geq \left(\sum_{ij\in E} (u_i^2 - u_j^2)\right)^2 \label{eqn: random walk speed of convergence conductance lemma 2 cauchy schwarz}
\end{align}
We aim now to bound the term within the square in the final expression.\\
Suppose that in every edge $ij\in E$, $i<j$. We can then rewrite the expression as
\[ \sum_{ij\in E} (u_i^2-u_j^2) = \sum_{ij\in E} \sum_{l=i}^{j-1} (u_l^2-u_{l+1}^2) = \sum_{l=1}^{n} (u_l^2 - u_{l+1}^2) e\left([l],[n]\setminus[l]\right). \]
It is very clear now how the conductance enters the picture. Since we can disregard the terms of the summation after $l=m$, the expression on the right is bounded below by $dl\Phi_G$. That is,
\begin{align*}
	\sum_{ij\in E} (u_i^2-u_j^2) &\geq \sum_{l=1}^m (u_l^2-u_{l+1}^2) dl\Phi_G \\
	&= d\Phi_G\sum_{l=1}^m u_l^2.
\end{align*}
Substituting the above in \Cref{eqn: random walk speed of convergence conductance lemma 2 cauchy schwarz},
\[ \sum_{ij\in E}(u_i-u_j)^2 \geq \frac{d}{2}\Phi_G^2 \sum_{l=1}^n u_l^2, \]
which is exactly what we want to show.
\end{proof}

Since $d_1(t)^2 \leq n d_2(t)$, we have the following corollary of \Cref{random walk speed of convergence conductance}.

\begin{corollary}
Every simple random walk on a connected $d$-regular graph $G$ satisfies
\[ d_1(t) \leq (2n)^{1/2} \left(1-\frac{1}{4}\Phi_G^2\right)^{t/2} \]
\end{corollary}

Note that if $G$ is connected (so $\Phi_G\neq 0$), then for
\begin{equation}
\label{eqn: condition on time in terms of conductance}
t > 8\Phi_G^{-2}\left(\log(2n)+\log(1/\varepsilon)\right) > \frac{2}{-\log\left(1-\frac{1}{4}\Phi_G^2\right)} \left(\log(2n)+\log(1/\varepsilon)\right),
\end{equation}
$d_1(t)<\varepsilon$. Thus, we have the following sufficient condition for rapid mixing.

\begin{lemma}
Let $(G_i)_{i\in\N}$ be a sequence of regular graphs with $|G_i|=n_i\to\infty$. If there exists $k\in\N$ such that
\[ \Phi_{G_i} \geq (\log n_i)^{-k} \]
for sufficiently large $i$, then the simple random walks on $(G_i)$ are rapidly mixing.
\end{lemma}

The entirety of the discussion thus far has been regarding simple random walks. How would one go about generalizing this to aperiodic reversible random walks on finite sets in general?

\begin{fdef}
Let $V$ be a finite set and $X$ a random walk on $V$ with transition probabilities $p(u,v)$ such that for each $u$, $p(u,u)\geq \frac{1}{2}$. Let $\lambda$ be the (reversible) stationary distribution that satisfies $\lambda(u)p(u,v)=\lambda(v)p(v,u)$. Also, for $U\subseteq V$, write $\lambda(U)=\sum_{u\in U}\lambda(u)$. The \textit{conductance} of $X$ is then
\[ \tilde{\Phi}_X = \min_{\lambda(U)\leq\frac{1}{2}} \frac{\sum_{u\in U}\sum_{v\in V\setminus U} \lambda(u)p(u,v)}{\lambda(U)}. \]
\end{fdef}

Similar to earlier, the lower the conductance, the higher the probability of getting ``stuck" somewhere. Note that the conductance here is half as large as the definition we gave for regular graphs (since in this case, $p(u,v)=1/2d$ replaces the $1/d$ earlier). That is, if $X$ is a simple random walk on a regular graph,
\[ \tilde{\Phi}_X = \frac{1}{2}\Phi_G. \]
As earlier, we can measure the distance from $\lambda$ by
\[ d_2(t) = \sum_{v\in V} \left(p_v^{(t)}-\lambda(v)\right)^2. \]
We can then prove the following analogue of \Cref{random walk speed of convergence conductance} (in exactly the same way).

\begin{ftheo}
\label{large conductance implies rapidly mixing}
Let $X$ be a reversible random walk. Then with the notation above,
\[ d_2(t+1) \leq (1-\tilde{\Phi}_X^2)d_2(t). \]
In particular,
\[ d_2(t) \leq 2(1-\tilde{\Phi}_X^2)^t. \]
\end{ftheo}

\subsubsection{An Overview of Random Walks for Uniform Distributions}

The basic algorithm used in most algorithms that attempt to solve this problem, which we mentioned at the beginning of this section, was proposed by Dyer, Frieze, and Kannan in \cite{dyer-frieze-kannan} and has remained largely unchanged.\\
This algorithm is $\mathcal{O}(n^{23}(\log n)^5\varepsilon^{-2}\log(1/\varepsilon)\log(1/\eta))$. Henceforth, to make things relatively simple, we use the $\mathcal{O}^*$ notation that suppresses any powers of $\log n$ and polynomials of $(1/\varepsilon)$ and $\log(1/\eta)$. With this notation, the algorithm is $\mathcal{O}^*(n^{23})$.\\

We use a multiphase Monte Carlo algorithm while using random walks to sample. The improvements on this algorithm since its proposal have primarily involved changing the random walk used, using the conductance to bound the mixing time when we are likely to be close to the stationary distribution, and bounding the conductance using isoperimetric inequalities.\\

There are mainly three different types of random walks used.

\paragraph{Walking on the Grid.}

This is probably the simplest graph. It defines a sufficiently fine grid $\mathbb{L}_\delta$ where each step is of size $\delta$. Suppose we are at $x_t$. At each step, we stay put at $x_t$ with probability $\frac{1}{2}$. Otherwise, we choose a random vector $v$ of the $2n$ possible directions. If $x_t+v\not\in K$, we remain at $x_t$ and otherwise, we move to $x_{t+1}=x_t+v$.\\
\cite{dyer-frieze-kannan} uses this walk with a value of $\delta$ around $n^{-5/2}$. In \cite{lovasz-simonovits-mixing-rate-isoperimetric}, this was improved to a $\delta$ around $n^{-3/2}$.

\paragraph{Ball-Steps.}

In this random walk, we choose some small step-size $\delta$. We use a lazy random walk but when we try to move, we choose a random $v\in\delta B_2^n$. Similar to the grid, if $x_j+v\not\in K$, we remain at $x_j$ and otherwise, we move to $x_{j+1}=x_j+v$. In \cite{KLS-n5}, the value of $\delta$ was around $n^{-1/2}$.

\paragraph{Hit-and-Run.}

Unlike the previous two walks where we had to choose a step-size $\delta$, this walk doesn't need anything of the sort. We choose a random unit vector $v$ from $B_2^n$. We then find the length of the intersection of $\{x+tv:t\in\R\}$ with $K$ and pick a uniformly distributed $x'$ from this segment. It is believed that this walk converges very rapidly.\\

An issue (in any of the walks) that we must figure out how to rectify is that of getting stuck in some corner (we had given this as motivation for defining the conductance of a random walk).\\
For example, in the ball-step walk, we can consider the \textit{local conductance}
\[ \ell_\delta(x) \coloneqq \frac{\vol(K\cap (x+\delta B_2^n))}{\vol(\delta B_2^n)} \]
and the overall conductance
\[ \lambda\coloneqq \frac{1}{\vol(K)} \int_K \ell_\delta(x)\d{x}. \]

In recent times, it has also been a common theme to use \textit{Metropolis chains}, which are defined as follows.

\paragraph{Metropolis Chain.} Suppose we have a function $f$ on $K$ and a random walk (of any of the above types). We can modify our walk using the same laziness as above, but when we wish to move, we check if $f(x)\geq f(x+v)$ (where $x$ is the original position and $x+v$ is the new proposed position) and
\begin{itemize}
	\item if \texttt{yes}, move to $x+v$.
	\item if \texttt{no}, move to $x+v$ with probability $\frac{f(x)}{f(x+v)}$ (and stay at $x$ otherwise).
\end{itemize}

If $\int f < \infty$, this produces a random walk with stationary distribution that is proportional to $f(x)$.

So far, we have only tried finding uniform distributions within the body $K$ (and never return a point outside the body $K$). Often, however, we sacrifice this in favour of a distribution that can return a point outside of $K$ with not too high probability (say less than $\frac{1}{2}$) that mixes more rapidly. We detail one such algorithm, similar to that in \cite{dyer-frieze-sample-outside}, in the following section.

\subsection{A Modified Grid Walk that Runs in \texorpdfstring{$\mathcal{O}^*(n^8)$}{O(n8)}}

\subsubsection{A Description of the Walk}

The algorithm we describe here uses the ``Walking on the Grid" mentioned in the previous section. This involves splitting the body into cubes. To this end, it was observed that if want to sandwich a body between two concentric \textit{cubes} instead of balls, then a ratio of $\mathcal{O}(n)$ can be obtained (instead of the ball-sandwiching ratio of $\mathcal{O}(n^{3/2})$). In particular, \cite{applegate-kannan-cube-sandwich} shows that we can find an affine transformation $\tilde K$ of $K$ such that
\[ B_\infty^n\subseteq \tilde{K}\subseteq 2(n+1)B_\infty^n. \]
% We shall instead just stick with a far looser affine transformation that gives
% \[ B_\infty^n \subseteq \tilde{K} \subseteq n^2 B_\infty^n \]
Henceforth, we refer to this $\tilde{K}$ as $K$. So in this case, it is more convenient to consider $K_i=2^{i/n}$, $0\leq i\leq m\coloneqq \lceil n\log_2(2(n+1))\rceil$ instead of the intersections with the balls we used earlier. That is, at each phase we have two bodies $K$ and $L$ such that
\[ K_0 = B_\infty^n \subseteq L \subseteq K \subseteq 2(n+1) B_\infty^n = K_m \]
and
\[ L\subseteq K\subseteq 2^{1/n}L. \]
The grid graph over which we design our random walk has vertex set
\[ V = \frac{1}{2n}\Z^n \cap K_m.  \]
That is, $V$ is the vertex set of the grid graph $P_l^n$ with $l=8n(n+1)+1$ (having $l^n$ vertices). Denote this graph by $G$ (there is an edge between points whose distance under the $\ell_\infty$ norm is $\frac{1}{2n}$).\\
We wish to create a rapidly mixing random walk that converges to the stationary distribution on $K$ (or something that could serve the same purpose). Let us now define a distribution on $V$ that is the stationary distribution of a specific random walk.\\
Consider the function $\varphi_0$ on $\R^n$ defined by
\[ \varphi_0(x) = \min\left\{s\geq 0 : x \in \left(1+\frac{s}{2n}\right)K\right\} \]
and $\varphi$ defined by $\varphi(x)=\left\lceil \varphi_0(x)\right\rceil$. Finally, define $f(x)=2^{-\varphi(x)}$.\\
There are a few things to observe that make it apparent why this $f$ is a good choice for our purposes:
\begin{itemize}
	\item For $x\in K$, $f(x)=1$.
	\item If $x,y$ are such that $\norm{x-y}_\infty \leq \frac{1}{2n}$, then $|\varphi_0(x)-\varphi_0(y)|\leq 1$. Indeed,
	\[ x = y + (x-y) \in \left(1+\frac{\varphi_0(y)}{2n}\right)K + \frac{1}{2n}K = \left(1+\frac{\varphi_0(y)+1}{2n}\right)K \]
	so $\varphi_0(x) \leq \varphi_0(y)+1$.
	\item How many $x\in V$ are there such that $\varphi(x)=s>0$ (so $f(x)=2^{-s}$)? We must have
	\[ x \in \left(1+\frac{s}{2n}\right)K \mathbin{\big\backslash} \left(1+\frac{s-1}{2n}\right)K. \]
	The volume of the body on the right is about
	\[ \left(\left(1+\frac{s}{2n}\right)^n - \left(1+\frac{s-1}{2n}\right)^n\right) \vol(K) < (e^{s/2}-1) \vol(K). \]
	Multiplying by an appropriate factor on either side, the number of points in $V$ in this body is at most $(e^{s/2}-1)f(K)$. Therefore, for $n>3$,
	\[ f(V) = f(K) + \sum_{s=1}^\infty 2^{-s}(e^{s/2}-1)f(K) < 5f(K). \]
\end{itemize}
The above suggests that a Metropolis chain under this function might be exactly what we want -- the first point ensures that the resulting distribution is constant on $K$.\\

What is the Metropolis chain corresponding to $f$ for $G$? It is easily checked that its transition matrix is given by
\[
p(x,y) =
\begin{cases}
\frac{1}{4n}, & xy\in E\text{ and }\varphi(y)\leq\varphi(x), \\
\frac{1}{8n}, & xy\in E\text{ and }\varphi(y)=\varphi(x)+1, \\
1-\sum_{z\in\Gamma(x)}p(x,z), & x=y, \\
0, & \text{otherwise.}
\end{cases}
\]

It can also easily be checked that this walk is reversible.\\
% It is also seen that the walk stays put at $x$ with probability at least $\frac{1}{2}$.\\
Now, let the stationary distribution of this walk be $\lambda$, given by $\lambda(x)=cf(x)$ for a suitable normalizing constant $c$. The third point above ensures that $\lambda(K)>1/5$ and we don't get points outside of $K$ too often.\\

There is another issue that we haven't mentioned so far that this walk takes care of. When we have such a walk (in general), we would want to be able to compute the transition probabilities efficiently only at the points where we need it -- it would be absurd to store the entire transition matrix all the time. In this example, all we have to do is ``carry" the current value of $\varphi$ with us. At most $4n$ appeals to the oracle will give us the values of $\varphi$ at all the neighbours! We can start at a point that we know the value of $\varphi$ of, such as $0\in B_\infty^n \subseteq K$.

\subsubsection{Showing Rapid Mixing by Bounding Conductance}

The only thing that remains to show now is that it suffices to run the above random walk for a polynomial amount of time to get sufficiently close to the stationary distribution, that is, that the walk is rapidly mixing.\\
By \Cref{large conductance implies rapidly mixing}, it suffices to show that this walk has large conductance. To do this, we use the following isoperimetric inequality given in \cite{lovasz-simonovits-mixing-rate-isoperimetric}.

\begin{ftheo}
\label{conductance isoperimetric inequality}
	Let $M\subseteq\R^n$ and $\mathcal{B}(M)$ be the $\sigma$-field of Borel subsets of $M$. Let $F:\Int M\to\Rp$ be a log-concave function and let $\mu$ be the measure on $\mathcal{B}(M)$ with density $F$
	\[ \mu_F(A) = \int_A F \]
	for $A\in\mathcal{B}(M)$. Then for $A_1,A_2\in\mathcal{B}(M)$,
	\[ \mu_F(M \setminus (A_1\cup A_2)) \geq \frac{d(A_1,A_2)}{\diam M} \min(\mu_F(A_1),\mu_F(A_2)), \]
	where $\diam M = \sup\{\norm{x-y}:x,y\in M\}$.
\end{ftheo}

This inequality is slightly loose. The best possible constant on the right has an extra multiplicative factor of $2$ and was proved in \cite{dyer-frieze-sample-outside}. We omit the proof of the above for now and later prove a better bound in \Cref{improvement of conductance isoperimetric inequality} (and in the process, prove \Cref{conductance isoperimetric inequality} as a simple consequence of \Cref{localization lemma}). We shall now use this to show that the conductance of our random walk is large.\\
Let us have $U\subseteq V$ with $0<\lambda(U)<\frac{1}{2}$ and let $\overline{U}=V\setminus U$. Also, let $\partial{U}$ be the set of vertices in $\overline{U}$ with at least one neighbour in $U$.\\
Let $M$ be the union of the cubes of side length $1/2n$ centered at vertices of $V$ ($M$ is a solid cube) and $A_1$ be the union of cubes of side length $1/2n$ centered at vertices of $U$. Let $B$ be the union of cubes of volume $2/(2n)^n$ centered at vertices of $\partial U$ and $A_2 = M \setminus (A_1 \cup B)$. Obviously,
\begin{equation}
	\diam(M) = \mathcal{O}(n^{3/2}).
\end{equation}
Then, observe that
\begin{equation}
	d(A_1,A_2) \geq \frac{1}{2n}\frac{\sqrt{n}}{2} (2^{1/n} - 1) = \Omega(n^{-3/2}).
\end{equation}
for some suitable positive constant $c_1$. Also, for some positive constant $c_2$,
\begin{align*}
	\sum_{\substack{u\in U \\ v \in \overline{U}}}\lambda(u)p(u,v) &= \sum_{\substack{u\in U \\ v \in B}}\lambda(v)p(v,u)\\
	&\geq \sum_{v\in B} \frac{1}{8n} \lambda(v) = c_2\frac{\lambda(B)}{n}.
\end{align*}

We may assume that $\lambda(B)$ is small.
Now, define a measure $\mu$ on $\mathcal{B}(M)$ as in \Cref{conductance isoperimetric inequality} with $F=2^{-\varphi_1}$, where $\varphi_1$ is the maximal convex function on $M$ bounded above by $\varphi$. Observe that $\lambda(u)$ is always within a constant factor of the $\mu$-measure of the unit cube centered at $u$. Thus, we have
\begin{align}
	\frac{\sum_{u\in U} \sum_{v\in\overline{U}} \lambda(u)p(u,v)}{\lambda(U)} &\geq \frac{c_3}{n}\frac{\mu(B)}{\min\{\lambda(U),\lambda(\overline{U}\setminus\partial U)\}} \nonumber \\ % *** HOW??? ***
	&= \frac{c_3}{n}\frac{\mu(M \setminus (A_1\cup A_2))}{\min\{\mu(A_1),\mu(A_2)\}} \nonumber \\
	&\geq \frac{c_3}{n} \frac{d(A_1,A_2)}{\diam M} \nonumber \\
	&= \Omega(n^{-4}) \label{eqn: number of steps}
\end{align}

So what is the total time complexity of the algorithm? Combining \Cref{eqn: number of steps} and \Cref{eqn: condition on time in terms of conductance} (or rather, the corresponding result for $d_2(t)$ that does not have the $\log n$ factor), the number of steps in the random walk of each phase of the multiphase Metropolis walk is $\mathcal{O}^*(n^{8})$. At each step of the walk, we perform $\mathcal{O}(n)$ oracle queries. Finally, there are $\mathcal{O}^*(n)$ phases. All together, the algorithm is $\mathcal{O}^*(n^{10})$.\\

In \cite{dyer-frieze-sample-outside}, a more careful analysis is done to show that this algorithm is in fact $\mathcal{O}^*(n^{8})$.\footnote{The conductance is actually $\Omega(n^{-3})$.} More precisely, it is
\[ \mathcal{O}\left(n^8\varepsilon^{-2}\log\left(\frac{n}{\varepsilon}\right)\log\left(\frac{1}{\eta}\right)\right). \]

% \subsubsection{Closing Statements}

% When \cite{dyer-frieze-kannan} originally showed that it was possible to compute the volume in polynomial time, pre-sandwiching was a negligible step in the process being nearly insignificant compared to the $\mathcal{O}^*(n^{23})$ that the main part of the algorithm took. However, as time has passed and the second part of the algorithm has become faster and faster, it has become necessary to get a more efficient algorithm for sandwiching as well. In particular, in \cite{KLS-n5}, both the sandwiching and the second part were $\mathcal{O}^*(n^5)$. If we want to get faster than this, then we have to speed up the sandwiching as well.\\
% The main innovation in this is to do \textit{approximate sandwiching} instead. Instead of insisting that $B_2^n\subseteq K\subseteq d B_2^n$, we instead want $B_2^n\subseteq K$ and that $d' B_2^n\cap K$ is ``most of" $K$. Despite what we had said earlier that might be slightly misleading, even \cite{KLS-n5} uses an approximate sandwiching algorithm.
% % More recently, \cite{vempala-n4} uses such an algorithm to achieve an $\mathcal{O}^*(n^4)$ algorithm for volume estimation.

% There is also the following problem by Lov\'asz related to sandwiching.
% \begin{quote}
%     Given a convex $K\subseteq\R^n$ and an $\varepsilon>0$, can one always find two homethetic ellipsoids $\mathcal{E}_1$ and $\mathcal{E}_2$ such that their ratio is $\leq(\log n)^c$ and $\vol(K\setminus\mathcal{E}_2)<\varepsilon\vol(K)$ and $\vol(\mathcal{E}_1\setminus K)<\varepsilon\vol(K)$?
% \end{quote}

% With this approximate sandwiching as motivation, we introduce the following.

% There exists a unique ellipsoid $\mathcal{E}_K$ such that for every $s\in\R^n$,
% \[ \int_K \langle \textbf{x},s\rangle^2 \d{\textbf{x}} = \int_{\mathcal{E}_K} \langle \textbf{x},s\rangle^2 \d{\textbf{x}}. \]
% This ellipsoid is known as the Legendre ellipsoid of $K$. A body is said to be in isotropic position if its Legendre ellipsoid is $B_2^n$. That is,

% \begin{fdef}[Isotropic Position]
% A convex body $K\subseteq\R^n$ is said to be in \textit{isotropic position} if its center of gravity is the origin
% \[ \textbf{b}(K) = \int \textbf{x}\d{\textbf{x}} = 0, \]
% and for every $1\leq i\leq j\leq n$,
% \[ \frac{1}{\vol(K)} \int_K x_i x_j =
% \begin{cases}
% 1, & i=j, \\
% 0, & \text{otherwise.}
% \end{cases}
% \]
% \end{fdef}

% Note that this implies
% \[ \int_K \norm{\textbf{x}}^2\d{\textbf{x}} = n. \]
% Markov's inequality then implies that all but an $\varepsilon$ portion of $K$ belongs to $\displaystyle\sqrt{\frac{n}{\varepsilon}}B_2^n$.

% For computational purposes,

% \begin{definition}
% For $\nu\in(0,1)$, $K$ is said to be in \textit{$\nu$-almost-isotropic position} if $\norm{\textbf{b}(K)}\leq \nu$ and for every $v\in\R^n$,
% \[ (1-\nu)\norm{v}^2 \leq \frac{1}{\vol(K)} \int_{K-\textbf{b}(K)} \langle v,\textbf{x}\rangle^2\d{\textbf{x}} \leq (1+\nu)\norm{v}^2. \]
% \end{definition}

% There is in fact a randomized algorithm that finds in $\mathcal{O}\left(n^5 \log (n) \log\left(\frac{1}{\nu\eta}\right)\right)$ steps a linear transformation $\tilde{K}$ of $K$ such that $\tilde{K}$ is $\nu$-almost-isotropic with probability at least $1-\eta$. Further, with probability at most $1-\eta$,
% \[ \vol\left(\tilde{K} \setminus 2\sqrt{2n}\log\left(\frac{1}{\varepsilon}\right)B_2^n\right) < \varepsilon\vol(\tilde{K}). \]

% This isotropic position is what has been used instead of usual sandwiching in \cite{KLS-n5,vempala-n4}.\\

% We now state one of the most important conjectures in this field, commonly known as the KLS Conjecture (named after Kannan, Lov\'asz, and Simonovits).

Over the course of the next few sections, we describe a $\mathcal{O}^*(^7)$ volume estimation algorithm given in \cite{lov-sim-on7}.

\subsection{Measure-Theoretic Markov Chains and Conductance}
\label{subsec: measure theoretic markov chains}

\subsubsection{Some Basic Definitions}

\begin{fdef}
Let $\Omega$ be a non-empty set and $\mathcal{A}$ a $\sigma$-algebra on $\Omega$. For every $u\in\Omega$, let $P_u$ be a probability measure on $\Omega$. Also assume that as a function of $u$, $P_u(A)$ is measurable for any $A\in\mathcal{A}$. We call the triple $(\Omega,\mathcal{A},\{P_u:u\in\Omega\})$ a \textit{Markov scheme}. Together, with an initial distribution $Q_0$ on $\Omega$, this defines a \textit{Markov chain}.
\end{fdef}

A Markov chain is just a sequence of random variables $w_0,w_1,\ldots$ such that $w_0$ is drawn from $Q_0$ and $w_{i+1}$ is drawn from $P_{w_i}$ (independently of the values of $w_0,\ldots,w_{i-1}$). Therefore,
\[ \Pr[w_{i+1}\in A \mid w_1=u_1,\ldots,w_i=u_i] = \Pr[w_{i+1}\in A\mid w_i=u_i] = P_{u_i}(A). \]
Let $f:\Omega\times\Omega\to\R$ be an integrable function (with respect to the product measure $\mu\times\mu$) such that $\int_\Omega f(u,v)\d{\mu}(v)=1$ for all $u\in\Omega$. $f$ then defines a Markov scheme as
\[ P_u(A) = \int_A f(u,v)\d{\mu}(v). \]
In this case, $f$ is known as the \textit{transition function} of the Markov scheme. The transition function is said to be \textit{symmetric} if $f(x,y)=f(y,x)$.\\

A probability measure $Q$ on $\Omega$ is said to be the \textit{stationary distribution} of the Markov scheme if for all $A\in\mathcal{A}$,
\[ \int_{\Omega}P_u(A)\d{Q}(u) = Q(A). \]
This just means that every $w_i$ has the same distribution as that of $Q$.\\

Now, consider the inner product space $L^2 = L^2 (\Omega,\mathcal{A},Q)$ with inner product
\[ \langle f,g\rangle = \int_\Omega f g\d{Q}. \]
Suppose we have some function $g\in L^2$. Then note that the expectation of $g(w_{i+1})$ (as a function of $w_i = u$) defines a positive linear operator\footnote{a linear operator $A$ such that $\langle Ax,x\rangle\geq 0$ for any $x$.} $M:L^2\to L^2$ by
\[ (Mg)(u) = \int_\Omega g(v)\d{P_u}(v). \]
Further note that $(M^k g)(u)$ represents the expectation of $g(w_{i+k})$ given that $w_i=u$.

Now, consider a Markov chain where the first element is drawn from the stationary distribution. Then observe that for any function $g\in L^2$,
\begin{align*}
	\expec[g(w_i)] &= \expec[g(w_0)] = \langle g,1\rangle \\
	\expec[g(w_i)^2] &= \expec[g(w_0)^2] = \langle g,g\rangle \\
	\expec[g(w_i)g(w_{i+k)}] &= \expec[g(w_0)g(w_k)] = \langle g,M^k g\rangle
\end{align*}

A Markov chain is said to be \textit{time-reversible} if for any $A,B\in\mathcal{A}$, the probability of going from $A$ to $B$ is the same as that of going from $B$ to $A$. That is,
\[ \int_B P_u(A)\d{Q}(u) = \int_A P_u(B) \d{Q}(u). \]
It is easy to see that it suffices to have the above for all disjoint sets $A$ and $B$. The above can be rewritten in an even more symmetric fashion as
\[ \int_B \int_A 1\d{P}_u(v)\d{Q}(u) = \int_A \int_B 1\d{P}_u(v)\d{Q}(u). \]
This is equivalent to saying that for any function $g:\Omega\times\Omega\to\R$ (assuming both sides are well-defined),
\begin{equation}
\label{eqn: time reversible function formulation}
	\int_\Omega \int_\Omega F(u,v)\d{P}_u(v)\d{Q}(u) = \int_\Omega \int_\Omega F(v,u)\d{P}_u(v)\d{Q}(u).
\end{equation}
It is equivalent to say that the operator $M$ is self-adjoint.\footnote{an operator $A$ such that $\langle Ax,y\rangle=\langle x,A y\rangle$ for any $x,y$.} If the Markov scheme can be described by a transition function $f$ (with respect to $Q$), then time-reversibility is equivalent to the symmetry of $f$.\\
If the Markov scheme is time-reversible, then for any $g\in L^2$,
\begin{align}
	\langle g,g\rangle - \langle g,M g\rangle &= \int_{\Omega} g^2\d{Q} - \int_\Omega\int_\Omega g(u)g(v)\d{P_u}(v)\d{Q}(u) \nonumber \\
	&= \int_{\Omega}\int_\Omega g^2(u)\d{P_u}(v)\d{Q}(u) - \int_\Omega\int_\Omega g(u)g(v)\d{P_u}(v)\d{Q}(u) \nonumber \\
	&= \frac{1}{2} \left(\int_{\Omega}\int_\Omega (g^2(u)+g^2(v))\d{P_u}(v)\d{Q}(u) - \int_\Omega\int_\Omega 2g(u)g(v)\d{P_u}(v)\d{Q}(u)\right) & (\text{by \Cref{eqn: time reversible function formulation}}) \nonumber \\
	&= \frac{1}{2}\int_\Omega\int_\Omega (g(u)-g(v))^2\d{P_u}(v)\d{Q}(u) \geq 0. \label{eqn: self-adjoint spectral radius 1}
\end{align}
Therefore, the spectral radius\footnote{the largest absolute value of its eigenvalues.} of $M$ is exactly $1$.

\begin{definition}[Laziness]
A Markov chain is said to be \textit{lazy} if for each $u$,
\[ P_u(\{u\})\geq\frac{1}{2}. \]
\end{definition}

There are two main, albeit minor and technical, reasons for desiring laziness:
\begin{itemize}
	\item Sometimes, a lack of laziness can cause parity issues which result in the limit distribution of a chain not converging to the stationary distribution.
	\item In the time-reversible case, it makes the operator $M$ positive semidefinite, thus making it far easier to analyze.
\end{itemize}
To see why the latter occurs, note that if $M$ is self-adjoint, then so is $2M-I$ and by a proof exactly like that of \Cref{eqn: self-adjoint spectral radius 1},
\[ \langle f,M f\rangle = \frac{1}{2}\langle f,f\rangle + \frac{1}{2}\langle f,(2M-I)f\rangle \geq 0. \]
Any Markov scheme can be made lazy easily by flipping a (fair) coin at each step and making a move only if it lands on tails.

\begin{lemma}
Let $w_1,w_2,\ldots$ be a time-reversible Markov chain generated by a lazy Markov scheme $\mathcal{M}$ with $w_0$ drawn from the stationary distribution $Q$ of $\mathcal{M}$. Then for any function $g\in L^2$,
\[ \expec[g(w_i)g(w_j)] \geq \expec[g(w_i)]\expec[g(w_j)] = \expec[g(w_0)^2]. \]
\end{lemma}
\begin{proof}
Assume without loss of generality that $j>i$ and $j-i=k$. Then for any function $h$, the positive semidefiniteness of $M$ implies that
\[ \expec[h(w_i)h(w_j)] = \langle h, M^k h\rangle \geq 0. \]
Applying this to $(g - \expec[g(w_0)])$ yields the result.
\end{proof}

\subsubsection{Conductance}

\begin{definition}[Ergodic Flow]
Define the \textit{ergodic flow} $\Phi:\mathcal{A}\to[0,1]$ of a Markov scheme by
\[ \Phi(A) = \int_A P_u(\Omega \setminus A) \d{Q}(u). \]

\end{definition}

This just measures how likely $w_1$ is to leave the subset $A$ if $w_0$ is initially drawn from $Q$. Observe that since $Q$ is stationary,
\begin{align*}
	\Phi(A) - \Phi(\Omega\setminus A) &= \int_A P_u(\Omega\setminus A)\d{Q}(u) - \int_{\Omega\setminus A} P_u(A)\d{Q}(u) \\
	&= Q(A) - \int_A P_u(A)\d{Q}(u) - \int_{\Omega\setminus A} P_u(A)\d{Q}(u) & (\text{since } P_u(\Omega\setminus A)=1-P_u(A)) \\
	&= Q(A) - \int_\Omega P_u(A)\d{Q}(u) = 0.
\end{align*}
Even conversely, if for some probability distribution $Q'$, the function $\Phi':\mathcal{A}\to[0,1]$ defined by
\[ A\mapsto \int_A P_u(\Omega\setminus A)\d{Q}(u) \]
is invariant under complementation, then $Q'$ is stationary.\\

\begin{fdef}
	\label{def: conductance}
	The \textit{conductance} of the Markov scheme is then defined as
	\[ \Phi = \inf_{0<Q(A)<1/2} \frac{\Phi(A)}{Q(A)}. \]
	For $0\leq s\leq 1$, the \textit{$s$-conductance} is defined as
	\[ \Phi_s = \inf_{s < Q(A) \leq 1/2} \frac{\Phi(A)}{Q(A)-s}. \]
\end{fdef}

The lower the conductance is, the more likely the Markov chain is to ``get stuck" somewhere.\\

For any $u$, $1-P_u(\{u\})$ is called the \textit{local conductance} of the Markov chain at $u$. If $Q(u)>0$,\footnote{it is an atom.} then the local conductance is an upper bound on the conductance.\\
More generally, let
\[ H_t = \{u\in\Omega:P_u(\{u\}) > 1-t\} \]
and $s=Q(H_t)$. Then
\[ \Phi(H_t) = \int_{H_t} P_u(\Omega\setminus H_t)\d{Q}(u) < t Q(H_t). \]
Therefore, the $(s/2)$-conductance is at most $2t$.

The main use of defining conductance is that it is closely related to how fast Markov chains converge to their stationary distribution.\\
Suppose that $Q_k$ is the distribution in the $k$th step of the chain ($Q_k(A)=\Pr[w_k\in A]$). It turns out that if for all $A\in\mathcal{A}$ such that $Q(A)>0$, $\Phi(A)>0$, then $Q_k\to Q$ (in the $\ell_1$ distance\footnote{given by $\norm{f}_1 = \int_\Omega |f|\d{Q}$}.). This naturally provides a bound on the speed of convergence.\\
Let us consider the following particular distance function.

\subsubsection{A Distance Function}

\begin{fdef}
For $x\in[0,1]$, consider all measurable functions $g:\Omega\to[0,1]$ such that
\[ \int_{\Omega} g\d{Q} = x. \]
We then define the \textit{distance function} of $Q$ and $Q_k$ by
\[ h_k(x) = \sup_g \int_\Omega g(\d{Q}_k-\d{Q}) = \sup_g \int_\Omega g\d{Q}_k - x. \]
\end{fdef}

For example, it is easily shown that for a finite Markov chain with $N$ states and uniform stationary distribution, $h_k(j/N)$ is the sum of the $j$ largest $\left(Q_k(\omega)-\frac{1}{n}\right)$.\\

There are a few things to note.
\begin{itemize}
	\item For any $x$, $0\leq h_k\leq 1-x$. The lower bound is because one can consider the constant function $x$ on $\Omega$. The upper bound is because $\int_\Omega g\d{Q}_k$ is bounded above by $1$. In particular, $h_k(1)=0$.
	\item $h_k$ is a convex function of $x$. We shall see below in \Cref{hk distance supremum attained} that the supremum in the definition of $h_k$ is attained. Then, for any $a,b,\lambda\in[0,1]$, set $x=\lambda a+(1-\lambda)b$ and let $g_1,g_2$ be the functions that attain the supremums for $h_k(a)$ and $h_k(b)$. Then,
	\[ \sup_g \int_\Omega g\d{Q}_k - x \geq \int_\Omega (\lambda g_1 + (1-\lambda)g_2) \d{Q}_k = \lambda h_k(a) + (1-\lambda) h_k(b). \]
\end{itemize}

This definition might seem quite artificial at the moment, but we hope to give more context to it with the following few lemmas.

\begin{lemma}
	For every set $A\in\mathcal{A}$ with $Q(A)=x$,
	\[ -h_k(1-x) \leq Q_k(A) - Q(A) \leq h_k(x). \]
\end{lemma}
\begin{proof}
	The upper bound is immediate from the definition of the distance function by taking $g = \indic_A$ (the indicator function on $A$). The similar upper bound for $\Omega\setminus A$ immediately gives the result.
\end{proof}

\begin{lemma}
\label{hk distance supremum attained}
	For every $0<x<1$, there exists a function $G$ that is $0$-$1$ valued except possibly on a $Q$-atom\footnote{a $Q$-atom is a set $V\in\mathcal{A}$ such that $Q(V)>0$ and for any $V'\subseteq V$, either $Q(V')=Q(V)$ or $Q(V')=0$.} that attains the supremum in the definition of $h_k(x)$.
\end{lemma}
\begin{proof}
	Let $U\in\mathcal{A}$ such that $Q(U)=0$ and $Q_k(U)$ is maximum. Let $Q'$ and $Q_k'$ be the restrictions of $Q$ and $Q_k$ to $\Omega\setminus U$. Clearly, the way we have defined $U$ implies that $Q_k'$ is absolutely continuous with respect to $Q'$. Thus, let $\phi$ be the Radon-Nikodym derivative of $Q_k'$ with respect to $Q'$.\\
	Now, let $x\in[0,1]$ and $g:\Omega\to[0,1]$ such that $\int_\Omega g\d{Q} = x$.\\
	For $t\geq 0$, define
	\[ A_t = U \cup \{u \in \Omega\setminus U : \phi(u) \geq t\}\text{ and }s=\inf\{t\geq 0 : Q(A_t) \leq x\}. \]
	Observe that since $A_s = \bigcap_{t<s}A_t$, upper semicontinuity implies that $Q(A_s)\geq x$. Also define
	\[ A' = \bigcup_{t>s} A_t = U \cup \{u \in \Omega\setminus U : \phi(u) > s\}. \]
	Lower semicontinuity implies that $Q(A')\leq x$. We also have that $A'\subseteq A_s$ and for every $u\in A_s\setminus A'$, $\phi(u)=s$.\\
	Now, choose a $B\in\mathcal{A}$ such that $A'\subseteq B\subseteq A_s$, $Q(B)\leq x$, and $Q(B)$ is maximum.\\
	We first show that if $Q(B)=x$, then the indicator function on $B$ suffices. Indeed, in this case,
	\begin{align*}
		\int_\Omega g\d{Q}_k &= \int_U g\d{Q}_k + \int_{B\setminus U} g\phi\d{Q} + \int_{\Omega\setminus B} g\phi\d{Q} \\
		&= \int_U g\d{Q}_k + \int_{B\setminus U} (g-1)\phi\d{Q} + \int_{B\setminus U} \d{Q}_k + \int_{\Omega\setminus B} g\phi\d{Q} \\
		&\leq \int_U \d{Q}_k + s\int_{B\setminus U} (g-1)\d{Q} + \int_{B\setminus U} \d{Q}_k + s\int_{\Omega\setminus B} g \d{Q} & \text{($0\leq g\leq 1$ and $\phi\leq s$ $Q$-almost everywhere on $\Omega\setminus B$)} \\
		&= Q_k(B) + s\int_{\Omega\setminus U} g\d{Q} - s\int_{B\setminus U}\d{Q} \\
		&= Q_k(B) + s(x - Q(B)) = Q_k(B).
	\end{align*}
	We also see that the supremum is attained when $g=\indic_B$.\\
	Next, assume that $Q(B)<x$. Then for every $W\subseteq A_s\setminus B$, either $Q(W)=0$ or $Q(W)>x-Q(B)$. That is, the measure on $A'\setminus B$ is concentrated at atoms. Let $V$ be one such atom. As shown above,
	\[ \int_\Omega g\d{Q}_k \leq Q_k(B) + s(x-Q(B)). \]
	To show that this bound is attained, let $g=\indic_B+\lambda\indic_{V}$ where $\lambda = (x-Q(B))/Q(V)$. Clearly, $0\leq g\leq 1$. Further,
	\[ \int_\Omega g\d{Q} = Q(B) + \lambda Q(V) = x \]
	and
	\[ \int_\Omega g\d{Q}_k = Q_k(B)+\lambda Q_k(V) = Q_k(B) + s(x-Q(B)) \]
	where the last step follows since $\phi(u)=s$ for all $u\in V\subseteq A_s\setminus A'$.
\end{proof}

\begin{lemma}
	If $Q$ is atom-free, then
	\[ h_k(x) = \sup_{\substack{A\in\mathcal{A} \\ Q(A)=x}} \left(Q_k(A) - Q(A)\right). \]
\end{lemma}

This follows directly from the previous lemma.\\
Although we did say what a rapidly mixing random walk is earlier in \Cref{def: rapidly mixing random walks}, we now define it more generally.\\

\begin{lemma}
	
\end{lemma}

First, observe that

\[ \sup_{x} h_k(x) = \sup_{A\in\mathcal{A}} |Q_k(A) - Q(A)| = \frac{1}{2}\norm{Q_k - Q}_1. \]

Let us now get on to the main subject of this section, namely that of bounding the speed of convergence of rapidly mixing Markov chains.

\subsubsection{Rapidly Mixing Markov Chains}

\begin{fdef}[Rapidly Mixing Markov Chain]
A Markov chain is said to be \textit{rapidly mixing} if for some $\theta<1$, $\sup_x h_k(x)$ is $\mathcal{O}(\theta)^k$.
\end{fdef}

\begin{theorem}
For $k\geq 1$, if $s\leq x\leq 1/2$, then
\[ h_k(x) \leq \frac{1}{2} \left(h_{k-1}(x-2\Phi_s(x-s)) + h_{k-1}(x+2\Phi_s(x-s))\right) \]
and if $1/2 \leq x\leq 1-s$, then
\[ h_k(x) \leq \frac{1}{2} \left(h_{k-1}(x-2\Phi_s(1-x-s)) + h_{k-1}(x+2\Phi_s(1-x-s))\right). \]
\end{theorem}
\begin{proof}
We prove the first inequality alone.

By \Cref{hk distance supremum attained}, let $A$ be a set such that $Q(A)=x$ and $h_k(x)=Q_k(A)-Q(A)$. Define $g_1,g_2:\Omega\to[0,1]$ by
\[ g_1(u) = 
\begin{cases}
2P_u(A) - 1, & u\in A, \\
0, & \text{otherwise,}
\end{cases}
\quad
% \text{ and }
g_2(u) = 
\begin{cases}
1, & u\in A, \\
2P_u(A), & \text{otherwise.}
\end{cases}
\]
The functions map into $[0,1]$ because the chain is lazy.\\
Also, let $x_1 = \int_\Omega g_1\d{Q}$ and $x_2 = \int_\Omega g_2\d{Q}$. Observe that $x_1 + x_2 = \int_\Omega 2P_u(A)\d{Q}(u) = 2x$. We have
\begin{align*}
	h_k(x) &= Q_k(A) - Q(A) \\
	&= \frac{1}{2} \left( \left(\int_\Omega g_1\d{Q}_{k-1} - x_1\right) + \left( \int_\Omega g_2\d{Q}_{k-1} - x_2 \right) \right) \\
	&\leq h_{k-1}(x_1) + h_{k-1}(x_2).
\end{align*}
We also have
\[ x_2-x = x-x_1 = \int_A (2-2P_u(A))\d{Q}(u) = 2\Phi(A) \geq 2\Phi_s(x-s). \]
Then with the above, the concavity of $h_{k-1}$ then implies that
\[ h_{k-1}(x_1)+h_{k-1}(x_2) \leq h_{k-1}(x - \Phi_s(x-s)) + h_{k-1}(x+\Phi_s(x-s)), \]
which is what we want.
\end{proof}

The next result is analogous to \Cref{large conductance implies rapidly mixing} and is our main tool in bounding the speed of convergence using the conductance.

\begin{ftheo}
\label{decrease in markov distance}
	Let $0\leq s\leq 1/2$ and suppose we have $c_1,c_2$ such that for $s\leq x\leq 1-s$,
	\[ h_0(x) \leq c_1 + c_2\min\{\sqrt{x-s},\sqrt{1-s-x}\}. \]
	Then for every $k\geq 0$ and $s\leq x\leq 1-s$,
	\[ h_k(x) \leq c_1 + c_2\min\{\sqrt{x-s},\sqrt{1-s-x}\} \left(1-\frac{\Phi_s^2}{2}\right)^k. \]
\end{ftheo}
\begin{proof}
	We prove this via induction. It clearly holds for $k=0$. Suppose that $k\leq 1$ and $s\leq x\leq 1/2$. Using induction,
	\begin{align*}
		h_k(x) &\leq \frac{1}{2} \left(h_{k-1}(x-2\Phi_s(x-s)) + h_{k-1}(x+2\Phi_s(x-s))\right) \\
		&\leq c_1 + \frac{c_2}{2} \left(1-\frac{\Phi_s^2}{2}\right)^{k-1} \left(\sqrt{x-2\Phi_s(x-s)-s} + \sqrt{x+2\Phi_s(x-s)-s}\right) \\
		&= c_1 + \frac{c_2}{2} \sqrt{x-s} \left(1-\frac{\Phi_s^2}{2}\right)^{k-1} \left(\sqrt{1-2\Phi_s} + \sqrt{1+2\Phi_s}\right) \\
		&\leq c_1 + \frac{c_2}{2}\sqrt{x-s}\left(1-\frac{\Phi_s^2}{2}\right)^{k-1} \left(1-\frac{2\Phi_s}{2}-\frac{4\Phi_s^2}{8} + 1+\frac{2\Phi_s}{2}-\frac{4\Phi_s^2}{8}\right) \\
		&= c_1 + c_2 \sqrt{x-s} \left(1-\frac{\Phi_s^2}{2}\right)^k \qedhere
	\end{align*}
\end{proof}

Writing the above in a slightly more useful form,

\begin{corollary}
	Let $M=\sup_A Q_0(A)/Q(A)$. Then for every $A\in\mathcal{A}$,
	\[ |Q_k(A) - Q(A)| \leq \sqrt{M}\left(1-\frac{\Phi_s^2}{2}\right)^k. \]
\end{corollary}
\begin{proof}
	Clearly, for any $x$, $h_0(x) \leq Mx$. We also have $h_0(x) \leq 1-x$. Therefore,
	\[ h_0(x) \leq \sqrt{Mx(1-x)} \leq \sqrt{M}\min\{\sqrt{x},\sqrt{1-x}\}. \]
	\Cref{decrease in markov distance} then implies the required.
\end{proof}

% \begin{corollary}
%     Let $0\leq s\leq 1/2$ and $H_s=\sup\{|Q_0(A)-Q(A)|:Q(A)\leq s\}$. Then for every $A\in\mathcal{A}$,
%     \[|Q_k(A)-Q(A)| \leq H_s + \frac{H_s}{s}\left(1-\frac{\Phi_s^2}{2}\right)^k. \]
% \end{corollary}
% \begin{proof}
%     We show that for every $0\leq x\leq 1$,
%     \[ h_0(x) \leq H_s + \frac{H_s}{s}\sqrt{x-s}. \]
%     For $0\leq x\leq s$,
%     \[ h_0(x) = \sup_{\substack{A\in\mathcal{A} \\ Q(A)=x}} (Q_0(A)-Q(A)) \leq H_s. \]
%     It is similarly shown (by taking the complement) that for $1-s\leq x\leq 1$, $h_0(x) \leq H_s$. Now, the concavity of $h_0(x)$ implies that for any $s\leq x\leq 1$,
%     \[ h_0(x) \leq \frac{h_0(s)\cdot x}{s} \leq H_s \frac{H_s}{s}(x-s) \leq H_s + \frac{H_s}{s}\sqrt{x-s}. \]
%     Similarly, for $0\leq x\leq 1-s$, $h_0(x) \leq H_s + \frac{H_s}{s}\sqrt{1-x-s}$.\\
%     Therefore, for all $s\leq x\leq 1-s$,
%     \[ h_k(s) \leq H_s + \frac{H_s}{s}\left(1-\frac{\Phi_s^2}{2}\right)^k\min\{\sqrt{x-s},\sqrt{1-x-s}\} \leq H_s + \frac{H_s}{s}\left(1-\frac{\Phi_s^2}{2}\right)^k. \]
%     The inequality trivially holds for $x<s$ and $x>1-s$, since we have $h_0(x) \leq H_s$ by definition (with complementation in the second case) and $h_k(x) \leq h_0(x)$.
% \end{proof}

\subsubsection{An Important Inequality involving the operator \texorpdfstring{$M$}{M}}

A little bit of thought makes it quite clear that to analyze the speed of mixing of Markov chains, the spectrum of the operator $M$ is an important parameter.

\begin{ftheo}
\label{rapid mixing expec 0}
	Let $\mathcal{M}$ be a time-reversible Markov scheme with conductance $\Phi$. Then for every $g\in L^2$ with $\expec[g]=0$,
	\[ \langle g,Mg\rangle \leq \left(1-\frac{\Phi^2}{2}\right)\norm{g}^2. \]
\end{ftheo}

\begin{proof}
	As might be expected, we use \Cref{eqn: self-adjoint spectral radius 1} in this proof. It suffices to show that if $\expec[g]=0$,
	\[ \int_\Omega \int_\Omega (g(u)-g(v))^2 \d{P}_u(v)\d{Q}(u) \geq \Phi^2 \norm{g}^2. \]
	Choose a median $r$ of $Q$, that is, a real number such that $Q(\{x:g(x)>r\}) \leq 1/2$ and $Q(\{x:g(x)<r\}) \leq 1/2$. Let $h(x)=\max\{g(x)-r,0\}$. Observe that
	\[ \int_\Omega \int_\Omega (g(u)-g(v))^2 \d{P}_u(v)\d{Q}(u) \geq \int_\Omega \int_\Omega (h(u)-h(v))^2 \d{P}_u(v)\d{Q}(u) \]
	Therefore, it suffices to bound the quantity on the right suitably. To do this, use the Cauchy-Schwarz inequality to get
	\begin{align*}
		\int_\Omega \int_\Omega (h(u)-h(v))^2 \d{P}_u(v)\d{Q}(u) &\geq \frac{\left(\int_\Omega \int_\Omega |h^2(u)-h^2(v)| \d{P}_u(v)\d{Q}(u)\right)^2}{\int_\Omega \int_\Omega (h(u)+h(v))^2 \d{P}_u(v)\d{Q}(u)}
	\end{align*}
	Now, by definition, $Q(\{h\geq 0\})\leq 1/2$. Therefore,
	\[ \int_\Omega h^2\d{Q} \geq \frac{1}{2} \int_\Omega (g(x)-r)^2 = \frac{\norm{g}^2+r^2}{2} \geq \frac{\norm{g}^2}{2}. \]
	To bound the denominator,
	\[ \int_\Omega \int_\Omega (h(u)+h(v))^2 \d{P}_u(v)\d{Q}(u) \leq 2 \int_\Omega \int_\Omega (h^2(u)+h^2(v)) \d{P}_u(v)\d{Q}(u) = 2\norm{h} \]
	For each $t$, define $A_t = \{x\in\Omega : h(x)^2 \geq t\}$. Then
	\begin{align*}
		\int_\Omega \int_\Omega |h^2(u)-h^2(v)| \d{P}_u(v)\d{Q}(u) &= 2 \int_\Omega \int_{A(h^2(u))} (h^2(v)-h^2(u)) \d{P}_u(v)\d{Q}(u) \\
			&= 2 \int_\Omega \int_{h^2(u)}^\infty P_u(A(t)) \d{t}\d{Q}(u) & (\text{by Fubini's Theorem}) \\
			&= 2 \int_0^\infty \int_{\Omega\setminus A(t)} P_u(A(t)) \d{Q}(u)\d{t} \\
			&\geq 2\Phi \int_0^\infty Q(A(t))\d{t} = 2\Phi \int_\Omega h^2\d{Q} = 2\Phi\norm{h}^2.
	\end{align*}
	The result follows directly, since we now have
	\[ \int_\Omega \int_\Omega (h(u)-h(v))^2 \d{P}_u(v)\d{Q}(u) \geq \frac{4\Phi^2\norm{h}^4}{4\norm{h}^2} = 2\Phi^2\norm{h}^2 \geq \Phi^2\norm{g}^2, \]
	which is exactly what we set out to show.
\end{proof}

\begin{corollary}
	Let $\mathcal{M}$ be a time-reversible Markov scheme with conductance $\Phi$. Then for every $f\in L^2$ with $\expec[f]=0$,
	\[ \langle f,M^k f\rangle \leq \left(1-\frac{\Phi^2}{2}\right)^k\norm{f}^2. \]
\end{corollary}

We omit the proof of the above.\footnote{It may be shown by considering $\tilde{M}$, the restriction of $M$ to the invariant subspace $\expec[f]=0$. By the above lemma, $\norm{\tilde{M}}\leq 1-\Phi^2/2$. Then $\norm{\tilde{M}^k} \leq \norm{\tilde{M}}^k$, which imediately gives the result.} This quite neatly captures the fact that rapid mixing depends heavily on conductance. Indeed, it implies that $\norm{M^k f} \leq (1-\Phi^2/2)^k \norm{f}$, so as time progresses, $f$ ``flattens out" and goes closer to $0$.

The next inequality can be thought of a central limit theorem style inequality.

\begin{theorem}
	Let $\mathcal{M}$ be a time-reversible Markov scheme with stationary distribution $Q$ and let $w_1,w_2,\ldots$ be a Markov chain generated by $\mathcal{M}$ with initial distribution $Q$. Let $F\in L^2$ and $\xi = \sum_{i=0}^{T-1}F(w_i)$ for some $T$. Then,
	\[ \Var[\xi] \leq \frac{4T}{\Phi^2}\norm{F}^2 \]
\end{theorem}
\begin{proof}
	We may assume that $\expec[\xi] = 0$. Then \Cref{rapid mixing expec 0} implies that
	\begin{align*}
		\Var[\xi] = \expec[\xi^2] &= \sum_{0\leq i,j\leq T-1} \expec[F(w_i)F(w_j)] \\
			&= T\expec[F(w_0)^2] + 2 \sum_{0\leq i<j\leq T-1} \expec[F(w_0)F(w_{|i-j|}] \\
			&= T\norm{F}^2 + 2 \sum_{0\leq i<j\leq T-1} \expec[F(w_0)F(w_{|i-j|}] \\
			&= T\norm{F}^2 + 2 \sum_{k=1}^{T-1} (T-k) \langle F,M^{k}F\rangle \\
			&\leq \norm{F}^2 \left(T + 2 \sum_{k=1}^{T-1} (T-k)\left(1-\frac{\Phi^2}{2}\right)^k\right) \\
			&< 2T\norm{F}^2 \sum_{k=0}^{T-1} \left(1-\frac{\Phi^2}{2}\right)^k \leq \frac{4T}{\Phi^2} \norm{F}^2.
	\end{align*}
\end{proof}

\subsubsection{Metropolis Chains}

While we have used Metropolis chains previously, let us define them more formally for the sake of completeness.

\begin{fdef}[Metropolis Chain]
	\label{def: metropolis chain}
	Let $\mathcal{M}$ be a time-reversible Markov chain on $(\Omega,\mathcal{A})$ and let $F:\Omega\to\R$ be a non-negative measurable function. Suppose that $\overline{F} = \displaystyle\int_\Omega F\d{Q}$ is finite. Denote by $\mu_F$ the measure with density $F$. We then define the \textit{filtering} of $\mathcal{M}$ by $F$, denoted $\mathcal{M}/F$, as the Markov scheme with transition probabilities
	\[
		P_u^F(A) = 
		\begin{cases}
			\displaystyle\int_A \min\left\{1,\frac{F(v)}{F(u)}\right\}\d{P}_u(v), & u\not\in A, \\
			\displaystyle\int_A \min\left\{1,\frac{F(v)}{F(u)}\right\}\d{P}_u(v) + \ell(u), & u\in A,
		\end{cases}
	\]
	where
	\[ \ell(u) = \int_\Omega \max\left\{0,1-\frac{F(v)}{F(u)}\right\}\d{P}_u(u). \]
\end{fdef}

And as mentioned, this modified chain converges to a distribution proportional to $F$.

\begin{theorem}
	If $\mathcal{M}$ is time-reversible, then $\mathcal{M}/F$ is also time-reversible and has stationary distribution $Q_F = (1/\overline{F})\mu_F$.
\end{theorem}
\begin{proof}
	It suffices to show that for any disjoint measurable sets $A$ and $B$,
	\[ \int_B P_u^F(A)\d{Q}_F(u) = \int_A P_u^F(B) \d{Q}_F(u), \]
	that is,
	\[ \int_B \int_A \min\left\{1,\frac{F(v)}{F(u)}\right\} \frac{F(u)}{\overline{F}} \d{P}_u(v) \d{Q}(u) = \int_A \int_B \min\left\{1,\frac{F(v)}{F(u)}\right\} \frac{F(u)}{\overline{F}} \d{P}_u(v) \d{Q}(u). \]
	Rewriting, we want to show that
	\[ \int_B \int_A \min\left\{F(u),F(v)\right\} \d{P}_u(v) \d{Q}(u) = \int_A \int_B \min\left\{F(u),F(v)\right\} \d{P}_u(v) \d{Q}(u), \]
	but this follows from the time-reversibility of $\mathcal{M}$.
\end{proof}

Before we move on to the main algorithm, we set up some prerequisite results to make the discussion in the subsequent section more natural.


\subsection{An Isoperimetric Inequality}

\subsubsection{Log-Concave Functions}

A function $f:\R^n\to\Rp$ is said to be \textit{log-concave} if for any $x,y\in\R^n$ and $0<\lambda<1$,
\[ f(\lambda x + (1-\lambda)y) \leq f(x)^\lambda f(y)^{1-\lambda}. \]
This just means that $\log f$ is concave. While we did not mention this by name, we discussed similar ideas back in the (multiplicative) Brunn-Minkowski inequality (\ref{eqn multiplicative brunn minkowski}).\\

It is quite obvious that if $f$ and $g$ are log-concave functions, then so are $fg$ and $\min\{f,g\}$. The following is far less obvious however.

\begin{lemma}
\label{convolution of log concave functions is log concave}
	Let $f$ and $g$ be two log-concave functions. If their convolution $h$ defined by $h(x)=\int_{\R^n}g(u)f(x-u)\d{u}$ is well-defined, it is log-concave.
\end{lemma}

Let $F$ be a non-negative integrable funciton on $\R^n$. As in \Cref{conductance isoperimetric inequality}, denote by $\mu_F$ the measure with density $F$. We then get the following corollary.

\begin{corollary}
\label{F(x+K) log concave K convex}
	Let $K\subseteq\R^n$ be a convex body and $F:\R^n\to\R$ a log-concave function. Then $\mu_F(x+K)$ is a log-concave function of $x$.
\end{corollary}

This is quite easily proved by setting $f=F$ and $g=\indic_K$ in \Cref{convolution of log concave functions is log concave}.\\

Setting $K$ to be a rectangle aligned with the axes having edges of length $\varepsilon$ in $k$ directions and $1/\varepsilon$ in the remaining directions gives

\begin{corollary}
	\label{cor: 4.30}
	Let $F:\R^n\to\Rp$ be a log-concave function with finite integral. Then for any subset $\{x_1,\ldots,x_k\}$ of variables, the function
	\[ \int_{\R} \int_{\R} \cdots \int_{\R} F \d{x}_1\ldots\d{x_k} \]
	in the remaining variables is log-concave.
\end{corollary}

Slightly more generally, setting $f=\indic_{K'}$ and $g=\indic_K$, we get that the function $x\mapsto \vol((x+K')\cap K)$ is log-concave.

\begin{corollary}
	Let $K$ and $K'$ be two convex bodies and $t>0$. If $\{x\in\R^n:\vol((x+K')\cap K)>t\}$ has an interior point, then it is convex. In particular, for any $0<s<1$,
	\[ K_s = \{x\in K : \vol((x+K)\cap K) \geq s\vol(K)\} \]
	is a convex body.
\end{corollary}

\subsubsection{A Localization Lemma}

The main result of this section is the following result, which is an improvement of \Cref{conductance isoperimetric inequality}.

\begin{ftheo}
\label{localization lemma}
	Let $g$ and $h$ be upper semi-continuous Lebesgue integrable functions on $\R^n$ such that their integrals on $\R^n$ are positive. Then there exist points $a,b\in\R^n$ and a linear function $\ell:[0,1]\to\Rp$ such that
	\[ \int_0^1 \ell(t)^{n-1}g((1-t)a+tb)\d{t} > 0 \text{ and } \int_0^1 \ell(t)^{n-1}h((1-t)a+tb)\d{t} > 0 \]
\end{ftheo}

Alternatively, this means that if $g$ and $h$ have positive integrals, then there is some truncated cone such that the restriction of $g$ and $h$ to this region have positive integrals.\footnote{Technically, this isn't completely accurate since we only really integrate along the axis of the cone.} It converts a \textit{global} phenomenon (the integrals over $\Rn$ being positive) to a \textit{local} phenomenon (the integrals over a truncated cone being positive).\\

It suffices to consider the case where $g$ and $h$ are continuous. If not, we can find some monotone (strictly) increasing sequence of continuous integrable functions $(g_k)$ and $(h_k)$ that converge to $g$ and $h$. We then have
\[ \lim_{k\to\infty}\int_{\Rn} g_k = \int_{\Rn} g > 0 \]
so for sufficiently large $k$, $\int_{\Rn} g_k > 0$ and likewise, $\int_{\Rn} h_k > 0$. It then suffices to show it for the $g_k$ and $h_k$ for sufficiently large $k$, so we may assume continuity.\\
Further, note that it suffices to show the inequality with $\geq 0$ instead of $>0$. Indeed, we can then apply it to $(g-a)$ and $(h-a)$ for some function $a$ that is everywhere positive and continuous with a sufficiently small integral to get the strict inequality.\\

We continue the proof over two lemmas. In \Cref{localization lemma lemma 2}, we in fact obtain the exact required result with a concave function instead of a linear one. Over the remainder, we refine it to obtain linearity.

\begin{lemma}
\label{localization lemma lemma 1}
	There exists a sequence $K_0\supseteq K_1 \supseteq \cdots$ of convex bodies such that for each $i$,
	\[ \int_{K_i} g(x)\d{x} > 0 \text{ and } \int_{K_i} h(x)\d{x} > 0 \]
	and $K=\bigcap_i K_i$ is a point or a segment.
\end{lemma}
\begin{proof}
	We may choose $K_0$ to be a sufficiently large ball. To find $K_{i+1}$ given $K_i$, we use a ``bisection argument''. We choose a half-space $H$ such that
	\[ \int_{K_i\cap H} g(x)\d{x} = \frac{1}{2} \int_{K_i} g(x)\d{x}. \]
	The hyperplane supporting the half-space is called a ``bisecting hyperplane''. It remains to show that it is possible to choose $H$ such that the $K_i$ shrink to a $0$ or $1$-dimensional body.\\

	\textbf{Claim.} Given any $(n-2)$-dimensional affine subspace $A$, there is a bisecting hyperplane containing it.\\
	This is easily shown by taking any hyperplane containing $A$, rotating it about $A$, and using the continuity of the resulting map that maps a hyperplane to the integral over the half-space on a particular side. This reduces to showing that for a continuous map $g$ from the unit circle in $\R^2$ to $\R$, there exists $x$ on the unit circle such that $f(x)=f(-x)$. This is trivial on modifying the function into a continuous one from $\R\to\R$ and then using the intermediate value property.\footnote{Alternatively, this is a trivial application of the \href{https://en.wikipedia.org/wiki/Borsuk-Ulam_theorem}{Borsuk-Ulam Theorem}.}\\

	Now, choose $A_0,A_1,\ldots$ to be a sequence of the $(n-2)$-dimensional affine subspaces such that at least one of them passes through any point with rational coordinates.\footnote{this is justified since $\Q^n$ is countable.} For each $i$, let $P_i$ be the bisecting hyperplane of $K_i$ that passes through $A_i$ and define $K_{i+1}=A_i\cap H_i$, where $H_i$ is a half-space with supporting hyperplane $P_i$.\\
	We wish to show that $\bigcap_i K_i$ is at most $1$-dimensional. Suppose instead that it is (at least) $2$-dimensional. Then the projection of $K$ onto one of the planes spanned by two coordinates axes (say $x_1$ and $x_2$) must be two-dimensional, and therefore has a rational interior point $(r_1,r_2)$. However, the hyperplane defined by $x_1=r_1$, $x_2=r_2$ is one of the $A_i$, and in this case, $P_i$ bisects $K$, and therefore also $K_{i+1}$, which is a contradiction.
\end{proof}

In the case where the resulting $K$ is a point $a$, it follows that $g(a)\geq 0$ and $h(a)\geq 0$, so \Cref{localization lemma} follows for $b=a$ and $\ell\equiv 1$.\\
Therefore, we assume that $K$ is a segment. Let $a$ and $b$ be the endpoints of this segment.

\begin{lemma}
\label{localization lemma lemma 2}
	There exists a concave function $\psi:[0,1]\to\Rp$, not identically zero, such that
	\[ \int_0^1 \psi(t)^{n-1}g((1-t)a+tb)\d{t} \geq 0 \text{ and } \int_0^1 \psi(t)^{n-1}h((1-t)a+tb)\d{t} \geq 0. \]
\end{lemma}
\begin{proof}
	Assume without loss of generality that $a=0$ and $b=e_1$. For $t\in\R$, set $Z_t = \{x\in\Rn : x_1=t\}$ and for each $i\in\N$,
	\[ \psi_i(t) = \left(\frac{\vol_{n-1}(K_i\cap Z_t)}{\vol(K_i)}\right)^{1/(n-1)}. \]
	Denote by $\alpha_i$ and $\beta_i$ the maximum and minimum of $x_1$ over $K_i$. Since $K\subseteq K_i$, $\alpha_i\geq 1$ and $\beta_i\leq 0$ for each $i$ and moreover, $\alpha_i\to 1$ and $\beta_i\to 0$. Also observe that by \nameref{brunn's theorem}, each $\psi_i$ is concave on $[\beta_i,\alpha_i]$.\\
	Note that on any closed subinterval $[s,t]\subseteq(0,1)\subseteq[\beta_i,\alpha_i]$, the $(\psi_i)$ are Lipschitz (due to concavity).\footnote{See \href{https://math.stackexchange.com/a/2662341/447210}{this StackExchange answer} if you are unfamiliar with the result.} In particular, if $\delta\leq s\leq t\leq 1-\delta$, for any $s\leq x\leq y\leq t$,
	\[ |\psi_i(y)-\psi_i(x)| \leq \frac{\sup_{x\in (0,1)} \psi_i(x)}{\delta} |y-x|. \]
	The above implies that if we show uniform boundedness of the $(\psi_i)$ on $[s,t]$, then (uniform) equicontinuity follows as well. And indeed, since the $(\psi_i)$ are concave, then letting $\psi_i$ attain its supremum in $[s,t]$ at $x_i$, we have 
	\[ 1 \geq \int_s^t \psi_i(t)^{n-1}\d{t} \geq \int_{(s+x_i)/2}^{(x_i+t)/2} \left(\frac{\sup_{x\in[s,t]} \psi_i(x)}{2}\right)^{n-1}\d{t}, \]
	which, after a straightforward simplification, implies uniform boundedness.\\
	Therefore, by the Arzel\'a-Ascoli Theorem, the $(\psi_i)$ have a uniformly convergent subsequence on any $[s,t]$; let $\psi$ be the resulting limit function. Extending this appropriately to a function on $[0,1]$, we clearly have that $\psi$ is non-negative, concave, and
	\[ \int_0^1 \psi(t)^{n-1}\d{t} = 1. \]
	Now, setting $x=(t,y)$ for $t\in\R$ and $y\in\R^{n-1}$,
	\begin{align*}
		\int_{K_i} g(x)\d{x} &= \int_{\beta_i}^{\alpha_i} \int_{K_i\cap Z_t} g(t,y)\d{y}\d{t} \\
			&= \int_{\beta_i}^{\alpha_i} \left( \frac{1}{\vol_{n-1}(K_i\cap Z_t)} \int_{K_i\cap Z_t} g(t,y)\d{y} \right) \vol(K_i) \psi_i(t)^{n-1} \d{t}. \\
		\frac{1}{\vol(K_i)}\int_{K_i} g(x)\d{x} &= \int_{\beta_i}^{\alpha_i} \left( \frac{1}{\vol_{n-1}(K_i\cap Z_t)} \int_{K_i\cap Z_t} g(t,y)\d{y} \right) \psi_i(t)^{n-1} \d{t}.
	\end{align*}
	By definition, the left hand side is non-negative and
	\[ \int_{\beta_i}^{\alpha_i} \left( \frac{1}{\vol_{n-1}(K_i\cap Z_t)} \int_{K_i\cap Z_t} g(t,y)\d{y} \right) \psi_i(t)^{n-1} \d{t} \to \int_0^1 g(t,0)\psi(t)^{n-1}\d{t}. \]
	Therefore, this integral is non-negative and the claim is proved (the expression for $h$ is non-negative by an identical argument).
\end{proof}

Let us get back to the main proof equipped with the above two intermediate steps.\\
We need to somehow get from the above concave function $\psi$ to a linear function.

\begin{proof}[Continued Proof.]
	Define $\psi$, $a$, and $b$ as in the previous lemma but further, we make two assumptions: 
	\begin{itemize}
		\item Choose $a,b\in K$ such that $\norm{a-b}$ is minimum. When we say ``minimum'', we mean that one of the two integrals involved is equal to $0$, so we cannot make the segment any shorter. Without loss of generality, assume $a=0$ and $b=e_1$.
		\item Let $[\alpha,\beta]\subseteq[0,1]$ be the largest interval that $\psi$ is linear on. That is, $\psi$ is linear on $[\alpha,\beta]$ and $|\beta-\alpha|$ is maximum -- in the case where $\psi$ is nowhere linear, $\beta-\alpha=0$ (How can we assume that such an interval exists?).
	\end{itemize}
	For the sake of simplicity, define $\pi_1$ to be the projection of $x\in\Rn$ onto the first axis ($\pi_1(x)=x_1$). Define the functions $\hat{g},\hat{h}:\Rn\to\R$ by
	\[ \hat{g}(x)=g(\pi_1(x)e_1) \text{ and } \hat{h}(x)=h(\pi_1(x)e_1). \]
	Also, consider the convex body
	\[ K' = \{x\in\Rn : 0\leq x_1\leq 1, x_2,\ldots,x_n\geq 0,\text{ and }x_2+\cdots+x_n\leq\psi(x_1)\}. \]
	The body $K'$ can be visualized more naturally as taking the union of the $(n-1)$-dimensional simplices spanned by $te_1, te_1+\psi(t)e_2,\ldots,te_1+\psi(t)e_n$ over all $0\leq t\leq 1$. As a result,
	\[ \vol_{n-1} (K'\cap Z_t) = \frac{\psi(t)^{n-1}}{n!}, \]
	where $Z_t=\pi_1^{-1}(t)$ is defined as in the previous lemma.\\
	Then observe that
	\[ \int_{K'} \hat{g}(x)\d{x} = \int_0^1 \vol(K' \cap Z_t) g(t e_1) \d{t} = \frac{1}{(n-1)!} \int_0^1 \psi(t)^{n-1} g(te_1) \geq 0, \]
	% *** PAPER SAYS n! ??? ***
	where the last inequality follows from the definition of $\psi$ in the previous lemma. We have a similar inequality for the integral of $\hat{h}$ as well.\\
	Since we have taken $\norm{a-b}$ to be minimal, we may assume without loss of generality that $\int_{K'}\hat{g}(x)\d{x}=0$.\\
	% Now, for any convex body $L$, define the function
	% \[ \psi_L(x) = \vol(L\cap Z_t)^{1/(n-1)}. \]
	% Brunn's Theorem implies that $\psi_L$ is concave.\\
	Consider the $(n-2)$-dimensional affine space $A$ defined by $x_1=\sigma$ and $x_2+\cdots+x_n=\tau$ for some $0<\sigma<1$ and $\tau>1$. Suppose that $A$ intersects the interior of $K'$. Then by \Cref{localization lemma lemma 1}, there exists a hyperplane $H$ through $A$ that splits $K'$ into convex bodies $L_A$ and $L_A'$ such that
	\[ \int_{L_A}\hat{g}(x)\d{x} = \int_{L_A'}\hat{g}(x)\d{x} = 0 \text{ and } \int_{L_A}\hat{h}(x)\d{x} \geq 0. \]
	Note that
	\begin{equation*}
	\tag{$*$}
	\label{eqn: localization lemma intermediate 1}
		L_A'\cap (Z_0\cap K')\neq\emptyset \text{ and } L_A'\cap (Z_1\cap K')\neq\emptyset
	\end{equation*}
	since otherwise, the minimality of $\norm{a-b}$ is contradicted.\footnote{we can then restrict ourselves to $L_A$ instead of $K'$ while maintaining non-negativity of the two integrals.} As a consequence, $H$ cannot be orthogonal to the $x_1$-axis and so, it can be described as $x_2+\cdots+x_n=\ell(x_1)$ for some linear function $\ell$ (due to the structure of the $(n-2)$-dimensional affine space $A$ it contains). Due to (\ref{eqn: localization lemma intermediate 1}), this $\ell$ must also satisfy $\ell(0)\geq 0$ and $\ell(1)\geq 0$. Further, observe that we can succinctly describe $L_A\cap K'$ as
	\[ \left\{x\in\Rn : 0 \leq x_1 \leq 1, x_2,\ldots,x_n\geq 0, \text{ and } x_2+\cdots+x_n \leq \psi'(t) \right\}, \]
	where
	\[ \psi'(t) = \min\{\psi(t),\ell(t)\} \]
	is concave as well.\\

	Now, since $\psi$ is concave on $[0,1]$, it is continuous on $(0,1)$. Therefore, if there is a discontinuity, it must be at either $0$ or $1$. Based on this, we take $3$ cases and in each, construct an $A$ that yields the desired linear function.
	\begin{itemize}
		\item \textbf{Case 1.} $\psi(0)=\psi(1)=0$. Consider the affine space $A$ for $\sigma=1/2$. Due to (\ref{eqn: localization lemma intermediate 1}), $(1/2) e_1 \in L_A$ (Why?). Since $\ell(0),\ell(1)\geq 0$ and $\ell(1/2)=\tau$, as we take $\tau\to 0$, $\ell$ tends to $0$ \textit{uniformly} on $[0,1]$. Making $\tau$ sufficiently small, we see that $\psi'$ satisfies the required condition and becomes linear on a length tending to $1$ (it becomes equal to $\ell$ for a larger portion as $\tau$ becomes smaller and smaller).

		\item \textbf{Case 2.} $\psi(0)=0$ and $\psi(1)>0$ (the reversed case is nearly identical). Consider the affine space $A$ for $\tau=\psi(1)\sigma$. (\ref{eqn: localization lemma intermediate 1}) implies that $0\leq\ell(1)\leq\psi(1)$. However, in this case, $\psi'$ must be linear on $[\sigma,1]$, so we can obtain linearity on a length tending to $1$.

		\item \textbf{Case 3.} $\psi(0),\psi(1)>0$.\footnote{While slightly irrelevant, it might be worth noting that we could try applying the same strategy as in Case 2; however, an issue arises because instead of $[\sigma,1]$, we only get the smaller of the two intervals $[0,\sigma]$ and $[\sigma,1]$.} Consider a convex(!) function $\eta:[0,1]\to\R^{\geq 0}$ such that
		\begin{itemize}
			\item $0\leq\eta(0)\leq\psi(0)$ and $0\leq\eta(1)\leq\psi(1)$,
			\item the convex body $K_\eta$ defined by
			\[ K_\eta = \{ x\in\Rn : 0\leq x_1\leq 1, x_2,\ldots,x_n\geq 0, \text{ and } \eta(x_1)\leq x_2+\cdots+x_n\leq\psi(x_1) \} \]
			satisfies
			\[ \int_{K_\eta} \hat{g}(x)\d{x} = 0 \text{ and } \int_{K_\eta} \hat{h}(x)\d{x}\geq 0, \text{ and } \]
			\item $\displaystyle\int_0^1 \eta(x)\d{x}$ is maximal.
		\end{itemize}
		Such an $\eta$ must exist since the zero function satisfies the first two conditions.\\
		If $\psi(0)=0$ or $\psi(1)=1$, then we are done, since $\psi-\eta$ satisfies the original conditions (non-negative integrals), and it is already covered in the first two cases.\\
		Otherwise, we have $\eta(0)<\psi(0)$ and $\eta(1)<\psi(1)$. Let $(\sigma,\tau)$ be the intersection point of the segments joining $(0,\eta(0))$, $(1,\psi(1))$ and $(0,\psi(0))$, $(1,\eta(1))$ and $A$ be the corresponding affine space. Let $H$ be a hyperplane containing $A$ dividing space into two half-spaces $M$ and $M'$ such that
		\[ \int_{K_\eta \cap M} \hat{g}(x)\d{x} = \int_{K_\eta \cap M'} \hat{g}(x)\d{x} = 0.  \]
		Also, choose $M$ to be the half-space that contains $(1/2) e_1$ (the one that ``faces down'').\\
		It is clear from contruction that any $(n-1)$-dimensional hyperplane containing $A$, $H$ in particular, either intersects both $Z_0\cap K_\eta$ and $Z_1\cap K_\eta$ or neither. The latter cannot happen due to (\ref{eqn: localization lemma intermediate 1}) (or rather, the corresponding expression for $K_\eta$ rather than $K'$).\\
		Now, note that
		\[ \int_{M'} \hat{h}(x)\d{x} < 0. \]
		Indeed, otherwise, the linear function $\ell$ corresponding to $H$ contradicts the maximality of $\eta$. However, this implies that
		\[ \int_{M\cap K'} \hat{g}(x)\d{x} = \int_{K'} \hat{g}(x)\d{x} - \int_{M'}\hat{g}(x)\d{x} = 0 \]
		and
		\[ \int_{M\cap K'} \hat{h}(x)\d{x} = \int_{K'} \hat{h}(x)\d{x} - \int_{M'}\hat{h}(x)\d{x} > 0. \]
		Finally, note that $M\cap K'$ is a truncated cone, thus completing the proof.\qedhere
	\end{itemize}
\end{proof}

The above result has some interesting corollaries, namely \Cref{improvement of conductance isoperimetric inequality,inequality integral setminus ball}, which are of interest to us. The following is an improvement of \Cref{conductance isoperimetric inequality}.

\begin{ftheo}
\label{improvement of conductance isoperimetric inequality}
	Let $K\subseteq\Rn$ be a convex body, $0<t<1$, and $K_1,K_2$ be two measurable sets in $K$ such that for any $a,b\in K$, the distance between $K_1\cap[a,b]$ and $K_2\cap[a,b]$ is at least $t\norm{a-b}$. Then for any log-concave function $F$ with support $K$,
	\[ \mu_F(M\setminus(K_1\cup K_2)) \geq \frac{2t}{1-t} \min\{\mu_F(K_1),\mu_F(K_2)\}. \]
\end{ftheo}

The above bound is tight when $K$ is a cylinder and $F$ is identically $1$. Note that $t \geq d(K_1, K_2)/\diam(K)$ (thus explaining why this is an improvement of \Cref{conductance isoperimetric inequality}).

\begin{proof}
	Suppose otherwise. Assume that $K_1$ and $K_2$ are open (otherwise, we can delete the boundary and enlarge them slightly to open sets while preserving both the assumptions and the inequality). Denote $K_3 = M\setminus(K_1\cup K_2)$. Define
	\[
		g(x) = 
		\begin{cases}
			F(x), & x\in K_1, \\
			\frac{1-t}{2t}F(x), & x\in K_3, \\
			0, & \text{otherwise},
		\end{cases}
		\text{ and }
		h(x) = 
		\begin{cases}
			F(x), & x\in K_2, \\
			\frac{1-t}{2t}F(x), & x\in K_3, \\
			0, & \text{otherwise}.
		\end{cases}
	\]
	By our assumption, the integrals of $g$ and $h$ over $\Rn$ are positive. We can then apply \Cref{localization lemma} to get a linear functional $\ell$ and $a,b\in\Rn$ such that
	\[ \int_0^1 \ell(u)^{n-1}g(ua+(1-u)b) > 0 \text{ and } \int_0^1 \ell(u)^{n-1}h(ua+(1-u)b) > 0. \]
	For each $i$, define
	\[ H_i = \{u : ua+(1-u)b \in K_i\} \text{ and } G = \ell(u)^{n-1}F(ua+(1-u)b). \]
	Substituting $g$ and $h$ in the above equation, we get
	\begin{equation}
	\label{eqn: cond iso ineq imp contradictory eqn}
		\int_{H_3} G(u)\d{u} < \frac{2t}{1-t} \min\left\{\int_{H_1} G(u)\d{u}, \int_{H_2} G(u)\d{u}\right\}.
	\end{equation}
	Intuitively, the worst case to prove appears to be when $H_3$ is a single interval -- as we shall see later in the proof, we can reduce it to this case even in general. To prove this case, we show the following claim, which just asserts that \Cref{eqn: cond iso ineq imp contradictory eqn} is incorrect. Define $\mu_G$ to be measure on $[0,1]$ with density $G$.\\

	\textbf{Claim.} For $0\leq s < s+t\leq 1$,
	\begin{equation}
	\label{eqn: 4.15}
		\mu_G([s,s+t]) \geq \frac{2t}{1-t} \min\{\mu_G([0,s]),\mu_G([s+t,1])\}.	
	\end{equation}

	Before we move to the proof, note that \Cref{conductance isoperimetric inequality}, which has a $t$ in place of $2t/(1-t)$ above, is trivial now. Indeed, since $G$ is unimodal\footnote{there is some $m\in[0,1]$ such that $G$ is monotonically increasing for $x\leq m$ and monotonically decreasing for $x\geq m$.},
	\[ \mu_G([s,s+t]) \geq t \inf_{x\in[s,s+t]} G(x) \geq t \max\{s, 1-s-t\}\inf_{x\in[s,s+t]}G(x) \geq t \min\{\mu_G([0,s]),\mu_G([s+t,1])\}. \]
	First, note that since $G$ is the product of two log-concave functions, it is log-concave as well. Choose constants $c,c_0$ such that $G(s)=c_0 e^{cs}$ and $G(s+t)=c_0 e^{c(s+t)}$. By the log-concavity of $G$, $G(u)\geq c_0 e^{cu}$ for $s\leq u\leq s+t$ and $G(u)\leq c_0 e^{cu}$ elsewhere. As a consequence, observe that it suffices to show \Cref{eqn: 4.15} for the case where $G(u) = c_0 e^{cu}$. Further, we may assume that $c_0=1$. We wish to show that
	\[ e^{c(s+t)}-e^{cs} \geq \frac{2t}{1-t} \min\left\{e^{cs}-1, e^c - e^{c(s+t)}\right\}. \]
	The worst case is when $e^{cs}-1 = e^{c} - e^{c(s+t)}$ (Why?). That is,
	\[ e^{cs} = \frac{1+e^c}{1+e^{ct}}. \]
	We then aim to show that
	\[ (e^{ct}-1)\frac{1+e^c}{1+e^{ct}} \geq \frac{2t}{1-t} \left(\frac{1+e^c}{1+e^{ct}} - 1\right). \]
	Let $x=e^{ct}>1$ and $\lambda=1/t>1$. The equation then becomes
	\[ (x-1)(1+x^\lambda) \geq \frac{2}{\lambda-1}(x^\lambda - x), \]
	that is,
	\[ (\lambda+1)(x-x^\lambda) + (\lambda-1)(x^{\lambda+1}-1) \geq 0. \]
	Letting the expression on the left be $h(x)$, we get $h(1)=h'(1)=0$ and $h''(x)=(\lambda^2-1)\lambda(x^{\lambda-1}-x^{\lambda-2}) \geq 0$, thus proving the claim for all $x\geq 1$.\\

	Now, how do we reduce the general case problem to this? For each maximal interval $I=(a,b)$ contained in $H_3$ of length at least $t$, colour $[0,a]$ red if $\mu_G([0,a])<\mu_G([b,1])$ and $[b,1]$ red otherwise. The above claim implies that each interval introduces a red set of ($\mu_G$) measure at most $\frac{1-t}{2t}\mu_G((a,b))$. The entirety of the red set then has measure at most $\frac{1-t}{2t}\mu_G(H_3)$. It then suffices to show that either $H_1$ or $H_2$ is completely red.\\
	Suppose otherwise. Denote by $U$ the region that is uncoloured. We then have that $U$ intersects both $H_1$ and $H_2$. By construction, $U$ is an open interval. Since the distance between $H_1$ and $H_2$ is at least $t$, $U\cap H_3$ must contain a subinterval of length at least $t$. This is a contradiction because there is an adjacent red interval (that intersects $U$). 
\end{proof}

Setting $F$ to be identically $1$ on $K$ in the above result, we get the following.

\begin{corollary}
	Let $K\subseteq\Rn$ be a convex body, $0<t<1$, and $K_1,K_2$ be two measurable sets in $K$ such that for any $a,b\in K$, the distance between $K_1\cap[a,b]$ and $K_2\cap[a,b]$ is at least $t\norm{a-b}$. Then
	\[ \vol(M\setminus(K_1\cup K_2)) \geq \frac{2t}{1-t} \min\{\vol(K_1),\vol(K_2)\}. \]
	In particular, if $K$ has diameter $d$ and the distance between $K_1$ and $K_2$ is at least $1$,
	\[ \vol(M\setminus(K_1\cup K_2)) \geq \frac{2}{d-1} \min\{\vol(K_1),\vol(K_2)\}. \]
\end{corollary}

\begin{theorem}
\label{inequality integral setminus ball}
	Let $F$ be a log-concave function on $\Rn$ and $\theta\leq 1$ such that
	\[ \int_{\Rn \setminus B_2^n} F(x)\d{x} = \theta \int_{\Rn} F(x)\d{x}. \]
	Then for every $u\geq 1$,
	\[ \int_{\Rn \setminus uB_2^n} F(x)\d{x} \leq \theta^{(u+1)/2} \int_{\Rn} F(x)\d{x}. \]
\end{theorem}
\begin{proof}
	Suppose otherwise. Then exactly as we did in the proof of \Cref{improvement of conductance isoperimetric inequality}, there exist $a,b\in\Rn$ and a linear function $\ell:[0,1]\to\Rp$ such that on setting
	\begin{align*}
		G(t) &= \ell(t)^{n-1}F((1-t)a+tb),\\
		H_1 &= \{t\in[0,1]: (1-t)a+tb\in B_2^n\},\\
		H_2 &= \{t\in[0,1]: (1-t)a+tb\in uB_2^n\setminus B_2^n\},\text{ and }\\
		H_3 &= [0,1]\setminus(H_1\cup H_2),
	\end{align*}
	we have
	\begin{equation}
	\label{eqn: 4.16}
		\int_{H_2\cup H_3} G(t)\d{t} \leq \theta\int_0^1 G(t)\d{t}
	\end{equation}
	and
	\begin{equation}
	\label{eqn: 4.17}
		\int_{H_3} G(t)\d{t} > \theta^{(u+1)/2}\int_0^1 G(t)\d{t}
	\end{equation}
	As seen before, $G$ is log-concave. Further, note that $H_1$ is an interval and $H_2,H_3$ consist of $1$ or $2$ intervals.\\

	If $0\not\in H_1$, we can choose a point $s\in H_1$ such that
	\[ \frac{\int_{[0,s]\cap H_1} G(t)\d{t}}{\int_{[s,1]\cap H_1} G(t)\d{t}} = \frac{\int_{[0,s]} G(t)\d{t}}{\int_{[s,1]} G(t)\d{t}}. \]
	We can then replace $[a,b]$ with one of $[a,(1-s)a+sb]$ and $[(1-s)a+sb,b]$ (that is, replace $[0,1]$ with $[0,s]$ or $[s,1]$).\footnote{Multiplying by the appropriate factors leaves the \Cref{eqn: 4.16} unchanged in the restricted segment, and one of the two intervals will also satisfy the (restriction of) \Cref{eqn: 4.17}.} Therefore, we may assume wlog that $0\in H_1$, that is, $a\in B_2^n$.\\
	Let $H_1=[0,\alpha]$, $H_2=(\alpha,\beta]$, and $H_3=(\beta,1]$. It is easily shown that $\beta \geq (\frac{u+1}{2})\alpha$.\\

	Now, choose constants $c$ and $c_0$ such that
	\[ \int_0^\alpha G(t)\d{t} = \int_0^\alpha c_0e^{-ct}\d{t} \text{ and } \int_\beta^1 G(t)\d{t} = \int_\alpha^\infty c_0e^{-ct}\d{t}. \]
	Note that the log-concavity (unimodality in particular) of $G$ implies that for all $\alpha\leq t\leq\beta$, $G(t)\geq c_0e^{-ct}$. Therefore,
	\[ \int_0^1 G(t)\d{t} \geq \int_0^\infty c_0e^{-ct}\d{t}. \]
	However, \Cref{eqn: 4.16} then implies that
	\begin{align*}
		\theta &\geq \frac{\int_\alpha^1 G(t)\d{t}}{\int_0^1 G(t)\d{t}} \\
			&= 1 - \frac{\int_0^\alpha G(t)\d{t}}{\int_0^1 G(t)\d{t}} \\
			&\geq 1 - \frac{\int_0^\alpha c_0e^{-ct}\d{t}}{\int_0^\infty c_0e^{-ct}\d{t}} \\
			&= \frac{\int_\alpha^\infty c_0e^{-ct}\d{t}}{\int_0^\infty c_0e^{-ct}\d{t}} = e^{-c\alpha}.
	\end{align*}
	Using the above, we now have
	\begin{align*}
		\theta^{(u+1)/2} &\geq e^{-c\alpha(u+1)/2} \\
			&\geq e^{-c\beta} \\
			&= \frac{\int_\beta^\infty c_0e^{ct}\d{t}}{\int_0^\infty c_0e^{ct}\d{t}} \geq \frac{\int_\beta^1 G(t)\d{t}}{\int_0^1 G(t)\d{t}},
	\end{align*}
	thus resulting in a contradiction to \Cref{eqn: 4.17} and thus proving the claim.
\end{proof}

% Setting $F$ to be identically $1$ in the above result, we get the following.

% \begin{corollary}
% 	Let $K$ be a convex body in $\Rn$ and $\theta=\vol(K\setminus B_2^n)/\vol(K)$. Then $\vol(K\setminus uB_2^n)\leq\theta^{(u+1)/2}\vol(K)$.	
% \end{corollary}
% *** HOW??? ***

Before we conclude this section, we give two more lemmas that will be useful in the future.

\begin{lemma}
	Let $K$ be a convex body and $\theta=\vol(K\setminus B_2^n)/\vol(K)$. Then $K\subseteq \frac{2n}{1-\theta}B_2^n$.
\end{lemma}
\begin{proof}
	Let $x\in K$ be the point farthest from the origin and $R=\norm{x}$. It suffices to show the result for $\conv{(K\cap B_2^n)\cup\{x\}}$ (Why?). Therefore, assume $K$ to be of this form.\\
	Note that $K$ is contained in
	\[ \left\{x+\left(\frac{R+1}{R-1}\right) v : x+v\in K\setminus B_2^n\right\}. \]
	Then,
	\[ \vol(K) \leq \left(\frac{R+1}{R-1}\right)^n \vol(K\setminus B_2^n) \leq \left(\frac{R+1}{R-1}\right)^n \vol(K). \]
	That is,
	\[ R \leq \frac{2}{1-\theta^{1/n}} \leq \frac{2n}{1-\theta}.\qedhere \]
\end{proof}

\begin{lemma}
	Let $0\leq t\leq 1$. If $\vol(K\setminus (x+K)) \leq 1/2 \vol(K)$, then $\vol(K\setminus (tx+K)) \leq (2t/3) \vol(K)$.
\end{lemma}
\begin{proof}
	Define $\psi:[0,1]\to\R$ by
	\[ \psi(u) = \frac{\vol(K\cap (ux+K))}{\vol(K)}. \]
	\Cref{cor: 4.30} (or rather, the line right after it) implies that $\psi$ is log-concave. We have $\psi(0)=1$ and $\psi(1)\leq 1/2$. Therefore, $\psi(t) \leq 2^{-t}$ so
	\[ \vol(K\setminus (tx+K)) = (1 - 2^{-t})\vol(K) \leq \frac{2t}{3}\vol(K).\qedhere \]
\end{proof}


\subsection{An \texorpdfstring{$\mathcal{O}^*(n^7)$}{O(n7)} Algorithm using the Ball-Step}

As in nearly all volume estimation algorithms, the basic idea remains the same, the changes being only in the walk. The algorithm used here involves the ``ball-step'' mentioned earlier. It is worth noting instead of the ``ball'' in ball-step, one could use any other symmetric convex body $G$. An obvious choice is the cube, which is quite convenient to draw points uniformly randomly from from a programming perspective. Our analysis shall be done in this general case.

\subsubsection{Ball-Step}

The basic walk $\mathcal{M}$ is as follows: when we are at $v_k$, we let $v_{k+1}=v_k$ with probability $1/2$. Otherwise, we generate a random vector $u$ from the uniform distribution on $G$. If $v_k+u\in K$, set $v_{k+1}=v_k+u$ and otherwise, set $v_{k+1}=v_k$. This is termed as the \textit{lazy random walk in $K$ with $G$-steps}.\\
In this particular algorithm, we further filter it to obtain a Metropolis chain (recall that we defined this in \Cref{def: metropolis chain}). That is, for any measurable $A$ such that $x\not\in A$,
\[ P_x(A) = \frac{1}{2\vol(G)} \int_{(x+G)\cap A} \min\left\{1,\frac{F(y)}{F(x)}\right\}\d{y} = \frac{1}{2\vol(G)} \int_{(x+G)\cap A} \min\left\{\frac{1}{F(y)},\frac{1}{F(x)}\right\}\d{\mu}_F(y) \]
and
\[ P_x(x) = \frac{1}{2} + \frac{1}{2\vol(G)} \int_{x+G} \max\left\{0,1-\frac{F(y)}{F(x)}\right\}\d{y}. \]
The stationary distribution of the above chain is that 

Recall from the discussion after \Cref{def: conductance} that we denote by $H_t$ (for $0\leq t\leq 1/2$) the set of points such that
\[ \int_{x+G} \min\{F(x),F(y)\}\d{y} < tF(x)\vol(G). \]
We also saw that the $(Q_F(H_t)/2)$-conductance of the chain is at most $2t$.\\

The main result of this section, which is a sort of localization lemma in the context of this problem, says that if the local conductance is large and the diameter (not the usual diameter) of $K$ is small, then the overall conductance is large.

\begin{ftheo}
	Let $0\leq t\leq 1/2$, $0<\theta<1$, and $s=Q_F(H_t)$. Assume that for all $x,y\in K$,
	\[ \vol(G\cap (\theta(x-y)+G)) \geq \frac{1}{2}\vol(G). \]
	Then the $(7s/t)$-conductance of $\mathcal{M}(K,G)/F$ is at least $(1/6)t^2\theta$.
\end{ftheo}

\begin{proof}
	Alternatively, the above result states that
	\[ \int_{S_1} \int_{S_2\cap (x+G)} \min\{F(x),F(y)\}\d{y}\d{x} \geq \frac{t^2\theta\vol(G)}{6} \min\left\{Q_F(S_1)-\frac{7s}{t}, Q_F(S_2)-\frac{7s}{t}\right\}. \]
	We may assume that $Q_F(S_1)$ and $Q_F(S_2)$ are both greater than $7s/t$.
\end{proof}


% Drawing uniformly randomly from the ball is not too difficult. Letting $\xi_1,\ldots,\xi_n$ be iid standard normal distributions and $\eta$ be uniformly distributed in $[0,1]$, we see that
% \[ v_0 = \left( \eta^{1/n} \frac{\xi_1}{\sqrt{\sum_i \xi_i^2}}, \ldots, \eta^{1/n} \frac{\xi_n}{\sqrt{\sum_i \xi_i^2}} \right) \]
% is uniformly distributed in $B_2^n$.\\

% \subsubsection{The Walk}

% We modify the $G$-walk described above into a suitable Metropolis chain, with the primary function being quite similar to that we used in the $\mathcal{O}^*(n^8)$ algorithm described in a previous section. Define
% \[ \phi_K(x) = \min\{t\geq 0 : x\in tK\} \]
% and $F_K(x)=e^{-\phi_K(x)}$. Clearly, $0<F_K \leq 1$. We often refer to these functions as just $\phi$ and $F$ if it is clear from context what convex body we are talking about. First of all, we can use \Cref{find sphere volume,eqn: volume in terms of radial distance} to get
% \[ \int_{\R^n} F = n\vol(K)\int_0^\infty t^{n-1}e^{-t}\d{t}. \]
% That is,
% \begin{equation}
% 	\vol(K) = \frac{1}{n!} \int_{\R^n} F.
% \end{equation}

% Now, define
% \[ \lambda(s) = \frac{1}{s}\left(\frac{1}{(n-1)!}\int_0^s e^{-t}t^{n-1}\d{t}\right)^{1/n}. \]
% \begin{lemma}
% 	Let $v$ be randomly distributed in $\R^n$ with density $e^{-\phi(v)}/(n-1)!$. Then
% 	\[ H(v) = \lambda(\phi(v))v \]
% 	is uniformly distributed on $K$.
% \end{lemma}
% \begin{proof}
	
% \end{proof}

% \subsubsection{Bounding the Conductance of the Walk}

% As before, we shall determine how rapidly the walk converges by bounding the conductance.