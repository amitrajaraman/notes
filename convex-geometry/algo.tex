\section{Computing Volume in High Dimensions}

A very popular problem in high-dimensional convex geometry is that of determining the volume of an arbitrary body.\\

For a fixed dimension $n$, this problem isn't too difficult if we want to measure it up to some precision $\varepsilon$. We could assume that the body $K$ is enclosed in a box $B=\bigtimes_{i\leq n}[a_i,b_i]$, subdivide this box up to precision $\varepsilon$, and count how many subdivided boxes have non-empty intersection with $K$. This (after normalizing appropriately) can be considered efficient in a fixed dimension, where polynomiality is measure in $\frac{1}{\varepsilon}$. If we measure it in $n$ on the other hand, this method is useless.\\
We are looking for algorithms that are efficient (polynomial) in the \textit{dimension} $n$.\\

There are a few issues that arise when we even want to formulate this problem.
\begin{itemize}
	\item What does it mean when we say that a convex body $K$ is ``given"? In what form is it given?
	\item What does ``efficient" exactly mean?
\end{itemize}

We have already answered the second question above -- we are looking for algorithms that are \textit{polynomial} in the dimension $n$. We either want the exact volume or an approximation up to some small \textit{relative error} $\varepsilon>0$. If it is the latter, we would also like the algorithm to be polynomial in $\frac{1}{\varepsilon}$.\footnote{Note that $\varepsilon$ takes only $\log(1/\varepsilon)$ bits, so this is a relaxation in some sense.}

\subsection{Sandwiching and Deterministic Algorithms}
\label{subsec: sandwiching and deterministic algorithms}

Let us answer the first question that we mentioned -- how is an arbitrary body $K$ represented? What information do we have access to?

\subsubsection{Oracles}

We represent the body using an \textit{oracle}. We explain the different types of oracles one may consider over the course of this section.

\begin{definition}
A body $K$ is said to be \textit{well-guaranteed} if it contains $r B_2^n$ and is contained in $R B_2^n$ for some $r,R>0$.
\end{definition}

We restrict ourselves to well-guaranteed bodies since otherwise, we may ask any (finite) number of questions about the body (say of the form ``is $x\in K$") and receive a \texttt{no} every time. This doesn't allow us to make any useful inferences about $\vol(K)$. The fact that $K$ contains $r B_2^n$ ensures that it isn't too small and the containment in $R B_2^n$ ensures that it isn't ``at infinity".\\

A body $K$ is given by an \textit{oracle} $K$ if we know nothing about it other than the fact that it is well-guaranteed with $r$ and $R$ and we may ask questions about $K$, and receive answers to said questions. Depending on the questions and answers, we get different types of oracles.\\
We primarily use \textit{weak separation oracles} and \textit{strong membership oracles}.

\begin{definition}[Strong Membership Oracle]
For a fixed convex body $K$, the \textit{strong membership oracle} (correctly) answers questions of the form ``Is $x\in K$?".
\end{definition}

Now, for $x\not\in K$, we know that there is a hyperplane separating them by \Cref{hyperplane separation theorem}. This gives rise to the strong separation oracle.

\begin{definition}[Strong Separation Oracle]
For a fixed convex body $K$, the \textit{strong separation oracle} (correctly) answers questions of the form ``Is $x\in K$?". If the answer is \texttt{no}, it also returns a hyperplane $S$ separating $x$ from $K$.
\end{definition}
This hyperplane is returned as a vector $s\in\R^n$ with $\norm{s}_\infty=1$ such that $\langle s,x\rangle > 1$ and $\langle s,y\rangle\leq 1$ for any $y\in K$.\\
This leads to the weak separation oracle.

\begin{definition}[Weak Separation Oracle]
For a fixed convex body $K$, we can fix an $\varepsilon>0$ and ask the \textit{weak separation oracle} questions of the form ``Is $x\in K$ for the positive number $\varepsilon$?". However, in this case, the precision of the answer is $\varepsilon$ in the sense that
\begin{enumerate}[(i)]
	\item If $d(x,\partial K)<\varepsilon$, we can get any answer.
	\item If $B(x,\varepsilon)\subseteq K$, we get the correct answer (\texttt{yes}).
	\item If $d(x,K)\geq\varepsilon$, we get the correct answer (\texttt{no}) and a vector $s$ normalized by $\norm{s}_\infty=1$ for which $\langle s,y\rangle < \langle s,x\rangle+\varepsilon$ for every $y\in K$.
\end{enumerate}
\end{definition}

A \textit{weak membership oracle} is similar, where if it is within $\varepsilon$ of $\partial K$, it may return either answer and if it is farther than $\varepsilon$, it returns the correct answer.\\

The complexity of the algorithm is measured in the number of calls to the oracle, since this is usually the most expensive step.

\subsubsection{Sandwiching}
\label{sandwiching}

We earlier mentioned that we only consider well-guaranteed bodies. As might be expected, the ratio $R/r$ is quite important. Defining it slightly more concretely,

\begin{definition}
Given a convex body $K$, let $\mathcal{E}$ an ellipsoid centered at $0$ such that $\mathcal{E}\subseteq K\subseteq d\mathcal{E}$ for some $d\geq 1$. We are then said to have a \textit{sandwiching} of $K$ with \textit{sandwiching ratio} $d$.
\end{definition}

We are given a sandwiching of sandwiching ratio $R/r$ initially. It is natural to want to obtain a sandwiching that has a lower ratio to make whatever algorithm we use more efficient.\\
Further, note that by \Cref{fritz john banach mazur distance}, the minimum possible sandwiching ratio of (an affine transformation of) a body is at most $n$.\\

The information given to us initially ($r$ and $R$) are not even necessarily useful all the time. For example, one could have a very ``pencil-like" body in $\R^n$ such that the inscribed ball is far far smaller than the circumscribed one. Thus, before we even begin our algorithm, we would want to do some preliminary sandwiching -- perform an affine transformation to get a sandwiching with a more manageable sandwiching ratio.\\

Lov\'asz showed in \cite{Lovsz1986AlgorithmicTO} that it is possible to compute an affine transformation $\tilde{K}$ of $K$ in polynomial time such that
\begin{equation}
\label{eqn weak lowner john ellipsoid}
	B_2^n\subseteq \tilde{K} \subseteq (n+1)\sqrt{n} B_2^n.
\end{equation}

We first introduce the following common tool.

\begin{lemma}[Basic ellipsoid method]
For a convex body $K\subseteq\R^n$ along with some $R>0$ such that $K\subseteq RB_2^n$ and a weak separation oracle, it is possible to find a point in $K$ in polynomial time.
\end{lemma}
	We prove it for the case where we have a \textit{strong} separation oracle. The algorithm basically works by cutting down our search space until we find a point.
\begin{proof}
	We construct a sequence $\mathcal{E}_0,\ldots,\mathcal{E}_k$ of ellipsoids with $\mathcal{E}_0 = R B_2^n$. Given $\mathcal{E}_r$, check if its center $x_r$ is contained in $K$. Otherwise, we have a half-space $H_r$ such that $K\subseteq H_r$. We set $\mathcal{E}_{r+1}$ to be the ellipsoid of minimal volume that contains $K\cap H_r$. The sequence terminates when the center of an ellipsoid is contained in $K$.

	It may be shown that
	\[ \vol(\mathcal{E}_{r+1}) = \left(\frac{n}{n+1}\right)^{(n+1)/2}\left(\frac{n}{n-1}\right)^{(n-1)/2} \vol(\mathcal{E}_{r}). \]
	Rewriting it more suggestively,
	\begin{align*}
		\vol(\mathcal{E}_{r+1}) &= \left(\frac{n^2}{n^2-1}\right)^{(n-1)/2} \frac{n}{n+1} \vol(\mathcal{E}_{r}) \\
		&< \left(1+\frac{1}{n^2}\right)^{(n-1)/2} \vol(\mathcal{E}_r) < e^{-1/2n} \vol(\mathcal{E}_r).
	\end{align*}
	The thing to note here is that $e^{-1/2n}$ is independent of the ellipsoids involved. Since we have $K\subseteq \mathcal{E}_k$,
	\[ \vol (K)\leq \vol(\mathcal{E}_k) \leq e^{-k/2n}(2R)^n.  \]
	That is,
	\[ k \leq 2n^2 \log(2R) - 2n \log(\vol (K)) \]
	so there is a polynomial upper bound on the number of steps.
\end{proof}

If each ellipsoid is given by
\[ \mathcal{E}_r = \left\{x\in\R^n : (x-x_k)^\top A_k^{-1} (x-x_k) \leq 1 \right\}, \]
$c_k$ is the vector returned by the separation oracle and $g_k = \frac{1}{\sqrt{c_k^\top A_k c_k}} c_k$, then
\begin{align*}
	x_{k+1} &= x_k - \frac{1}{n+1}A_k g_k\text{ and } \\
	A_{k+1} &= \frac{n^2}{n^2-1} \left( A_k - \frac{2}{n+1}A_k g_k g_k^\top A_k. \right)
\end{align*}
Since there is rounding anyway (irrationals might become involved due to the $\sqrt{\cdot}$), it turns out that it suffices to have a weak separation oracle.\\

A pair of ellipsoids like that in \Cref{eqn weak lowner john ellipsoid} is often known as a \textit{weak L\"owner-John pair} for $K$ (the sandwiching ratio must be $(n+1)\sqrt n$). 

\begin{ftheo}
	\label{lovasz pre-sandwich}
	Let $K\subseteq\R^n$ be a convex body given by a weak separation oracle. Then a weak L\"owner-John pair for $K$ can be computed in polynomial time. 
\end{ftheo}
Again, we prove it for the case where we have a strong separation oracle instead. This algorithm is nearly identical to that of basic ellipsoid method, but at each step we perform a little extra computation to check if the corresponding ellipsoid scaled down by a factor of $(n+1)\sqrt{n}$ is contained in $K$.
\begin{proof}
	We construct a sequence $\mathcal{E}_0,\ldots,\mathcal{E}_k$ of ellipsoids with $\mathcal{E}_0=R B_2^n$. Given $\mathcal{E}_r$, first check if its center $x_r$ is contained in $K$. if it is not, then use the basic ellipsoid method to get an ellipsoid that does; we abuse notation and refer to this as $\mathcal{E}_r$ as well.\\
	Next, let the endpoints of the axes of the ellipsoid be given by $x_r \pm a_i$ (for $1\leq i\leq n$). Check if the $2n$ points $x_r \pm \frac{1}{n+1} a_i$ are in $K$ for each $i$. If they all are, then we are done, since this implies that the convex hull of these points is contained in $K$ as well, and the maximal ellipsoid contained in the convex hull is just $\mathcal{E}_r$ scaled down by a factor of $(n+1)\sqrt{n}$.\\
	Otherwise, suppose that $x_r+\frac{1}{n+1}a_1$ is not in $K$ and $H_r$ is the half-space returned by the oracle that contains $K$. Similar to the basic ellipsoid method, find the minimal ellipsoid $\mathcal{E}_{r+1}$ that contains $\mathcal{E}_r\cap H_r$.\\
	It may be shown that in this case,
	\[ \vol(\mathcal{E}_{r+1}) < e^{-3/2(n+1)(2n+1)^2}\vol(\mathcal{E}_r). \]
	The sequence terminates when we have found a weak L\"owner-John pair.
\end{proof}

% It may be shown that this sandwiching algorithm can be run in $\tilde{\mathcal{O}}(n^4)$.\footnote{the $\tilde{O}$ notation means that we suppress powers of $\log n$.}\\

% One might be tempted to increase the $\frac{1}{n+1}$ factor we use to get something even better, but it is worth noting the reason for this algorithm working in the first place is that the volume of the ellipsoid decreases at each step.\\

It is also notable that for certain types of special convex bodies, we can improve the bound beyond $(n+1)\sqrt{n}$.\\
In particular, if $K$ is symmetric, we can attain a factor of $n$, if $K$ is a polytope given as the convex hull of a set of vectors, we can attain $n+1$, and if $K$ is a symmetric polytope given as above, we can attain $\sqrt{n+1}$.\\

Typically, we assume that after performing sandwiching, we perform a linear transformation such that $B_2^n$ becomes the maximal ellipsoid of the transformed body. That is, the problem boils down computing the volume of a body $K$ with
\[ B_2^n \subseteq K \subseteq (n+1)\sqrt{n} B_2^n. \]

Readers interested in oracles and sandwiching are highly recommended to take a look at \cite{GroetschelLovaszSchrijver1993}.

\subsubsection{The Problem and Deterministic Attempts}

Our problem is to find for some given convex body $K$, some quantities $\uvol(K)$ and $\ovol(K)$ such that
\[ \uvol(K) \leq \vol(K) \leq \ovol(K) \]
while minimizing $\frac{\ovol(K)}{\uvol(K)}$.

\Cref{lovasz pre-sandwich} produces estimates (equal to the volumes of the ellipsoids) with $\frac{\ovol(K)}{\uvol(K)} = n^n (n+1)^{n/2}$. This may seem ludicrously bad, but as it turns out, any deterministic attempts in general are destined to fail. Indeed, Elekes proved in \cite{Elekes1986} that for any positive $\varepsilon<2$, there exists no deterministic polynomial time algorithm that returns
\begin{equation}
\label{eqn elekes deterministic SUCKS}
	\frac{\ovol(K)}{\uvol(K)} \leq (2-\varepsilon)^n
\end{equation}
for every convex body $K$. The reason for this is that the convex hull of polynomially many points in $B_2^n$ is always bound to be far smaller than $B_2^n$ itself -- we've already seen this all the way back in \Cref{approximating sphere to polytope}. Let us now prove \Cref{eqn elekes deterministic SUCKS}.

\begin{lemma}[Elekes' Theorem]
	Every deterministic algorithm to estimate the volume of an arbitrary convex body $K\subseteq\R^n$ that uses $q$ oracle queries has $\frac{\ovol(K)}{\uvol(K)} \geq \frac{2^n}{q}$ for some $K$ given by a well-guaranteed weak separation oracle.
\end{lemma}

What exactly do we mean by a deterministic algorithm? Roughly, it means that if we pass the same body into the algorithm twice, we will get the exact same result. More specifically, if we pass two bodies $K_1$ and $K_2$ such that $(x_i)$ and $(y_i)$ are the queried points respectively, the first point where they differ, say $x_i\neq y_i$, must be such that $x_{i-1}=y_{i-1}$ is in $K_1\triangle K_2$. We abuse this fact.

\begin{proof}
	Let $\mathcal{A}$ be some deterministic algorithm to estimate the volume of a convex body. Fix $\varepsilon=2^n$ for the separation oracle. When we run $\mathcal{A}$ on $B_2^n$, suppose that the points queried are $x_1,\ldots,x_q$. Let $C$ be the convex hull of these $q$ points. Now, note that if we run $\mathcal{A}$ on $C$, the same points $(x_i)$ will be queried and as a result, the volume estimates $\uvol$ and $\ovol$ that are returned are the same as well!\\
	To conclude the argument, note that
	\[ C \subseteq \bigcup_{i=1}^q B\left(\frac{x_i}{2},\frac{1}{2}\right). \]
	Therefore,
	\[ \frac{\vol(C)}{\vol(B_2^n)} \leq \frac{q}{2^n}. \]
	We then have
	\[ \frac{\ovol}{\uvol} \geq \frac{\vol(B_2^n)}{\vol(C)} \geq \frac{2^n}{q}. \]
\end{proof}

\subsubsection{The B\'ar\'any-F\"uredi Theorem}

We now give a stronger result known as the B\'ar\'any-F\"uredi Theorem (given in \cite{no-deterministic-algo-barany-furedi}), which shows that deterministic algorithms in general aren't much better than even the estimate with an $n^n$ error returned by basic sandwiching.

\begin{ftheo}[B\'ar\'any-F\"uredi Theorem]
	There is no deterministic polynomial time algorithm that computes a lower bound $\uvol(K)$ and an upper bound $\ovol(K)$ for the volume of every convex body $K\subseteq\R^n$ given by some oracle such that for every convex body,
	\[ \frac{\ovol(K)}{\uvol(K)} \leq \left(c \frac{n}{\log n}\right)^n \]
\end{ftheo}

The basic outline of the proof is as follows.\\

\begin{proof}
	Rather than considering a simple separation oracle, we consider an even stronger oracle. First of all, we know beforehand that the convex body $K$ is such that $B_1^n\subseteq K\subseteq B_\infty^n$.\\
	For $x\in\R^n$, denote $x^\circ = x/\norm{x}$, $H^+(x^\circ) = \{ z\in\R^d:\langle z,x^\circ \rangle \leq 1 \}$ and $H^-(x^\circ) = \{ z\in\R^d:\langle z,x^\circ \rangle \geq 1 \}$.\\

	When we query $x\in\R^n$, in addition to the information given by the separation oracle, we also receive ``$x^\circ\in K$ and $-x^\circ\in K$ and $K\subseteq H^-(x^\circ)$ and $K\subseteq H^+(x^\circ)$". That is, if $x\not\in K$, in addition to a separating hyperplane, we also receive information as to whether the hyperplanes at $\pm x^\circ$ that are orthogonal to $x$ are tangential to $K$.\\

	Now, for the main part of the proof, suppose we have some deterministic polyomial time algorithm $\mathcal{A}$ that returns a lower and upper bound $\uvol(K)$ and $\ovol(K)$ for any body $K$. The basic idea is roughly similar to that of Elekes' Theorem. Suppose we run $\mathcal{A}$ on $B_2^n$ until $m\leq n^a - n$ questions have been asked for some $a\geq 2$ (due to the polynomial nature of the algorithm) and $x_1,\ldots,x_m$ are the points queried. Define
	\[ C = \conv(\pm e_1, \pm e_2, \ldots, \pm e_n, x_1^\circ, \ldots, x_m^\circ). \]
	Now, consider the dual $C^*$ of $C$ (recall what a dual is from the proof of \Cref{fritz johns theorem part 1}). Observe that for any of the $x_i$, the output of the oracle on passing $x_i$ (or $\pm e_i$) must be the same whether we pass it with regards to $C$, $C^*$, or $B_2^n$.\footnote{if $x_i\in B_2^n$, we receive a \texttt{yes}. Otherwise, we receive a \texttt{no} along with the information that the hyperplanes at the $x_i^\circ$ are tangential.} Indeed, each $H^+(x_i^\circ)$ and $H^-(x_i^\circ)$ is a supporting hyperplane of all three bodies. This implies that the estimates returned by $\mathcal{A}$ are the same for all three bodies!\\
	We then have
	\[ \ovol(B_2^n) \geq \vol(C^*)\text{ and }\uvol(B_2^n) \leq \vol(C). \]
	Therefore,
	\[ \frac{\ovol(C)}{\uvol(C)} = \frac{\ovol(C^*)}{\uvol(C^*)} = \frac{\ovol(B_2^n)}{\uvol(B_2^n)} \geq \frac{\vol(C^*)}{\vol(C)}. \]
\end{proof}

Over the rest of this section, we show that there is some constant $c$ such that
\[ \frac{\vol(C^*)}{\vol(C)} \geq \left(c \frac{n}{\log n}\right)^n. \]
To do this, we introduce some more notation. Let
\[ V(n,m) = \sup\{\vol(K):K=\conv(\{v_1,\ldots,v_m\})\subseteq B_2^n\} \]
and
\[ S(n,m) = \inf\{\vol(\{x:|\langle x,v_i\rangle| \leq 1\text{ for each $i$}\}) : (v_i)_1^m\in\R^n\text{ such that for each }i, \norm{v_i}\leq 1\} \]

Clearly, it suffices to show that
\begin{equation}
\label{eqn: barany furedy}
	\frac{S(n,n^a)}{V(n,n^a)} \geq \left(c \frac{n}{\log n}\right)^n
\end{equation}
since $C^*$ and $C$ are of the above considered forms.\\

\subsubsection{Bounding \texorpdfstring{$V(n,m)$}{V(nm)} and \texorpdfstring{$S(n,m)$}{S(nm)}}

For $1\leq k\leq n$, define
\[
\rho(n,k) = 
\begin{cases}
1, & \text{if }k=0 \\
\sqrt{(n-k)/nk}, & \text{if }1\leq k\leq n-2 \\
1/n, & \text{if }k=n-1.
\end{cases}
\]

\begin{lemma}
\label{rho span bound}
Let $S=\conv(\{v_0,v_1,\ldots,v_n\})\subseteq B_2^n$ be an $n$-dimensional simplex and $x\in S$. Then for every $k$ such that $0\leq k\leq n-1$, $S$ has a $k$-dimensional face $S_k = \conv(\{v_{i_0},v_{i_1},\ldots,v_{i_k}\})$ and a point $x_k$ in the interior\footnote{it is a convex combination of the $(v_{i_j})$} of $S_k$ such that $(x-x_k) \perp \Span(S_k)$ and $\norm{x-x_k}\leq \rho(n,k)$.
\end{lemma}

\begin{proof}
The result for $k=n-1$ follows directly from the fact that the maximal ellipsoid in $S$ is at most $\frac{1}{n}B_2^n$.\\
For $1\leq k\leq n-2$, we use strong (backward) induction on $k$.\\
Let $x_n=x$ and for each $r:n>k>k$, let $x_r$ be such that $(x_{r+1}-x_r)\perp \Span(S_r)$ and $\norm{x_{r+1}-x_r} \leq \rho(n,r)$.\\
Note that $x_n - x_{n-1}, x_{n-1}-x_{n-2},\ldots,x_{k+1}-x_{k}$ are all orthogonal and $\norm{x_{r+1} - x_r}\leq\frac{1}{r}$ for each $r$. We then have
\begin{align*}
	\norm{x_n - x_k}^2 &= \sum_{r=k}^{n-1} \norm{x_{r+1}-x_r}^2 \\
	&\leq \sum_{r=k+1}^{n} \frac{1}{r^2} \\
	&\leq \sum_{r=k+1}^{n} \frac{1}{r(r-1)} \\
	&= \frac{1}{k} - \frac{1}{n}.
\end{align*}
Finally, the result for $k=0$ follows from the fact that the $(v_i)$ are contained in $B_2^n$.
\end{proof}

Observe that this bound is only tight when $k$ is $1$ or $n$. Putting this in a slightly more compact form, let $S\subseteq\R^n$ and $U=\Span(S)$. If we define
\[ S^\rho = S + \left(U^\perp + \rho B_2^n\right), \]
\Cref{rho span bound} just says that for some $S_k$, $x \in S_k^{\rho(n,k)}$.\\
It is also worth noting that if $S$ is convex and $\dim U = k$,
\begin{equation}
\label{eqn: rho convex body volume}
	\vol(S^\rho) = \vol_k(S) v_{n-k} \rho^{n-k}
\end{equation}

\begin{ftheo}
\label{upper bound on V n m}
There is a constant $c>0$ such that
\[ \frac{V(n,m)}{v_n} \leq \left(c \frac{1+\log(m/n)}{n}\right)^{n/2} \]
and so,
\[ V(n,m) \leq \left(\gamma \frac{\sqrt{1+\log(m/n)}}{n}\right)^{n} \]
where $\gamma=\sqrt{2\pi e c}$.
\end{ftheo}

If $m/n\to\infty$ and $n/\log(m/n)\to\infty$, then there is a constant $c'$ such that
\[ \frac{V(n,m)}{v_n} \leq \left(c' \frac{\log(m/n)}{n}\right)^{n/2} \]

\begin{proof}
It may be shown that if $K=\conv(\{v_1,\ldots,v_m\})\subseteq\R^n$, then $K$ is the union of its $n$-dimensional simplices. That is,
\[ K = \bigcup_{i_0 < \cdots < i_n} \conv(\{v_{i_0},\ldots,v_{i_n}\}). \]
This allows us to bound the volume of $K$. For any $1\leq k\leq n-1$, we can write
\[ K \subseteq \bigcup_{i_0 < \cdots < i_k} \left\{S^{\rho(n,k)} : S=\conv(\{v_{i_0},\ldots,v_{i_k}\})\right\}. \]
Bounding the volume,
\[ \vol(K) \leq \binom{m}{k+1} \max\left\{\vol\left(S^{\rho(n,k)}\right) : S=\conv(\{x_0,\ldots,x_k\})\subseteq B_2^n\right\}. \]
Using \Cref{eqn: rho convex body volume},
\[ \vol(K) \leq \binom{m}{k+1} \cdot v_{n-k} \rho(n,k)^{n-k} \cdot \max\left\{\vol_k(S) : S=\conv(\{x_0,\ldots,x_k\})\subseteq B_2^n\right\}. \]
The right-most quantity is maximum when the body is the $k$-dimensional regular solid simplex, whose volume is $(n+1)^{(n+1)/2}/n^{n/2}n!$. This is easily computed by using induction on dimension and the maximality was briefly mentioned at the end of \Cref{subsection: reverse isoperimetric problem 2.3}. So,
\begin{align*}
	\vol(K) &\leq \binom{m}{k+1} \cdot \frac{\pi^{(n-k)/2}}{\Gamma\left(\frac{n-k}{2}+1\right)} \left(\frac{n-k}{nk}\right)^{(n-k)/2} \frac{(k+1)^{(k+1)/2}}{k^{k/2}k!} \\
	&\leq \binom{m}{k+1} \cdot \left(\frac{2\pi e}{n-k}\right)^{(n-k)/2} \left(\frac{n-k}{nk}\right)^{(n-k)/2} \frac{(k+1)^{(k+1)/2}}{k^{k/2}k!} \\
	&= \binom{m}{k+1} \cdot \left(\frac{2\pi e}{nk}\right)^{(n-k)/2} \frac{(k+1)^{(k+1)/2}}{k^{k/2}k!} \\
	&\leq \frac{m^{k+1}}{(k+1)!} \cdot \left(\frac{2\pi e}{nk}\right)^{(n-k)/2} \frac{(k+1)^{(k+1)/2}}{k^{k/2}k!} \\
	&\leq \left(\frac{em}{k+1}\right)^{k+1} \cdot \left(\frac{2\pi e}{nk}\right)^{(n-k)/2} \left(\frac{e}{k}\right)^k.
\end{align*}
And therefore,
\[ \frac{\vol (K)}{v_n} \leq \left(\frac{em}{k+1}\right)^{k+1} n^{k/2} k^{-(n+k)/2}. \]
It remains to choose a suitable value of $k$. For the case when $m/n\to\infty$ and $n/\log(m/n)\to\infty$, we can let $k = \left\lceil \frac{n}{2\log(m/n)} \right\rceil$ to obtain
\[ \frac{\vol(K)}{v_n} \leq e^{o(n)} \left(\frac{2e\log(m/n)}{n}\right)^{n/2}. \]
\end{proof}

Note that the above again leads to an inference similar to that we made in \Cref{approximating sphere to polytope} -- the volume is comparable only when $m$ is exponentially large.\\

It remains to bound $S(n,m)$.\\

To do this, we use the second of the following beautiful results (we state the first for the sake of completeness).

\begin{theorem}[Blaschke-Santal\'o Inequality]
\label{santalo inequality}
Let $K$ be a convex body in $\R^n$ with dual $K^*$. Then
\[ \vol(K)\vol(K^*) \leq \vol(B_2^n)^2 \]
with equality when $K$ is an ellipsoid.
\end{theorem}

\begin{ftheo}[Inverse Santal\'o Inequality]
\label{inverse santalo inequality}
Let $K$ be a convex body in $\R^n$ with dual $K^*$. Then
\[ \vol(K)\vol(K^*) \geq \frac{4^n}{n!} \]
with equality when $K$ is the regular solid simplex.
\end{ftheo}

For our purposes, it suffices to know that there is some constant $c_2$ such that
\[ \vol(K)\vol(K^*) \geq \left(\frac{c_2}{n}\right)^n. \]
If we let $K=\{x\in\R^n:|\langle x,v_i\rangle| \leq 1\text{ for each $i$}\}$ for some $(v_i)_1^m\in\R^n$ such that $\norm{v_i}\leq 1$ for each $i$, then note that $K^* = \conv(\{v_1,\ldots,v_m\})$.\\
An upper bound then directly follows from \Cref{inverse santalo inequality} and \Cref{upper bound on V n m}. We get for some constants $c_2$ and $\gamma$,
\begin{equation}
\label{eqn: lower bound on S n m}
	S(n,m) \geq \left(\frac{c_2}{n}\right)^n \left(\frac{n}{\gamma\sqrt{ \log(m/n)+1}}\right)^n = \left(\frac{c'}{\sqrt{\log(m/n)+1}}\right)^n
\end{equation}
for some constant $c'$.\\

Finally, to show the bound mentioned in \Cref{eqn: barany furedy}, use \Cref{upper bound on V n m} and \Cref{eqn: lower bound on S n m} to get
\[ \frac{\ovol(C)}{\uvol(C)} \geq \frac{S(n,n^a)}{V(n,n^a)} \geq \left(\frac{c_1}{\gamma a}\frac{n}{\log n}\right)^n \]
which is exactly what we want.

\subsection{Rapidly Mixing Random Walks}

It has now clearly been established beyond doubt that deterministic algorithms will get us nowhere. What if instead, we consider randomized algorithms? That is, we are fine with some small probability, say $\eta$, of getting the incorrect answer? We can do far \textit{far} better in this case.

\subsubsection{An Issue with High Dimensions and the Solution}

Reformulating the problem in this context, we pass some $0<\eta<1$, some $\varepsilon>0$, and a well-guaranteed strong membership oracle\footnote{it is in general not too important which oracle we are given.} of a body $K$, and ask for an estimate $\evol(K)$ such that with probability at least $1-\eta$,
\[ (1-\varepsilon)\evol(K) \leq \vol(K) \leq (1+\varepsilon)\evol(K). \]

Henceforth, we assume that the reader has a basic understanding of Markov chains and stationary distributions thereof, at least in the discrete case. In case the reader does not, they can skip ahead to \Cref{subsec: measure theoretic markov chains}.\\

A simple method that might come to mind is a Monte Carlo algorithm. Find some box $Q$ in which $K$ is contained, uniformly randomly generate a large number of points in $Q$, and find the fraction of points generated that are in $K$ -- this is a good estimate of $\frac{\vol(K)}{\vol(Q)}$.\\
However, the issue is one that we emphasised very heavily on in the very first (and quantified to some extent in the previous) section: if we take $K=B_2^n$ and $Q=B_\infty^n$, then $\vol(K)/\vol(Q)$ is extremely (exponentially) small, so it will not work (in polynomial time) at all.\\

That is, the issue is that $\vol(K)$ is extremely small compared to a box it is contained in. To get around this, there is a surprisingly simple solution.\\

Rather than considering just $K$, consider some $m+1$ bodies $K_0\subseteq K_1\subseteq \cdots \subseteq K_m=K$ (for appropriately large $m$) and for each $i$, estimate $\Lambda_i \coloneqq \vol(K_i)/\vol(K_{i-1})$ (we can then estimate $\vol(K)$ as $\vol(K_0)\prod_i\Lambda_i$).\\
Usually, we take $K_i = K \cap 2^{i/n}B_2^n$. Note that because $K_{i}\subseteq 2^{i/n}K_{i-1}$ in this case,
\[ \Lambda_i = \frac{\vol(K_i)}{\vol(K_{i-1})} \leq 2 \]
is not large at all.\\
The value of $\vol(K_0)$ is already known. But how do we estimate $\Lambda_i$?\\
As observed earlier, since $1/\Lambda_i$ is not small, we can just stick with Monte Carlo methods, the basic idea being to somehow generate a uniform random distribution on $K_i$ and find the fraction of points generated that are within $K_{i-1}$. Here on out, our main interest is just to figure out a way of efficiently uniformly randomly generating points from $K_i$.\\

To do this, we synthesize a Markov chain whose stationary distribution is the uniform distribution on $K_i$. We run the chain for polynomially many steps, and take the resultant state as a point uniformly randomly generated from $K_i$.\\

Obviously, we want Markov chains that converge to the stationary distribution very rapidly (in polynomial time) since that is the main part of the algorithm that must be made efficient. To restrict ourselves to finding the uniform distribution \textit{within} $K_i$, we skip any move where the random walk attempts to leave $K_i$.\footnote{For the hit-and-run strategy we describe later, this is unimportant since it never even tries to leave the body.} At the same time, we count how often we are in $K_{i-1}\subseteq K_i$. In the two algorithms we discuss, we modify this to a Metropolis chain, the idea of which is discussed at the end of \Cref{sec: 4.2.4} \\

Succinctly, we use ``Multiphase Monte Carlo Markov Chain methods" for volume computation. We call the random walk ``rapidly mixing" if it gets sufficiently close to the stationary distribution in polynomially many steps.\\

Another small change is that we make the random walk \textit{lazy}. That is, if we are at $a\in K$,  we stay at $a$ with probability $\frac{1}{2}$, and with probability $\frac{1}{2}$ we choose a random direction $a+v$. If $a+v\in K$, we move there. Otherwise, we stay put. There are two reasons for doing this. The first is that sometimes parity issues arise due to which the stationary distribution might not be the uniform one. The second is that it turns the matrix describing the random walk into a positive semidefinite matrix, which is much easier to analyze.

\subsubsection{Random Walks on Graphs}

Before we move onto the general case, let us define random walks in graphs and study them for a bit. Let $G=(V,E)$ be a connected $d$-regular simple graph with $V=\{1,\ldots,n\}$. A \textit{simple random walk on $G$ with initial state $X_0$} is given by
\[
\Pr[X_{t+1}=j\mid X_t=i] = 
\begin{cases}
\frac{1}{2}, & i=j, \\
\frac{1}{2d}, & ij\in E, \\
0, & \text{otherwise.}
\end{cases}
\]

It may be shown that irrespective of $X_0$, $\lim_{t\to\infty}\Pr[X_t=i] = \frac{1}{n}$ for any $i$. But to see whether the walk is rapidly mixing, we need to know how fast it converges. Let
\[ e_{i,t} = \Pr[X_t=i] - \frac{1}{n} \]
be the ``excess" probability at time $t$ on vertex $i$. Also, denote $\Pr[X_t=i]$ as $p_i^{(t)}$ for the sake of brevity. Denoting the neighbourhood of $i$ by $\Gamma(i)$,
\begin{align}
	e_{i,t+1} &= p_i^{(t+1)} - \frac{1}{n} \nonumber \\
	&= \left(\frac{1}{2}p_i^{(t)} + \frac{1}{2d}\sum_{j\in\Gamma(i)}p_j^{(t)}\right) - \frac{1}{n} \nonumber \\
	&= \frac{1}{2}e_{i,t} + \frac{1}{2d}\sum_{j\in\Gamma(i)}e_{j,t} \nonumber \\
	&= \frac{1}{2d} \sum_{j\in\Gamma(i)}(e_{i,t}+e_{j,t}) \label{eqn: alternate expression for future error}
\end{align}
To be able to quantify our closeness to the stationary distribution, define
\[d_1(t)=d_1(\tilde{X},t) = \sum_{i}|e_{i,t}|\]
and
\[d_2(t)=d_2(\tilde{X},t) = \sum_{i}e_{i,t}^2.\]

We call a walk $\tilde{X}$ on $G$ \textit{rapidly mixing} if there exists a polynomial $f$ such that for any $0<\varepsilon<\frac{1}{3}$ and $t\geq f(\log n)\log(1/\varepsilon)$, $d_1(t)\leq\varepsilon$. However, this doesn't completely make sense right now since if we only have a single graph, $n$ is constant.

\begin{fdef}[Rapidly Mixing Random Walks]
\label{def: rapidly mixing random walks}
	Let $(G_i)_{i\in\N}$ be a sequence of graphs where $G_i$ has $n_i$ vertices and $n_i\to\infty$. We say that the simple random walks on $G_1,G_2,\ldots$ are \textit{randomly mixing} if there is a polynomial $f$ (depending only on the sequence $(G_i)$) such that if $0<\varepsilon<\frac{1}{3}$ and $t\geq f(\log n_i)\log(1/\varepsilon)$, then $d(\tilde{X_i},t)\leq\varepsilon$ whenever $\tilde{X_i}$ is a simple random walk on $G_i$.
\end{fdef}

There are some issues that arise when we want to synthesize a rapidly mixing walk. For example, suppose we have a random walk on $[-1,1]^n$ and we somehow find ourselves near one of the corners. Then the probability of leaving the corner is extremely low (of the order of $2^{-n}$) at each step, which would greatly hinder the speed of convergence.

\subsubsection{Conductance and Bounding the Speed of Convergence}

In a graph, the analogous event is that we get stuck within some subset of vertices that is highly connected within itself, but not very well-connected to its complement. With this in mind, let us define the conductance of a graph. Let $G=(V,E)$ be a graph and $U\subseteq V$ be non-empty. Then define
\[ \Phi_G(U) = \frac{e(U,V\setminus U)}{d|U|} \]
where $e(U,V\setminus U)$ is the number of edges between $U$ and $V$.\\
$\Phi_G(U)$ gives a measure of the ``difficulty" we mentioned earlier. The lower it is, the more difficult it is to leave $U$. We might as well consider only sets $U$ with $|U|\leq\frac{n}{2}$. Thus, we define the \textit{conductance} of $G$ by
\[ \Phi_G = \min_{1\leq|U|\leq n/2}\Phi_G(U). \]
In graph theoretic contexts, this quantity is more often known as the \textit{Cheeger's constant} of a graph or its \textit{isoperimetric number}.\\

For graphs in general, denote $\vol(U)=\sum_{u\in U}d(u)$. Then its conductance is
\[ \Phi_G = \min_{\substack{ U\subsetneq V \\ U\neq\emptyset}} \frac{e(U,V\setminus U)}{\min(\vol(U),\vol(V\setminus U))}. \]
Obviously, $0\leq\Phi_G\leq 1$ for any graph $G$. The upper bound is only attained when $G$ is the graph containing a single vertex, a single edge, or a triangle. The lower bound is attained only when $G$ is disconnected. If $G$ is large, then the best we can hope for is that $\Phi_G$ is not too much lower than $\frac{1}{2}$.\\

It is intuitively clear that if a graph has high conductance, then any simple walk will converge quite rapidly. This is stated quantitatively in the following.

\begin{ftheo}
\label{random walk speed of convergence conductance}
Every simple random walk on a connected $d$-regular graph $G$ satisfies
\[ d_2(t+1) \leq \left(1-\frac{1}{4}\Phi_G^2\right) d_2(t). \]
In particular,
\[ d_2(t) \leq \left(1-\frac{1}{4}\Phi_G^2\right)^t d_2(0) \leq 2\left(1-\frac{1}{4}\Phi_G^2\right)^t. \]
\end{ftheo}

We prove this using two other lemmas.

\begin{lemma}
\label{random walk speed of convergence conductance lemma 1}
	For any simple random walk on a connected $d$-regular simple graph on $G$,
\[ d_2(t+1) \leq d_2(t) - \frac{1}{2d}\sum_{ij\in E}(e_{i,t}-e_{j,t})^2. \]
\end{lemma}
\begin{proof}
Using \Cref{eqn: alternate expression for future error} along with the Cauchy-Schwarz inequality,
\begin{align*}
	d_2(t+1) &= \frac{1}{4d^2} \sum_{i=1}^n \left( \sum_{j\in\Gamma(i)} e_{i,t}+e_{j,t}\right)^2 \\
	&\leq \frac{1}{4d} \sum_{i=1}^n \sum_{j\in\Gamma(i)} (e_{i,t}+e_{j,t})^2 \\
	&= \frac{1}{2d} \sum_{ij\in E} (e_{i,t}+e_{j,t})^2 \\
	&= \frac{1}{d} \sum_{ij\in E} (e_{i,t}^2 + e_{j,t}^2) - \frac{1}{2d} \sum_{ij\in E}(e_{i,t}-e_{j,t})^2 \\
	&= d_2(t) - \frac{1}{2d} \sum_{ij\in E} (e_{i,t}-e_{j,t})^2
\end{align*}
\end{proof}

\begin{lemma}
Suppose weights $x_i$ are assigned to the elements of the vertex set $V=[n]$ satisfying $\sum_i x_i = 0$. Then
\[ \sum_{ij\in E} (x_i-x_j)^2 \geq \frac{d}{2}\Phi_G^2 \sum_{i=1}^n x_i^2. \]
\end{lemma}

Observe that setting $x_i=e_{i,t}$ for each $i$ and substituting the above in \Cref{random walk speed of convergence conductance lemma 1} directly gives \Cref{random walk speed of convergence conductance}.

\begin{proof}
We may assume without loss of generality that $x_1\geq x_2\geq \cdots\geq x_n$. Fix $m=\lceil n/2\rceil$ and for each $i$, let $y_i = x_i-x_m$. Note that it suffices (and is in fact stronger) to prove the inequality for the $(y_i)$ instead of the $(x_i)$ since
\[ \frac{d}{2}\Phi_G^2\sum_{i=1}^n (x_i-x_m)^2 = \frac{d}{2}\Phi_G^2\sum_{i=1}^n x_i^2 + \frac{nd}{2}\Phi_G^2 x_m^2. \]
Also, let
\[
u_i =
\begin{cases}
y_i, & i\leq m, \\
0, & \text{otherwise},
\end{cases}
v_i =
\begin{cases}
0, & i\leq m, \\
y_i, & \text{otherwise}.
\end{cases}
\]
Obviously, it suffices to prove the inequality for the $(u_i)$ and $(v_i)$ since \[(y_i-y_j)^2 = (u_i-u_j + v_i-v_j)^2\geq (u_i-u_j)^2 + (v_i-v_j)^2\]
and $\sum_i x_i^2 = \sum_i u_i^2 + \sum_i v_i^2$.\\
We prove it only for the $(u_i)$. Using the Cauchy-Schwarz inequality,
\begin{align}
	2d\sum_{i=1}^n u_i^2 \sum_{ij\in E} (u_i-u_j)^2 &= \sum_{ij\in E} 2(u_i^2 + u_j^2) \sum_{ij\in E}(u_i-u_j)^2 \nonumber \\
	&\geq \sum_{ij\in E} (u_i + u_j)^2 \sum_{ij\in E}(u_i-u_j)^2 \nonumber \\
	&\geq \left(\sum_{ij\in E} (u_i^2 - u_j^2)\right)^2 \label{eqn: random walk speed of convergence conductance lemma 2 cauchy schwarz}
\end{align}
We aim now to bound the term within the square in the final expression.\\
Suppose that in every edge $ij\in E$, $i<j$. We can then rewrite the expression as
\[ \sum_{ij\in E} (u_i^2-u_j^2) = \sum_{ij\in E} \sum_{l=i}^{j-1} (u_l^2-u_{l+1}^2) = \sum_{l=1}^{n} (u_l^2 - u_{l+1}^2) e\left([l],[n]\setminus[l]\right). \]
It is very clear now how the conductance enters the picture. Since we can disregard the terms of the summation after $l=m$, the expression on the right is bounded below by $dl\Phi_G$. That is,
\begin{align*}
	\sum_{ij\in E} (u_i^2-u_j^2) &\geq \sum_{l=1}^m (u_l^2-u_{l+1}^2) dl\Phi_G \\
	&= d\Phi_G\sum_{l=1}^m u_l^2.
\end{align*}
Substituting the above in \Cref{eqn: random walk speed of convergence conductance lemma 2 cauchy schwarz},
\[ \sum_{ij\in E}(u_i-u_j)^2 \geq \frac{d}{2}\Phi_G^2 \sum_{l=1}^n u_l^2, \]
which is exactly what we want to show.
\end{proof}

Since $d_1(t)^2 \leq n d_2(t)$, we have the following corollary of \Cref{random walk speed of convergence conductance}.

\begin{corollary}
Every simple random walk on a connected $d$-regular graph $G$ satisfies
\[ d_1(t) \leq (2n)^{1/2} \left(1-\frac{1}{4}\Phi_G^2\right)^{t/2} \]
\end{corollary}

Note that if $G$ is connected (so $\Phi_G\neq 0$), then for
\begin{equation}
\label{eqn: condition on time in terms of conductance}
t > 8\Phi_G^{-2}\left(\log(2n)+\log(1/\varepsilon)\right) > \frac{2}{-\log\left(1-\frac{1}{4}\Phi_G^2\right)} \left(\log(2n)+\log(1/\varepsilon)\right),
\end{equation}
$d_1(t)<\varepsilon$. Thus, we have the following sufficient condition for rapid mixing.

\begin{lemma}
Let $(G_i)_{i\in\N}$ be a sequence of regular graphs with $|G_i|=n_i\to\infty$. If there exists $k\in\N$ such that
\[ \Phi_{G_i} \geq (\log n_i)^{-k} \]
for sufficiently large $i$, then the simple random walks on $(G_i)$ are rapidly mixing.
\end{lemma}

The entirety of the discussion thus far has been regarding simple random walks. How would one go about generalizing this to aperiodic reversible random walks on finite sets in general?

\begin{fdef}
Let $V$ be a finite set and $X$ a random walk on $V$ with transition probabilities $p(u,v)$ such that for each $u$, $p(u,u)\geq \frac{1}{2}$. Let $\lambda$ be the (reversible) stationary distribution that satisfies $\lambda(u)p(u,v)=\lambda(v)p(v,u)$. Also, for $U\subseteq V$, write $\lambda(U)=\sum_{u\in U}\lambda(u)$. The \textit{conductance} of $X$ is then
\[ \tilde{\Phi}_X = \min_{\lambda(U)\leq\frac{1}{2}} \frac{\sum_{u\in U}\sum_{v\in V\setminus U} \lambda(u)p(u,v)}{\lambda(U)}. \]
\end{fdef}

Similar to earlier, the lower the conductance, the higher the probability of getting ``stuck" somewhere. Note that the conductance here is half as large as the definition we gave for regular graphs (since in this case, $p(u,v)=1/2d$ replaces the $1/d$ earlier). That is, if $X$ is a simple random walk on a regular graph,
\[ \tilde{\Phi}_X = \frac{1}{2}\Phi_G. \]
As earlier, we can measure the distance from $\lambda$ by
\[ d_2(t) = \sum_{v\in V} \left(p_v^{(t)}-\lambda(v)\right)^2. \]
We can then prove the following analogue of \Cref{random walk speed of convergence conductance} (in exactly the same way).

\begin{ftheo}
\label{large conductance implies rapidly mixing}
Let $X$ be a reversible random walk. Then with the notation above,
\[ d_2(t+1) \leq (1-\tilde{\Phi}_X^2)d_2(t). \]
In particular,
\[ d_2(t) \leq 2(1-\tilde{\Phi}_X^2)^t. \]
\end{ftheo}

\subsubsection{An Overview of Random Walks for Uniform Distributions}
\label{sec: 4.2.4}

The basic algorithm used in most algorithms that attempt to solve this problem, which we mentioned at the beginning of this section, was proposed by Dyer, Frieze, and Kannan in \cite{dyer-frieze-kannan} and has remained largely unchanged.\\
This algorithm is $\mathcal{O}(n^{23}(\log n)^5\varepsilon^{-2}\log(1/\varepsilon)\log(1/\eta))$. Henceforth, to make things relatively simple, we use the $\mathcal{O}^*$ notation that suppresses any powers of $\log n$ and polynomials of $(1/\varepsilon)$ and $\log(1/\eta)$. With this notation, the algorithm is $\mathcal{O}^*(n^{23})$.\\

We use a multiphase Monte Carlo algorithm while using random walks to sample. The improvements on this algorithm since its proposal have primarily involved changing the random walk used, using the conductance to bound the mixing time when we are likely to be close to the stationary distribution, and bounding the conductance using isoperimetric inequalities.\\

There are mainly three different types of random walks used.

\paragraph{Walking on the Grid.}

This is probably the simplest graph. It defines a sufficiently fine grid $\mathbb{L}_\delta$ where each step is of size $\delta$. Suppose we are at $x_t$. At each step, we stay put at $x_t$ with probability $\frac{1}{2}$. Otherwise, we choose a random vector $v$ of the $2n$ possible directions. If $x_t+v\not\in K$, we remain at $x_t$ and otherwise, we move to $x_{t+1}=x_t+v$.\\
\cite{dyer-frieze-kannan} uses this walk with a value of $\delta$ around $n^{-5/2}$. In \cite{lovasz-simonovits-mixing-rate-isoperimetric}, this was improved to a $\delta$ around $n^{-3/2}$.

\paragraph{Ball-Steps.}

In this random walk, we choose some small step-size $\delta$. We use a lazy random walk but when we try to move, we choose a random $v\in\delta B_2^n$. Similar to the grid, if $x_j+v\not\in K$, we remain at $x_j$ and otherwise, we move to $x_{j+1}=x_j+v$. In \cite{KLS-n5}, the value of $\delta$ was around $n^{-1/2}$.

\paragraph{Hit-and-Run.}

Unlike the previous two walks where we had to choose a step-size $\delta$, this walk doesn't need anything of the sort. We choose a random unit vector $v$ from $B_2^n$. We then find the length of the intersection of $\{x+tv:t\in\R\}$ with $K$ and pick a uniformly distributed $x'$ from this segment. It is believed that this walk converges very rapidly.\\

An issue (in any of the walks) that we must figure out how to rectify is that of getting stuck in some corner (we had given this as motivation for defining the conductance of a random walk).\\
For example, in the ball-step walk, we can consider the \textit{local conductance}
\[ \ell_\delta(x) \coloneqq \frac{\vol(K\cap (x+\delta B_2^n))}{\vol(\delta B_2^n)} \]
and the overall conductance
\[ \lambda\coloneqq \frac{1}{\vol(K)} \int_K \ell_\delta(x)\d{x}. \]

In recent times, it has also been a common theme to use \textit{Metropolis chains}, which are defined as follows.

\paragraph{Metropolis Chain.} Suppose we have a function $f$ on $K$ and a random walk (of any of the above types). We can modify our walk using the same laziness as above, but when we wish to move, we check if $f(x)\geq f(x+v)$ (where $x$ is the original position and $x+v$ is the new proposed position) and
\begin{itemize}
	\item if \texttt{yes}, move to $x+v$.
	\item if \texttt{no}, move to $x+v$ with probability $\frac{f(x)}{f(x+v)}$ (and stay at $x$ otherwise).
\end{itemize}

If $\int f < \infty$, this produces a random walk with stationary distribution that is proportional to $f(x)$.

So far, we have only tried finding uniform distributions within the body $K$ (and never return a point outside the body $K$). Often, however, we sacrifice this in favour of a distribution that can return a point outside of $K$ with not too high probability (say less than $\frac{1}{2}$) that mixes more rapidly. We detail one such algorithm, similar to that in \cite{dyer-frieze-sample-outside}, in the following section.

\subsection{A Modified Grid Walk that Runs in \texorpdfstring{$\mathcal{O}^*(n^8)$}{O(n8)}}

\subsubsection{A Description of the Walk}

The algorithm we describe here uses the ``Walking on the Grid" mentioned in the previous section. This involves splitting the body into cubes. To this end, it was observed that if want to sandwich a body between two concentric \textit{cubes} instead of balls, then a ratio of $\mathcal{O}(n)$ can be obtained (instead of the ball-sandwiching ratio of $\mathcal{O}(n^{3/2})$). In particular, \cite{applegate-kannan-cube-sandwich} shows that we can find an affine transformation $\tilde K$ of $K$ such that
\[ B_\infty^n\subseteq \tilde{K}\subseteq 2(n+1)B_\infty^n. \]
% We shall instead just stick with a far looser affine transformation that gives
% \[ B_\infty^n \subseteq \tilde{K} \subseteq n^2 B_\infty^n \]
Henceforth, we refer to this $\tilde{K}$ as $K$. So in this case, it is more convenient to consider $K_i=2^{i/n}$, $0\leq i\leq m\coloneqq \lceil n\log_2(2(n+1))\rceil$ instead of the intersections with the balls we used earlier. That is, at each phase we have two bodies $K$ and $L$ such that
\[ K_0 = B_\infty^n \subseteq L \subseteq K \subseteq 2(n+1) B_\infty^n = K_m \]
and
\[ L\subseteq K\subseteq 2^{1/n}L. \]
The grid graph over which we design our random walk has vertex set
\[ V = \frac{1}{2n}\Z^n \cap K_m.  \]
That is, $V$ is the vertex set of the grid graph $P_l^n$ with $l=8n(n+1)+1$ (having $l^n$ vertices). Denote this graph by $G$ (there is an edge between points whose distance under the $\ell_\infty$ norm is $\frac{1}{2n}$).\\
We wish to create a rapidly mixing random walk that converges to the stationary distribution on $K$ (or something that could serve the same purpose). Let us now define a distribution on $V$ that is the stationary distribution of a specific random walk.\\
Consider the function $\varphi_0$ on $\R^n$ defined by
\[ \varphi_0(x) = \min\left\{s\geq 0 : x \in \left(1+\frac{s}{2n}\right)K\right\} \]
and $\varphi$ defined by $\varphi(x)=\left\lceil \varphi_0(x)\right\rceil$. Finally, define $f(x)=2^{-\varphi(x)}$.\\
There are a few things to observe that make it apparent why this $f$ is a good choice for our purposes:
\begin{itemize}
	\item For $x\in K$, $f(x)=1$.
	\item If $x,y$ are such that $\norm{x-y}_\infty \leq \frac{1}{2n}$, then $|\varphi_0(x)-\varphi_0(y)|\leq 1$. Indeed,
	\[ x = y + (x-y) \in \left(1+\frac{\varphi_0(y)}{2n}\right)K + \frac{1}{2n}K = \left(1+\frac{\varphi_0(y)+1}{2n}\right)K \]
	so $\varphi_0(x) \leq \varphi_0(y)+1$.
	\item How many $x\in V$ are there such that $\varphi(x)=s>0$ (so $f(x)=2^{-s}$)? We must have
	\[ x \in \left(1+\frac{s}{2n}\right)K \mathbin{\big\backslash} \left(1+\frac{s-1}{2n}\right)K. \]
	The volume of the body on the right is about
	\[ \left(\left(1+\frac{s}{2n}\right)^n - \left(1+\frac{s-1}{2n}\right)^n\right) \vol(K) < (e^{s/2}-1) \vol(K). \]
	Multiplying by an appropriate factor on either side, the number of points in $V$ in this body is at most $(e^{s/2}-1)f(K)$. Therefore, for $n>3$,
	\[ f(V) = f(K) + \sum_{s=1}^\infty 2^{-s}(e^{s/2}-1)f(K) < 5f(K). \]
\end{itemize}
The above suggests that a Metropolis chain under this function might be exactly what we want -- the first point ensures that the resulting distribution is constant on $K$.\\

What is the Metropolis chain corresponding to $f$ for $G$? It is easily checked that its transition matrix is given by
\[
p(x,y) =
\begin{cases}
\frac{1}{4n}, & xy\in E\text{ and }\varphi(y)\leq\varphi(x), \\
\frac{1}{8n}, & xy\in E\text{ and }\varphi(y)=\varphi(x)+1, \\
1-\sum_{z\in\Gamma(x)}p(x,z), & x=y, \\
0, & \text{otherwise.}
\end{cases}
\]

It can also easily be checked that this walk is reversible.\\
% It is also seen that the walk stays put at $x$ with probability at least $\frac{1}{2}$.\\
Now, let the stationary distribution of this walk be $\lambda$, given by $\lambda(x)=cf(x)$ for a suitable normalizing constant $c$. The third point above ensures that $\lambda(K)>1/5$ and we don't get points outside of $K$ too often.\\

There is another issue that we haven't mentioned so far that this walk takes care of. When we have such a walk (in general), we would want to be able to compute the transition probabilities efficiently only at the points where we need it -- it would be absurd to store the entire transition matrix all the time. In this example, all we have to do is ``carry" the current value of $\varphi$ with us. At most $4n$ appeals to the oracle will give us the values of $\varphi$ at all the neighbours! We can start at a point that we know the value of $\varphi$ of, such as $0\in B_\infty^n \subseteq K$.

\subsubsection{Showing Rapid Mixing by Bounding Conductance}

The only thing that remains to show now is that it suffices to run the above random walk for a polynomial amount of time to get sufficiently close to the stationary distribution, that is, that the walk is rapidly mixing.\\
By \Cref{large conductance implies rapidly mixing}, it suffices to show that this walk has large conductance. To do this, we use the following isoperimetric inequality given in \cite{lovasz-simonovits-mixing-rate-isoperimetric}.

\begin{ftheo}
\label{conductance isoperimetric inequality}
	Let $M\subseteq\R^n$ and $\mathcal{B}(M)$ be the $\sigma$-field of Borel subsets of $M$. Let $F:\Int M\to\Rp$ be a log-concave function and let $\mu$ be the measure on $\mathcal{B}(M)$ with density $F$
	\[ \mu_F(A) = \int_A F \]
	for $A\in\mathcal{B}(M)$. Then for $A_1,A_2\in\mathcal{B}(M)$,
	\[ \mu_F(M \setminus (A_1\cup A_2)) \geq \frac{d(A_1,A_2)}{\diam M} \min(\mu_F(A_1),\mu_F(A_2)), \]
	where $\diam M = \sup\{\norm{x-y}:x,y\in M\}$.
\end{ftheo}

This inequality is slightly loose. The best possible constant on the right has an extra multiplicative factor of $2$ and was proved in \cite{dyer-frieze-sample-outside}. We omit the proof of the above for now and later prove a better bound in \Cref{improvement of conductance isoperimetric inequality} (and in the process, prove \Cref{conductance isoperimetric inequality} as a simple consequence of \Cref{localization lemma}). We shall now use this to show that the conductance of our random walk is large.\\
Let us have $U\subseteq V$ with $0<\lambda(U)<\frac{1}{2}$ and let $\overline{U}=V\setminus U$. Also, let $\partial{U}$ be the set of vertices in $\overline{U}$ with at least one neighbour in $U$.\\
Let $M$ be the union of the cubes of side length $1/2n$ centered at vertices of $V$ ($M$ is a solid cube) and $A_1$ be the union of cubes of side length $1/2n$ centered at vertices of $U$. Let $B$ be the union of cubes of volume $2/(2n)^n$ centered at vertices of $\partial U$ and $A_2 = M \setminus (A_1 \cup B)$. Obviously,
\begin{equation}
	\diam(M) = \mathcal{O}(n^{3/2}).
\end{equation}
Then, observe that
\begin{equation}
	d(A_1,A_2) \geq \frac{1}{2n}\frac{\sqrt{n}}{2} (2^{1/n} - 1) = \Omega(n^{-3/2}).
\end{equation}
for some suitable positive constant $c_1$. Also, for some positive constant $c_2$,
\begin{align*}
	\sum_{\substack{u\in U \\ v \in \overline{U}}}\lambda(u)p(u,v) &= \sum_{\substack{u\in U \\ v \in B}}\lambda(v)p(v,u)\\
	&\geq \sum_{v\in B} \frac{1}{8n} \lambda(v) = c_2\frac{\lambda(B)}{n}.
\end{align*}

We may assume that $\lambda(B)$ is small.
Now, define a measure $\mu$ on $\mathcal{B}(M)$ as in \Cref{conductance isoperimetric inequality} with $F=2^{-\varphi_1}$, where $\varphi_1$ is the maximal convex function on $M$ bounded above by $\varphi$. Observe that $\lambda(u)$ is always within a constant factor of the $\mu$-measure of the unit cube centered at $u$. Thus, we have
\begin{align}
	\frac{\sum_{u\in U} \sum_{v\in\overline{U}} \lambda(u)p(u,v)}{\lambda(U)} &\geq \frac{c_3}{n}\frac{\mu(B)}{\min\{\lambda(U),\lambda(\overline{U}\setminus\partial U)\}} \nonumber \\ % *** HOW???
	&= \frac{c_3}{n}\frac{\mu(M \setminus (A_1\cup A_2))}{\min\{\mu(A_1),\mu(A_2)\}} \nonumber \\
	&\geq \frac{c_3}{n} \frac{d(A_1,A_2)}{\diam M} \nonumber \\
	&= \Omega(n^{-4}) \label{eqn: number of steps}
\end{align}

So what is the total time complexity of the algorithm? Combining \Cref{eqn: number of steps} and \Cref{eqn: condition on time in terms of conductance} (or rather, the corresponding result for $d_2(t)$ that does not have the $\log n$ factor), the number of steps in the random walk of each phase of the multiphase Metropolis walk is $\mathcal{O}^*(n^{8})$. At each step of the walk, we perform $\mathcal{O}(n)$ oracle queries. Finally, there are $\mathcal{O}^*(n)$ phases. All together, the algorithm is $\mathcal{O}^*(n^{10})$.\\

In \cite{dyer-frieze-sample-outside}, a more careful analysis is done to show that this algorithm is in fact $\mathcal{O}^*(n^{8})$.\footnote{The conductance is actually $\Omega(n^{-3})$.} More precisely, it is
\[ \mathcal{O}\left(n^8\varepsilon^{-2}\log\left(\frac{n}{\varepsilon}\right)\log\left(\frac{1}{\eta}\right)\right). \]

% \subsubsection{Closing Statements}

% When \cite{dyer-frieze-kannan} originally showed that it was possible to compute the volume in polynomial time, pre-sandwiching was a negligible step in the process being nearly insignificant compared to the $\mathcal{O}^*(n^{23})$ that the main part of the algorithm took. However, as time has passed and the second part of the algorithm has become faster and faster, it has become necessary to get a more efficient algorithm for sandwiching as well. In particular, in \cite{KLS-n5}, both the sandwiching and the second part were $\mathcal{O}^*(n^5)$. If we want to get faster than this, then we have to speed up the sandwiching as well.\\
% The main innovation in this is to do \textit{approximate sandwiching} instead. Instead of insisting that $B_2^n\subseteq K\subseteq d B_2^n$, we instead want $B_2^n\subseteq K$ and that $d' B_2^n\cap K$ is ``most of" $K$. Despite what we had said earlier that might be slightly misleading, even \cite{KLS-n5} uses an approximate sandwiching algorithm.
% % More recently, \cite{vempala-n4} uses such an algorithm to achieve an $\mathcal{O}^*(n^4)$ algorithm for volume estimation.

% There is also the following problem by Lov\'asz related to sandwiching.
% \begin{quote}
%     Given a convex $K\subseteq\R^n$ and an $\varepsilon>0$, can one always find two homethetic ellipsoids $\mathcal{E}_1$ and $\mathcal{E}_2$ such that their ratio is $\leq(\log n)^c$ and $\vol(K\setminus\mathcal{E}_2)<\varepsilon\vol(K)$ and $\vol(\mathcal{E}_1\setminus K)<\varepsilon\vol(K)$?
% \end{quote}

% With this approximate sandwiching as motivation, we introduce the following.

% There exists a unique ellipsoid $\mathcal{E}_K$ such that for every $s\in\R^n$,
% \[ \int_K \langle \textbf{x},s\rangle^2 \d{\textbf{x}} = \int_{\mathcal{E}_K} \langle \textbf{x},s\rangle^2 \d{\textbf{x}}. \]
% This ellipsoid is known as the Legendre ellipsoid of $K$. A body is said to be in isotropic position if its Legendre ellipsoid is $B_2^n$. That is,

% \begin{fdef}[Isotropic Position]
% A convex body $K\subseteq\R^n$ is said to be in \textit{isotropic position} if its center of gravity is the origin
% \[ \textbf{b}(K) = \int \textbf{x}\d{\textbf{x}} = 0, \]
% and for every $1\leq i\leq j\leq n$,
% \[ \frac{1}{\vol(K)} \int_K x_i x_j =
% \begin{cases}
% 1, & i=j, \\
% 0, & \text{otherwise.}
% \end{cases}
% \]
% \end{fdef}

% Note that this implies
% \[ \int_K \norm{\textbf{x}}^2\d{\textbf{x}} = n. \]
% Markov's inequality then implies that all but an $\varepsilon$ portion of $K$ belongs to $\displaystyle\sqrt{\frac{n}{\varepsilon}}B_2^n$.

% For computational purposes,

% \begin{definition}
% For $\nu\in(0,1)$, $K$ is said to be in \textit{$\nu$-almost-isotropic position} if $\norm{\textbf{b}(K)}\leq \nu$ and for every $v\in\R^n$,
% \[ (1-\nu)\norm{v}^2 \leq \frac{1}{\vol(K)} \int_{K-\textbf{b}(K)} \langle v,\textbf{x}\rangle^2\d{\textbf{x}} \leq (1+\nu)\norm{v}^2. \]
% \end{definition}

% There is in fact a randomized algorithm that finds in $\mathcal{O}\left(n^5 \log (n) \log\left(\frac{1}{\nu\eta}\right)\right)$ steps a linear transformation $\tilde{K}$ of $K$ such that $\tilde{K}$ is $\nu$-almost-isotropic with probability at least $1-\eta$. Further, with probability at most $1-\eta$,
% \[ \vol\left(\tilde{K} \setminus 2\sqrt{2n}\log\left(\frac{1}{\varepsilon}\right)B_2^n\right) < \varepsilon\vol(\tilde{K}). \]

% This isotropic position is what has been used instead of usual sandwiching in \cite{KLS-n5,vempala-n4}.\\

% We now state one of the most important conjectures in this field, commonly known as the KLS Conjecture (named after Kannan, Lov\'asz, and Simonovits).

Over the course of the next few sections, we describe a $\mathcal{O}^*(n^7)$ volume estimation algorithm given in \cite{lov-sim-on7}.

\subsection{Measure-Theoretic Markov Chains and Conductance}
\label{subsec: measure theoretic markov chains}

\subsubsection{Some Basic Definitions}

\begin{fdef}
	Let $\Omega$ be a non-empty set and $\mathcal{A}$ a $\sigma$-algebra on $\Omega$. For every $u\in\Omega$, let $P_u$ be a probability measure on $\Omega$. Also assume that as a function of $u$, $P_u(A)$ is measurable for any $A\in\mathcal{A}$. We call the triple $(\Omega,\mathcal{A},\{P_u:u\in\Omega\})$ a \textit{Markov scheme}. Together with an initial distribution $Q_0$ on $\Omega$, this defines a \textit{Markov chain}.
\end{fdef}

A Markov chain is essentially just a sequence of random variables $w_0,w_1,\ldots$ such that $w_0$ is drawn from $Q_0$ and $w_{i+1}$ is drawn from $P_{w_i}$ (independently of the values of $w_0,\ldots,w_{i-1}$). Therefore,
\[ \Pr[w_{i+1}\in A \mid w_1=u_1,\ldots,w_i=u_i] = \Pr[w_{i+1}\in A\mid w_i=u_i] = P_{u_i}(A). \]
Let $f:\Omega\times\Omega\to\R$ be an integrable function (with respect to the product measure $\mu\times\mu$) such that $\int_\Omega f(u,v)\d{\mu}(v)=1$ for all $u\in\Omega$. $f$ then defines a Markov scheme as
\[ P_u(A) = \int_A f(u,v)\d{\mu}(v). \]
In this case, $f$ is known as the \textit{transition function} of the Markov scheme. The transition function is said to be \textit{symmetric} if for all $x,y$, $f(x,y)=f(y,x)$.\\

A probability measure $Q$ on $\Omega$ is said to be the \textit{stationary distribution} of a Markov scheme if for all $A\in\mathcal{A}$,
\[ \int_{\Omega}P_u(A)\d{Q}(u) = Q(A). \]
This just means that every $w_i$ has the same distribution as $Q$.\\

Now, consider the inner product space $L^2 = L^2 (\Omega,\mathcal{A},Q)$ with inner product
\[ \langle f,g\rangle = \int_\Omega f g\d{Q}. \]
Suppose we have some function $g\in L^2$. Then note that the expectation of $g(w_{i+1})$ (as a function of $w_i = u$) defines a positive linear operator\footnote{a linear operator $A$ such that $\langle Ax,x\rangle\geq 0$ for any $x$.} $M:L^2\to L^2$ by
\[ (Mg)(u) = \int_\Omega g(v)\d{P_u}(v). \]
Further note that $(M^k g)(u)$ represents the expectation of $g(w_{i+k})$ given that $w_i=u$.

Now, consider a Markov chain where the first element is drawn from the stationary distribution. Then for any function $g\in L^2$,
\begin{align*}
	\expec[g(w_i)] &= \expec[g(w_0)] = \langle g,1\rangle \\
	\expec[g(w_i)^2] &= \expec[g(w_0)^2] = \langle g,g\rangle \\
	\expec[g(w_i)g(w_{i+k)}] &= \expec[g(w_0)g(w_k)] = \langle g,M^k g\rangle
\end{align*}

A Markov chain is said to be \textit{time-reversible} if for any $A,B\in\mathcal{A}$, the probability of going from $B$ to $A$ is the same as that of going from $A$ to $B$. That is,
\[ \int_B P_u(A)\d{Q}(u) = \int_A P_u(B) \d{Q}(u). \]
It is easy to see that it suffices to have the above for all disjoint sets $A$ and $B$. The above can be rewritten in a more symmetric fashion as
\begin{equation}
	\label{eqn: 4.12}
	\int_B \int_A 1\d{P}_u(v)\d{Q}(u) = \int_A \int_B 1\d{P}_u(v)\d{Q}(u).
\end{equation}
This is equivalent to saying that for any function $g:\Omega\times\Omega\to\R$ (assuming both sides are well-defined),
\begin{equation}
\label{eqn: time reversible function formulation}
	\int_\Omega \int_\Omega g(u,v)\d{P}_u(v)\d{Q}(u) = \int_\Omega \int_\Omega g(v,u)\d{P}_u(v)\d{Q}(u).
\end{equation}
It is also equivalent to say that the operator $M$ is self-adjoint.\footnote{an operator $A$ such that $\langle Ax,y\rangle=\langle x,A y\rangle$ for any $x,y$.} If the Markov scheme can be described by a transition function $f$ (with respect to $Q$), then time-reversibility is equivalent to the symmetry of $f$ (this isn't too difficult to prove using \Cref{eqn: 4.12} and Fubini's Theorem).\\
If the Markov scheme is time-reversible, then for any $g\in L^2$,
\begin{align}
	\langle g,g\rangle - \langle g,M g\rangle &= \int_{\Omega} g^2\d{Q} - \int_\Omega\int_\Omega g(u)g(v)\d{P_u}(v)\d{Q}(u) \nonumber \\
	&= \int_{\Omega}\int_\Omega g^2(u)\d{P_u}(v)\d{Q}(u) - \int_\Omega\int_\Omega g(u)g(v)\d{P_u}(v)\d{Q}(u) \nonumber \\
	&= \frac{1}{2} \left(\int_{\Omega}\int_\Omega (g^2(u)+g^2(v))\d{P_u}(v)\d{Q}(u) - \int_\Omega\int_\Omega 2g(u)g(v)\d{P_u}(v)\d{Q}(u)\right) & (\text{by \Cref{eqn: time reversible function formulation}}) \nonumber \\
	&= \frac{1}{2}\int_\Omega\int_\Omega (g(u)-g(v))^2\d{P_u}(v)\d{Q}(u) \geq 0. \label{eqn: self-adjoint spectral radius 1}
\end{align}
Therefore, the spectral radius\footnote{the largest absolute value of its eigenvalues.} of $M$ is exactly $1$.

\begin{definition}[Laziness]
A Markov chain is said to be \textit{lazy} if for each $u$,
\[ P_u(\{u\})\geq\frac{1}{2}. \]
\end{definition}

There are two main, albeit minor and technical, reasons for desiring laziness:
\begin{itemize}
	\item Sometimes, a lack of laziness can cause parity issues which result in the limit distribution of a chain not converging to the stationary distribution.
	\item In the time-reversible case, it makes the operator $M$ positive semidefinite, thus making it far easier to analyze.
\end{itemize}
To see why the latter occurs, observe that if $M$ is self-adjoint, then so is $2M-I$ (and is thus associated with a Markov scheme) and thus,
\[ \langle f,M f\rangle \geq \frac{1}{2}\langle f,f\rangle - \frac{1}{2}\langle f,(2M-I)f\rangle \geq 0. \]
Any Markov scheme can be made lazy easily by flipping a (fair) coin at each step and making a move only if it lands on tails.

\begin{lemma}
Let $w_1,w_2,\ldots$ be a time-reversible Markov chain generated by a lazy Markov scheme $\mathcal{M}$ with $w_0$ drawn from the stationary distribution $Q$ of $\mathcal{M}$. Then for any function $g\in L^2$,
\[ \expec[g(w_i)g(w_j)] \geq \expec[g(w_i)]\expec[g(w_j)] = \expec[g(w_0)^2]. \]
\end{lemma}
\begin{proof}
Assume without loss of generality that $j>i$ and $j-i=k$. Then for any function $h$, the positive semidefiniteness of $M$ implies that
\[ \expec[h(w_i)h(w_j)] = \langle h, M^k h\rangle \geq 0. \]
Applying this to $(g - \expec[g(w_0)])$ yields the result.
\end{proof}

\subsubsection{Conductance}

\begin{definition}[Ergodic Flow]
	\label{def: ergodic flow}
	Define the \textit{ergodic flow} $\Phi:\mathcal{A}\to[0,1]$ of a Markov scheme by
	\[ \Phi(A) = \int_A P_u(\Omega \setminus A) \d{Q}(u). \]
\end{definition}

This just measures how likely the Markov chain is to leave $A$ if it is initially in $A$ (and drawn from $Q$). Since $Q$ is stationary,
\begin{align*}
	\Phi(A) - \Phi(\Omega\setminus A) &= \int_A P_u(\Omega\setminus A)\d{Q}(u) - \int_{\Omega\setminus A} P_u(A)\d{Q}(u) \\
	&= Q(A) - \int_A P_u(A)\d{Q}(u) - \int_{\Omega\setminus A} P_u(A)\d{Q}(u) & (\text{since } P_u(\Omega\setminus A)=1-P_u(A)) \\
	&= Q(A) - \int_\Omega P_u(A)\d{Q}(u) = 0.
\end{align*}
Even conversely, if for some probability distribution $Q'$, the function $\Phi':\mathcal{A}\to[0,1]$ defined by
\[ A\mapsto \int_A P_u(\Omega\setminus A)\d{Q}'(u) \]
is invariant under complementation, then $Q'$ is stationary.\\

\begin{fdef}
	\label{def: conductance}
	The \textit{conductance} of the Markov scheme is then defined as
	\[ \Phi = \inf_{0<Q(A)<1/2} \frac{\Phi(A)}{Q(A)}. \]
	For $0\leq s\leq 1$, the \textit{$s$-conductance} is defined as
	\[ \Phi_s = \inf_{s < Q(A) \leq 1/2} \frac{\Phi(A)}{Q(A)-s}. \]
\end{fdef}

The lower the conductance is, the more likely the Markov chain is to ``get stuck" somewhere. Loosely, $Q(A)$ represents the probability of entering a ``bad region'' whereas $\Phi(A)$ represents the probability of leaving it. \\

For any $u$, $1-P_u(\{u\})$ is called the \textit{local conductance} of the Markov chain at $u$. If $Q(u)>0$, then the local conductance is an upper bound on the conductance.\\
More generally, let
\[ H_t = \{u\in\Omega:P_u(\{u\}) > 1-t\} \]
and $s=Q(H_t)$. Then
\[ \Phi(H_t) = \int_{H_t} P_u(\Omega\setminus H_t)\d{Q}(u) \leq t Q(H_t). \]
Therefore, the $(s/2)$-conductance is at most $2t$.

The main use of defining conductance is that it is closely related to how fast Markov chains converge to their stationary distribution.\\
Suppose that $Q_k$ is the distribution in the $k$th step of the chain ($Q_k(A)=\Pr[w_k\in A]$). It turns out that if for all $A\in\mathcal{A}$ such that $Q(A)>0$, $\Phi(A)>0$, then $Q_k\to Q$ (in the $\ell_1$ distance\footnote{given by $\norm{f}_1 = \int_\Omega |f|\d{Q}$}). This manifests as an associated bound in the speed of convergence.\\
To make concrete this notion of ``speed of convergence'', let us define the following specific distance function.

\subsubsection{A Distance Function}

\begin{fdef}
For $x\in[0,1]$, consider all measurable functions $g:\Omega\to[0,1]$ such that
\[ \int_{\Omega} g\d{Q} = x. \]
We then define the \textit{distance function} of $Q$ and $Q_k$ by
\[ h_k(x) = \sup_g \int_\Omega g(\d{Q}_k-\d{Q}) = \sup_g \int_\Omega g\d{Q}_k - x. \]
\end{fdef}

For example, it is easily shown that for a finite Markov chain with $N$ states and uniform stationary distribution, $h_k(j/N)$ is the sum of the $j$ largest $\left(Q_k(\omega)-\frac{1}{n}\right)$.\\

There are a few things to note.
\begin{itemize}
	\item For any $x$, $0\leq h_k\leq 1-x$. The lower bound is because one can consider the constant function $x$ on $\Omega$. The upper bound is because $\int_\Omega g\d{Q}_k$ is bounded above by $1$. In particular, $h_k(1)=0$.
	\item $h_k$ is a concave function of $x$. We shall see below in \Cref{hk distance supremum attained} that the supremum in the definition of $h_k$ is attained. Then, for any $a,b,\lambda\in[0,1]$, set $x=\lambda a+(1-\lambda)b$ and let $g_1,g_2$ be the functions that attain the supremums for $h_k(a)$ and $h_k(b)$. Then,
	\[ \sup_g \int_\Omega g\d{Q}_k - x \geq \int_\Omega (\lambda g_1 + (1-\lambda)g_2) \d{Q}_k = \lambda h_k(a) + (1-\lambda) h_k(b). \]
\end{itemize}

The definition of $h_k$ is similar to a continuous analogue of the fractional knapsack problem wherein the sum of the weights corresponds to integration with respect to $Q$, the sum of values corresponds to integration with respect to $Q_k$, and the weight capacity is $x$. This analogy also lends itself very naturally in the proof of \Cref{hk distance supremum attained}, where we ``assign $g$'' to the points which have a high value-weight ratio -- this corresponds to the Radon-Nikodym derivative in the continuous case.\\
This definition might still seem slightly artificial at the moment, but we hope to give more context to it with the following few lemmas.

\begin{lemma}
	For every set $A\in\mathcal{A}$ with $Q(A)=x$,
	\[ -h_k(1-x) \leq Q_k(A) - Q(A) \leq h_k(x). \]
\end{lemma}
\begin{proof}
	The upper bound is immediate from the definition of the distance function by taking $g = \indic_A$ (the indicator function on $A$). The similar upper bound for $\Omega\setminus A$ gives the result.
\end{proof}

\begin{lemma}
\label{hk distance supremum attained}
	For every $0<x<1$, there exists a function $G$ that is $0$-$1$ valued except possibly on a $Q$-atom\footnote{a $Q$-atom is a set $V\in\mathcal{A}$ such that $Q(V)>0$ and for any $V'\subseteq V$, either $Q(V')=Q(V)$ or $Q(V')=0$.} that attains the supremum in the definition of $h_k(x)$.
\end{lemma}
\begin{proof}
	Let $U\in\mathcal{A}$ such that $Q(U)=0$ and $Q_k(U)$ is maximum. Let $Q'$ and $Q_k'$ be the restrictions of $Q$ and $Q_k$ to $\Omega\setminus U$ respectively. Clearly, the way we have defined $U$ implies that $Q_k'$ is absolutely continuous with respect to $Q'$. Thus, let $\phi$ be the Radon-Nikodym derivative of $Q_k'$ with respect to $Q'$.\\
	Now, let $x\in[0,1]$ and $g:\Omega\to[0,1]$ such that $\int_\Omega g\d{Q} = x$.\\
	For $t\geq 0$, define
	\[ A_t = U \cup \{u \in \Omega\setminus U : \phi(u) \geq t\}\text{ and }s=\inf\{t\geq 0 : Q(A_t) \leq x\}. \]
	Observe that since $A_s = \bigcap_{t<s}A_t$, upper semicontinuity implies that $Q(A_s)\geq x$. Also define
	\[ A' = \bigcup_{t>s} A_t = U \cup \{u \in \Omega\setminus U : \phi(u) > s\}. \]
	Lower semicontinuity implies that $Q(A')\leq x$. We also have that $A'\subseteq A_s$ and for every $u\in A_s\setminus A'$, $\phi(u)=s$.\\
	Now, choose a $B\in\mathcal{A}$ such that $A'\subseteq B\subseteq A_s$, $Q(B)\leq x$, and $Q(B)$ is maximum.\\
	We first show that if $Q(B)=x$, then the indicator function on $B$ suffices. Indeed, in this case,
	\begin{align*}
		\int_\Omega g\d{Q}_k &= \int_U g\d{Q}_k + \int_{B\setminus U} g\phi\d{Q} + \int_{\Omega\setminus B} g\phi\d{Q} \\
		&= \int_U g\d{Q}_k + \int_{B\setminus U} (g-1)\phi\d{Q} + \int_{B\setminus U} \d{Q}_k + \int_{\Omega\setminus B} g\phi\d{Q} \\
		&\leq \int_U \d{Q}_k + s\int_{B\setminus U} (g-1)\d{Q} + \int_{B\setminus U} \d{Q}_k + s\int_{\Omega\setminus B} g \d{Q} & \text{($0\leq g\leq 1$ and $\phi\leq s$ $Q$-almost everywhere on $\Omega\setminus B$)} \\
		&= Q_k(B) + s\int_{\Omega\setminus U} g\d{Q} - s\int_{B\setminus U}\d{Q} \\
		&= Q_k(B) + s(x - Q(B)) = Q_k(B).
	\end{align*}
	We also see that the supremum is attained when $g=\indic_B$.\\
	Next, assume that $Q(B)<x$. Then for every $W\subseteq A_s\setminus B$, either $Q(W)=0$ or $Q(W)>x-Q(B)$. That is, the measure on $A'\setminus B$ is concentrated on atoms. Let $V$ be one such atom. As shown above,
	\[ \int_\Omega g\d{Q}_k \leq Q_k(B) + s(x-Q(B)). \]
	To show that this bound is attained, let $g=\indic_B+\lambda\indic_{V}$ where $\lambda = (x-Q(B))/Q(V)$. Clearly, $0\leq g\leq 1$. Further,
	\[ \int_\Omega g\d{Q} = Q(B) + \lambda Q(V) = x \]
	and
	\[ \int_\Omega g\d{Q}_k = Q_k(B)+\lambda Q_k(V) = Q_k(B) + s(x-Q(B)) \]
	where the last step follows since $\phi(u)=s$ for all $u\in V\subseteq A_s\setminus A'$.
\end{proof}

\begin{lemma}
	If $Q$ is atom-free, then
	\[ h_k(x) = \sup_{\substack{A\in\mathcal{A} \\ Q(A)=x}} \left(Q_k(A) - Q(A)\right). \]
\end{lemma}

This follows directly from the previous lemma.\\
If $Q$ is atom-free, note that
\begin{equation}
	\label{eqn: hk and one norm}
	\sup_{x} h_k(x) = \sup_{A\in\mathcal{A}} |Q_k(A) - Q(A)| = \frac{1}{2}\norm{Q_k - Q}_1.
\end{equation}

Although we did say what a rapidly mixing random walk is earlier in \Cref{def: rapidly mixing random walks}, we now define it more generally.\\

\subsubsection{Rapidly Mixing Markov Chains}

\begin{fdef}[Rapidly Mixing Markov Chain]
A Markov chain is said to be \textit{rapidly mixing} if for some $\theta<1$, $\sup_x h_k(x)$ is $\mathcal{O}(\theta^k)$.
\end{fdef}

For our needs, the stationary distribution $Q$ is usually atom-free. Unless mentioned otherwise, we assume henceforth that this is the case. We also assume that the chains are lazy.

Let us now get on to the main subject of this section, namely that of bounding the speed of convergence of rapidly mixing Markov chains.

\begin{theorem}
For $k\geq 1$, if $s\leq x\leq 1/2$, then
\[ h_k(x) \leq \frac{1}{2} \left(h_{k-1}(x-2\Phi_s(x-s)) + h_{k-1}(x+2\Phi_s(x-s))\right) \]
and if $1/2 \leq x\leq 1-s$, then
\[ h_k(x) \leq \frac{1}{2} \left(h_{k-1}(x-2\Phi_s(1-x-s)) + h_{k-1}(x+2\Phi_s(1-x-s))\right). \]
\end{theorem}
\begin{proof}
We prove the first inequality alone.

By \Cref{hk distance supremum attained}, let $A$ be a set such that $Q(A)=x$ and $h_k(x)=Q_k(A)-Q(A)$. Define $g_1,g_2:\Omega\to[0,1]$ by
\[ g_1(u) = 
\begin{cases}
2P_u(A) - 1, & u\in A, \\
0, & \text{otherwise,}
\end{cases}
\quad
% \text{ and }
g_2(u) = 
\begin{cases}
1, & u\in A, \\
2P_u(A), & \text{otherwise.}
\end{cases}
\]
The functions map into $[0,1]$ because the chain is lazy.\\
Also, let $x_1 = \int_\Omega g_1\d{Q}$ and $x_2 = \int_\Omega g_2\d{Q}$. Observe that $x_1 + x_2 = \int_\Omega 2P_u(A)\d{Q}(u) = 2x$. We have
\begin{align*}
	h_k(x) &= Q_k(A) - Q(A) \\
	&= \frac{1}{2} \left( \left(\int_\Omega g_1\d{Q}_{k-1} - x_1\right) + \left( \int_\Omega g_2\d{Q}_{k-1} - x_2 \right) \right) \\
	&\leq \frac{1}{2} \left( h_{k-1}(x_1) + h_{k-1}(x_2) \right).
\end{align*}
We also have
\[ x_2-x = x-x_1 = \int_A (2-2P_u(A))\d{Q}(u) = 2\Phi(A) \geq 2\Phi_s(x-s). \]
Then with the above, the concavity of $h_{k-1}$ then implies that
\[ h_{k-1}(x_1)+h_{k-1}(x_2) \leq h_{k-1}(x - \Phi_s(x-s)) + h_{k-1}(x+\Phi_s(x-s)), \]
completing the proof.
\end{proof}

The next result is analogous to \Cref{large conductance implies rapidly mixing} and is our main tool in bounding the speed of convergence using the conductance.

\begin{ftheo}
\label{decrease in markov distance}
	Let $0\leq s\leq 1/2$ and suppose we have $c_1,c_2$ such that for $s\leq x\leq 1-s$,
	\[ h_0(x) \leq c_1 + c_2\min\{\sqrt{x-s},\sqrt{1-s-x}\}. \]
	Then for every $k\geq 0$ and $s\leq x\leq 1-s$,
	\[ h_k(x) \leq c_1 + c_2\min\{\sqrt{x-s},\sqrt{1-s-x}\} \left(1-\frac{\Phi_s^2}{2}\right)^k. \]
\end{ftheo}
\begin{proof}
	We prove this via induction. It clearly holds for $k=0$. Suppose that $k\leq 1$ and $s\leq x\leq 1/2$. Using induction,
	\begin{align*}
		h_k(x) &\leq \frac{1}{2} \left(h_{k-1}(x-2\Phi_s(x-s)) + h_{k-1}(x+2\Phi_s(x-s))\right) \\
		&\leq c_1 + \frac{c_2}{2} \left(1-\frac{\Phi_s^2}{2}\right)^{k-1} \left(\sqrt{x-2\Phi_s(x-s)-s} + \sqrt{x+2\Phi_s(x-s)-s}\right) \\
		&= c_1 + \frac{c_2}{2} \sqrt{x-s} \left(1-\frac{\Phi_s^2}{2}\right)^{k-1} \left(\sqrt{1-2\Phi_s} + \sqrt{1+2\Phi_s}\right) \\
		&\leq c_1 + \frac{c_2}{2}\sqrt{x-s}\left(1-\frac{\Phi_s^2}{2}\right)^{k-1} \left(1-\frac{2\Phi_s}{2}-\frac{4\Phi_s^2}{8} + 1+\frac{2\Phi_s}{2}-\frac{4\Phi_s^2}{8}\right) \\
		&= c_1 + c_2 \sqrt{x-s} \left(1-\frac{\Phi_s^2}{2}\right)^k \qedhere
	\end{align*}
\end{proof}

Writing the above in a slightly more useful form,

\begin{corollary}
	\label{cor 4.23}
	Let $M=\sup_A Q_0(A)/Q(A)$. Then for every $A\in\mathcal{A}$,
	\[ |Q_k(A) - Q(A)| \leq \sqrt{M}\left(1-\frac{\Phi_s^2}{2}\right)^k. \]
\end{corollary}
\begin{proof}
	Clearly, for any $x$, $h_0(x) \leq Mx$. We also have $h_0(x) \leq 1-x$. Therefore,
	\[ h_0(x) \leq \sqrt{Mx(1-x)} \leq \sqrt{M}\min\{\sqrt{x},\sqrt{1-x}\}. \]
	\Cref{decrease in markov distance} then implies the required.
\end{proof}

% \begin{corollary}
%     Let $0\leq s\leq 1/2$ and $H_s=\sup\{|Q_0(A)-Q(A)|:Q(A)\leq s\}$. Then for every $A\in\mathcal{A}$,
%     \[|Q_k(A)-Q(A)| \leq H_s + \frac{H_s}{s}\left(1-\frac{\Phi_s^2}{2}\right)^k. \]
% \end{corollary}
% \begin{proof}
%     We show that for every $0\leq x\leq 1$,
%     \[ h_0(x) \leq H_s + \frac{H_s}{s}\sqrt{x-s}. \]
%     For $0\leq x\leq s$,
%     \[ h_0(x) = \sup_{\substack{A\in\mathcal{A} \\ Q(A)=x}} (Q_0(A)-Q(A)) \leq H_s. \]
%     It is similarly shown (by taking the complement) that for $1-s\leq x\leq 1$, $h_0(x) \leq H_s$. Now, the concavity of $h_0(x)$ implies that for any $s\leq x\leq 1$,
%     \[ h_0(x) \leq \frac{h_0(s)\cdot x}{s} \leq H_s \frac{H_s}{s}(x-s) \leq H_s + \frac{H_s}{s}\sqrt{x-s}. \]
%     Similarly, for $0\leq x\leq 1-s$, $h_0(x) \leq H_s + \frac{H_s}{s}\sqrt{1-x-s}$.\\
%     Therefore, for all $s\leq x\leq 1-s$,
%     \[ h_k(s) \leq H_s + \frac{H_s}{s}\left(1-\frac{\Phi_s^2}{2}\right)^k\min\{\sqrt{x-s},\sqrt{1-x-s}\} \leq H_s + \frac{H_s}{s}\left(1-\frac{\Phi_s^2}{2}\right)^k. \]
%     The inequality trivially holds for $x<s$ and $x>1-s$, since we have $h_0(x) \leq H_s$ by definition (with complementation in the second case) and $h_k(x) \leq h_0(x)$.
% \end{proof}

\subsubsection{An Important Inequality involving the operator \texorpdfstring{$M$}{M}}

A little bit of thought makes it quite clear that to analyze the speed of mixing of Markov chains, the spectrum of the operator $M$ is an important parameter.

\begin{ftheo}
\label{rapid mixing expec 0}
	Let $\mathcal{M}$ be a time-reversible Markov scheme with conductance $\Phi$. Then for every $g\in L^2$ with $\expec[g]=0$,
	\[ \langle g,Mg\rangle \leq \left(1-\frac{\Phi^2}{2}\right)\norm{g}^2. \]
\end{ftheo}

\begin{proof}
	As might be expected, we use \Cref{eqn: self-adjoint spectral radius 1} in this proof. It suffices to show that if $\expec[g]=0$,
	\[ \int_\Omega \int_\Omega (g(u)-g(v))^2 \d{P}_u(v)\d{Q}(u) \geq \Phi^2 \norm{g}^2. \]
	Choose a median $r$ of $g$, that is, a real number such that $Q(\{x:g(x)>r\}) \leq 1/2$ and $Q(\{x:g(x)<r\}) \leq 1/2$. Let $h(x)=\max\{g(x)-r,0\}$. Observe that
	\[ \int_\Omega \int_\Omega (g(u)-g(v))^2 \d{P}_u(v)\d{Q}(u) \geq \int_\Omega \int_\Omega (h(u)-h(v))^2 \d{P}_u(v)\d{Q}(u) \]
	Therefore, it suffices to bound the quantity on the right suitably. To do this, use the Cauchy-Schwarz inequality to get
	\begin{align*}
		\int_\Omega \int_\Omega (h(u)-h(v))^2 \d{P}_u(v)\d{Q}(u) &\geq \frac{\left(\int_\Omega \int_\Omega |h^2(u)-h^2(v)| \d{P}_u(v)\d{Q}(u)\right)^2}{\int_\Omega \int_\Omega (h(u)+h(v))^2 \d{P}_u(v)\d{Q}(u)}
	\end{align*}
	Now, by definition, $Q(\{h\geq 0\}) \geq 1/2$. Therefore,
	\[ \int_\Omega h^2\d{Q} \geq \frac{1}{2} \int_\Omega (g(x)-r)^2 = \frac{\norm{g}^2+r^2}{2} \geq \frac{\norm{g}^2}{2}. \]
	To bound the denominator,
	\[ \int_\Omega \int_\Omega (h(u)+h(v))^2 \d{P}_u(v)\d{Q}(u) \leq 2 \int_\Omega \int_\Omega (h^2(u)+h^2(v)) \d{P}_u(v)\d{Q}(u) = 2\norm{h} \]
	For each $t$, define $A_t = \{x\in\Omega : h(x)^2 \geq t\}$. Then
	\begin{align*}
		\int_\Omega \int_\Omega |h^2(u)-h^2(v)| \d{P}_u(v)\d{Q}(u) &= 2 \int_\Omega \int_{A(h^2(u))} (h^2(v)-h^2(u)) \d{P}_u(v)\d{Q}(u) \\
			&= 2 \int_\Omega \int_{h^2(u)}^\infty P_u(A(t)) \d{t}\d{Q}(u) & (\text{by Fubini's Theorem}) \\
			&= 2 \int_0^\infty \int_{\Omega\setminus A(t)} P_u(A(t)) \d{Q}(u)\d{t} \\
			&\geq 2\Phi \int_0^\infty Q(A(t))\d{t} = 2\Phi \int_\Omega h^2\d{Q} = 2\Phi\norm{h}^2.
	\end{align*}
	The result follows directly, since we now have
	\[ \int_\Omega \int_\Omega (h(u)-h(v))^2 \d{P}_u(v)\d{Q}(u) \geq \frac{4\Phi^2\norm{h}^4}{4\norm{h}^2} = 2\Phi^2\norm{h}^2 \geq \Phi^2\norm{g}^2, \]
	which is exactly what we set out to show.
\end{proof}

\begin{corollary}
	Let $\mathcal{M}$ be a time-reversible Markov scheme with conductance $\Phi$. Then for every $f\in L^2$ with $\expec[f]=0$,
	\[ \langle f,M^k f\rangle \leq \left(1-\frac{\Phi^2}{2}\right)^k\norm{f}^2. \]
\end{corollary}

We omit the proof of the above.\footnote{It may be shown by considering $\tilde{M}$, the restriction of $M$ to the invariant subspace $\expec[f]=0$. By the above lemma, $\norm{\tilde{M}}\leq 1-\Phi^2/2$. Then $\norm{\tilde{M}^k} \leq \norm{\tilde{M}}^k$, which immediately gives the result.} This quite neatly captures the fact that rapid mixing depends heavily on conductance. Indeed, it implies that $\norm{M^k f} \leq (1-\Phi^2/2)^k \norm{f}$, so as time progresses, $f$ ``flattens out" and goes closer to $0$.

The next inequality can be thought of a central limit theorem style inequality.

\begin{theorem}
	\label{bounding variance}
	Let $\mathcal{M}$ be a time-reversible Markov scheme with stationary distribution $Q$ and let $w_1,w_2,\ldots$ be a Markov chain generated by $\mathcal{M}$ with initial distribution $Q$. Let $F\in L^2$ and $\xi = \sum_{i=0}^{T-1}F(w_i)$ for some $T$. Then,
	\[ \Var[\xi] \leq \frac{4T}{\Phi^2}\norm{F}^2 \]
\end{theorem}
\begin{proof}
	We may assume that $\expec[\xi] = 0$. Then \Cref{rapid mixing expec 0} implies that
	\begin{align*}
		\Var[\xi] = \expec[\xi^2] &= \sum_{0\leq i,j\leq T-1} \expec[F(w_i)F(w_j)] \\
			&= T\norm{F}^2 + 2 \sum_{0\leq i<j\leq T-1} \expec[F(w_0)F(w_{|i-j|})] \\
			&= T\norm{F}^2 + 2 \sum_{k=1}^{T-1} (T-k) \langle F,M^{k}F\rangle \\
			&\leq \norm{F}^2 \left(T + 2 \sum_{k=1}^{T-1} (T-k)\left(1-\frac{\Phi^2}{2}\right)^k\right) \\
			&< 2T\norm{F}^2 \sum_{k=0}^{T-1} \left(1-\frac{\Phi^2}{2}\right)^k \leq \frac{4T}{\Phi^2} \norm{F}^2.
	\end{align*}
\end{proof}

\subsubsection{Metropolis Chains}

While we have used Metropolis chains previously, let us define them more formally for the sake of completeness.

\begin{fdef}[Metropolis Chain]
	\label{def: metropolis chain}
	Let $\mathcal{M}$ be a time-reversible Markov chain on $(\Omega,\mathcal{A})$ and let $F:\Omega\to\R$ be a non-negative measurable function. Suppose that $\overline{F} = \displaystyle\int_\Omega F\d{Q}$ is finite. Denote by $\mu_F$ the measure with density $F$. We then define the \textit{filtering} of $\mathcal{M}$ by $F$, denoted $\mathcal{M}/F$, as the Markov scheme with transition probabilities
	\[
		P_u^F(A) = 
		\begin{cases}
			\displaystyle\int_A \min\left\{1,\frac{F(v)}{F(u)}\right\}\d{P}_u(v), & u\not\in A, \\
			\displaystyle\int_A \min\left\{1,\frac{F(v)}{F(u)}\right\}\d{P}_u(v) + \int_{\Omega} \max\left\{0,1-\frac{F(v)}{F(u)}\right\}\d{P}_u(v),  & u\in A.
		\end{cases}
	\]
\end{fdef}

And as mentioned, this modified chain converges to a distribution proportional to $F$.

\begin{theorem}
	If $\mathcal{M}$ is time-reversible, then $\mathcal{M}/F$ is also time-reversible and has stationary distribution $Q_F = (1/\overline{F})\mu_F$.
\end{theorem}
\begin{proof}
	From the remark after \Cref{def: ergodic flow}, it suffices to show that for any disjoint measurable sets $A$ and $B$ ($B=\Omega\setminus A$ in particular),
	\[ \int_B P_u^F(A)\d{Q}_F(u) = \int_A P_u^F(B) \d{Q}_F(u), \]
	that is,
	\[ \int_B \int_A \min\left\{1,\frac{F(v)}{F(u)}\right\} \frac{F(u)}{\overline{F}} \d{P}_u(v) \d{Q}(u) = \int_A \int_B \min\left\{1,\frac{F(v)}{F(u)}\right\} \frac{F(u)}{\overline{F}} \d{P}_u(v) \d{Q}(u). \]
	Rewriting, we want to show that
	\[ \int_B \int_A \min\left\{F(u),F(v)\right\} \d{P}_u(v) \d{Q}(u) = \int_A \int_B \min\left\{F(u),F(v)\right\} \d{P}_u(v) \d{Q}(u), \]
	but this follows from the time-reversibility of $\mathcal{M}$.
\end{proof}

Before we move on to the main algorithm, we set up some prerequisite results to make the discussion in the subsequent section more natural.


\subsection{An Isoperimetric Inequality}

\subsubsection{Log-Concave Functions}

A function $f:\R^n\to\R^{\geq 0}$ is said to be \textit{log-concave} if for any $x,y\in\R^n$ and $0<\lambda<1$,
\[ f(\lambda x + (1-\lambda)y) \leq f(x)^\lambda f(y)^{1-\lambda}. \]
If $f$ is positive, this just means that $\log f$ is concave. While we did not mention this by name, we discussed similar ideas back in the (multiplicative) Brunn-Minkowski inequality (\ref{eqn multiplicative brunn minkowski}).\\

It is quite obvious that if $f$ and $g$ are log-concave functions, then so are $fg$ and $\min\{f,g\}$. The following is far less obvious however.

\begin{lemma}
\label{convolution of log concave functions is log concave}
	Let $f$ and $g$ be two log-concave functions. If their convolution $h$ defined by $h(x)=\int_{\R^n}g(u)f(x-u)\d{u}$ is well-defined, it is log-concave.
\end{lemma}

Let $F$ be a non-negative integrable funciton on $\R^n$. As in \Cref{conductance isoperimetric inequality}, denote by $\mu_F$ the measure with density $F$. We then get the following corollary.

\begin{corollary}
\label{F(x+K) log concave K convex}
	Let $K\subseteq\R^n$ be a convex body and $F:\R^n\to\R$ a log-concave function. Then $\mu_F(x+K)$ is a log-concave function of $x$.
\end{corollary}

This is quite easily proved by setting $f=F$ and $g=\indic_K$ in \Cref{convolution of log concave functions is log concave}.\\

Setting $K$ to be a rectangle aligned with the axes having edges of length $\varepsilon$ in $k$ directions and $1/\varepsilon$ in the remaining directions and then taking $\varepsilon\to 0$ gives

\begin{corollary}
	\label{cor: 4.30}
	Let $F:\R^n\to\Rp$ be a log-concave function with finite integral. Then for any subset $\{x_1,\ldots,x_k\}$ of variables, the function
	\[ \int_{\R} \int_{\R} \cdots \int_{\R} F \d{x}_1\ldots\d{x_k} \]
	in the remaining variables is log-concave.
\end{corollary}

Slightly more generally, setting $f=\indic_{K'}$ and $g=\indic_K$, we get that the function $x\mapsto \vol((x+K')\cap K)$ is log-concave.

\begin{corollary}
	Let $K$ and $K'$ be two convex bodies and $t>0$. If $\{x\in\R^n:\vol((x+K')\cap K)>t\}$ has an interior point, then it is convex. In particular, for any $0<s<1$,
	\[ K_s = \{x\in K : \vol((x+K)\cap K) \geq s\vol(K)\} \]
	is a convex body.
\end{corollary}

\subsubsection{An Improvement of a Past Result}

The main result of this section is \Cref{improvement of conductance isoperimetric inequality}, which is an improvement of \Cref{conductance isoperimetric inequality}.

\begin{ftheo}
\label{localization lemma}
	Let $g$ and $h$ be lower semicontinuous Lebesgue integrable functions on $\R^n$ such that their integrals on $\R^n$ are positive. Then there exist points $a,b\in\R^n$ and a linear function $\ell:[0,1]\to\R^{\geq 0}$ such that
	\[ \int_0^1 \ell(t)^{n-1}g((1-t)a+tb)\d{t} > 0 \text{ and } \int_0^1 \ell(t)^{n-1}h((1-t)a+tb)\d{t} > 0 \]
\end{ftheo}

The above result is quite useful and reduces integrals in $n$-dimensions to a single dimension. We shall solidify this relationship further and give a few related results in \Cref{sec: 5.1.2,sec: 5.1.3}. \\

It suffices to consider the case where $g$ and $h$ are continuous. If not, we can find some monotone (strictly) increasing sequence of continuous integrable functions $(g_k)$ and $(h_k)$ that converge to $g$ and $h$. We then have
\[ \lim_{k\to\infty}\int_{\Rn} g_k = \int_{\Rn} g > 0 \]
so for sufficiently large $k$, $\int_{\Rn} g_k > 0$ and likewise, $\int_{\Rn} h_k > 0$. It then suffices to show it for the $g_k$ and $h_k$ for sufficiently large $k$, so we may assume continuity.\\
Further, note that it suffices to show the inequality with $\geq 0$ instead of $>0$. Indeed, if we prove it for the weak inequality, we can apply it to $(g-a)$ and $(h-a)$ for some function $a$ that is everywhere positive and continuous with a sufficiently small integral to get the strict inequality.\\

We continue the proof over two lemmas. In \Cref{localization lemma lemma 2}, we in fact obtain the exact required result with a concave function instead of a linear one. Over the remainder, we refine it to obtain linearity.

\begin{lemma}
\label{localization lemma lemma 1}
	There exists a sequence $K_0\supseteq K_1 \supseteq \cdots$ of convex bodies such that for each $i$,
	\[ \int_{K_i} g(x)\d{x} > 0 \text{ and } \int_{K_i} h(x)\d{x} > 0 \]
	and $K=\bigcap_i K_i$ is a point or a segment.
\end{lemma}
\begin{proof}
	We may choose $K_0$ to be a sufficiently large ball. To find $K_{i+1}$ given $K_i$, we use a bisection argument. We choose a half-space $H$ such that
	\[ \int_{K_i\cap H} g(x)\d{x} = \frac{1}{2} \int_{K_i} g(x)\d{x}. \]
	The hyperplane supporting the half-space is referred to as a \textit{bisecting hyperplane}. It remains to show that it is possible to choose a $H$ at each step such that the $K_i$ shrink to a $0$ or $1$-dimensional body.\\

	\textbf{Claim.} Given any $(n-2)$-dimensional affine subspace $A$, there is a bisecting hyperplane containing it.\\
	This is easily shown by taking any hyperplane containing $A$, rotating it about $A$, and using the continuity of the resulting map that maps a hyperplane to the integral over the half-space on a particular side. This reduces to showing that for a continuous map $g$ from the unit circle in $\R^2$ to $\R$, there exists $x$ on the unit circle such that $f(x)=f(-x)$. This is trivial on modifying the function into a continuous one from $\R\to\R$ and then using the intermediate value property.\footnote{Alternatively, this is a trivial application of the \href{https://en.wikipedia.org/wiki/Borsuk-Ulam_theorem}{Borsuk-Ulam Theorem}.}\\

	Now, choose $A_0,A_1,\ldots$ to be a sequence of $(n-2)$-dimensional affine subspaces such that at least one of them passes through any point with rational coordinates.\footnote{This is justified since $\Q^n$ is countable.} For each $i$, let $P_i$ be the bisecting hyperplane of $K_i$ that passes through $A_i$ and define $K_{i+1}=A_i\cap H_i$, where $H_i$ is a half-space with supporting hyperplane $P_i$.\\
	We wish to show that $\bigcap_i K_i$ is at most $1$-dimensional. Suppose instead that it is (at least) $2$-dimensional. Then the projection of $K$ onto one of the planes spanned by two coordinates axes (say $x_1$ and $x_2$) must be two-dimensional, and therefore has a rational interior point $(r_1,r_2)$. However, the hyperplane defined by $x_1=r_1$, $x_2=r_2$ is one of the $A_i$, and in this case, $P_i$ bisects $K$, and therefore also $K_{i+1}$, which is a contradiction.
\end{proof}

In the case where the resulting $K$ is a point $a$, it follows that $g(a)\geq 0$ and $h(a)\geq 0$, so \Cref{localization lemma} follows for $b=a$ and $\ell\equiv 1$.\\
Therefore, we assume that $K$ is a segment. Let $a$ and $b$ be the endpoints of this segment.

\begin{lemma}
\label{localization lemma lemma 2}
	There exists a concave function $\psi:[0,1]\to\Rp$, not identically zero, such that
	\[ \int_0^1 \psi(t)^{n-1}g((1-t)a+tb)\d{t} \geq 0 \text{ and } \int_0^1 \psi(t)^{n-1}h((1-t)a+tb)\d{t} \geq 0. \]
\end{lemma}
\begin{proof}
	Assume without loss of generality that $a=0$ and $b=e_1$. For $t\in\R$, set $Z_t = \{x\in\Rn : x_1=t\}$ and for each $i\in\N$,
	\[ \psi_i(t) = \left(\frac{\vol_{n-1}(K_i\cap Z_t)}{\vol(K_i)}\right)^{1/(n-1)}. \]
	Denote by $\alpha_i$ and $\beta_i$ the maximum and minimum of $x_1$ over $K_i$. Since $K\subseteq K_i$, $\alpha_i\geq 1$ and $\beta_i\leq 0$ for each $i$ and moreover, $\alpha_i\to 1$ and $\beta_i\to 0$. By \nameref{brunn's theorem}, each $\psi_i$ is concave on $[\beta_i,\alpha_i]$.\\
	Note that on any closed subinterval $[s,t]\subseteq(0,1)\subseteq[\beta_i,\alpha_i]$, the $(\psi_i)$ are Lipschitz (due to concavity).\footnote{See \href{https://math.stackexchange.com/a/2662341/447210}{this StackExchange answer} if you are unfamiliar with the result.} In particular, if $\delta\leq s\leq t\leq 1-\delta$, for any $s\leq x\leq y\leq t$,
	\[ |\psi_i(y)-\psi_i(x)| \leq \frac{\sup_{x\in (0,1)} \psi_i(x)}{\delta} |y-x|. \]
	The above implies that if we show uniform boundedness of the $(\psi_i)$ on $[s,t]$, then (uniform) equicontinuity follows as well. And indeed, since the $(\psi_i)$ are concave, then letting $\psi_i$ attain its supremum in $[s,t]$ at $x_i$, we have 
	\[ 1 \geq \int_s^t \psi_i(t)^{n-1}\d{t} \geq \int_{(s+x_i)/2}^{(x_i+t)/2} \left(\frac{\sup_{x\in[s,t]} \psi_i(x)}{2}\right)^{n-1}\d{t}, \]
	which, after a straightforward simplification, implies uniform boundedness.\\
	Therefore, by the Arzel\'a-Ascoli Theorem, the $(\psi_i)$ have a uniformly convergent subsequence on any $[s,t]$; let $\psi$ be the resulting limit function. Extending this appropriately to a function on $[0,1]$, we clearly have that $\psi$ is non-negative, concave, and
	\[ \int_0^1 \psi(t)^{n-1}\d{t} = 1. \]
	Now, setting $x=(t,y)$ for $t\in\R$ and $y\in\R^{n-1}$,
	\begin{align*}
		\int_{K_i} g(x)\d{x} &= \int_{\beta_i}^{\alpha_i} \int_{K_i\cap Z_t} g(t,y)\d{y}\d{t} \\
			&= \int_{\beta_i}^{\alpha_i} \left( \frac{1}{\vol_{n-1}(K_i\cap Z_t)} \int_{K_i\cap Z_t} g(t,y)\d{y} \right) \vol(K_i) \psi_i(t)^{n-1} \d{t}. \\
		\frac{1}{\vol(K_i)}\int_{K_i} g(x)\d{x} &= \int_{\beta_i}^{\alpha_i} \left( \frac{1}{\vol_{n-1}(K_i\cap Z_t)} \int_{K_i\cap Z_t} g(t,y)\d{y} \right) \psi_i(t)^{n-1} \d{t}.
	\end{align*}
	By definition, the left hand side is non-negative and
	\[ \int_{\beta_i}^{\alpha_i} \left( \frac{1}{\vol_{n-1}(K_i\cap Z_t)} \int_{K_i\cap Z_t} g(t,y)\d{y} \right) \psi_i(t)^{n-1} \d{t} \to \int_0^1 g(t,0)\psi(t)^{n-1}\d{t}. \]
	Therefore, this integral is non-negative and the claim is proved (the expression for $h$ is non-negative by an identical argument).
\end{proof}

Let us now get back to the main proof equipped with the above two intermediate steps.\\
We need to somehow get from the above concave function $\psi$ to a linear function.

\begin{proof}[Continued Proof.]
	Define $\psi$, $a$, and $b$ as in the previous lemma but further, make two assumptions: 
	\begin{itemize}
		\item Choose $a,b\in K$ such that $\norm{a-b}$ is minimum. When we say ``minimum'', we mean that one of the two integrals involved is equal to $0$, so we cannot make the segment any shorter. Without loss of generality, assume $a=0$ and $b=e_1$.
		\item Let $[\alpha,\beta]\subseteq[0,1]$ be the largest interval that $\psi$ is linear on. That is, $\psi$ is linear on $[\alpha,\beta]$ and $|\beta-\alpha|$ is maximum -- in the case where $\psi$ is nowhere linear, $\beta-\alpha=0$ (How can we assume that such an interval exists?).
	\end{itemize}
	For the sake of simplicity, define $\pi_1$ to be the projection of $x\in\Rn$ onto the first axis ($\pi_1(x)=x_1$). Define the functions $\hat{g},\hat{h}:\Rn\to\R$ by
	\[ \hat{g}(x)=g(\pi_1(x)e_1) \text{ and } \hat{h}(x)=h(\pi_1(x)e_1). \]
	Also, consider the convex body
	\[ K' = \{x\in\Rn : 0\leq x_1\leq 1, x_2,\ldots,x_n\geq 0,\text{ and }x_2+\cdots+x_n\leq\psi(x_1)\}. \]
	The body $K'$ can be visualized more naturally as taking the union of the $(n-1)$-dimensional simplices spanned by $te_1, te_1+\psi(t)e_2,\ldots,te_1+\psi(t)e_n$ over all $0\leq t\leq 1$. As a result,
	\[ \vol_{n-1} (K'\cap Z_t) = \frac{\psi(t)^{n-1}}{n!}, \]
	where $Z_t=\pi_1^{-1}(t)$ is defined as in the previous lemma.\\
	Then,
	\[ \int_{K'} \hat{g}(x)\d{x} = \int_0^1 \vol(K' \cap Z_t) g(t e_1) \d{t} = \frac{1}{(n-1)!} \int_0^1 \psi(t)^{n-1} g(te_1) \geq 0, \]
	where the last inequality follows from the definition of $\psi$ in the previous lemma. We have a similar inequality for the integral of $\hat{h}$ as well.\\
	Since we have taken $\norm{a-b}$ to be minimal, we may assume without loss of generality that $\int_{K'}\hat{g}(x)\d{x}=0$.\\
	% Now, for any convex body $L$, define the function
	% \[ \psi_L(x) = \vol(L\cap Z_t)^{1/(n-1)}. \]
	% Brunn's Theorem implies that $\psi_L$ is concave.\\
	Consider the $(n-2)$-dimensional affine space $A$ defined by $x_1=\sigma$ and $x_2+\cdots+x_n=\tau$ for some $0<\sigma<1$ and $\tau>1$ (which we shall fix later). Suppose that $A$ intersects the interior of $K'$. Then by \Cref{localization lemma lemma 1}, there exists a hyperplane $H$ through $A$ that splits $K'$ into convex bodies $L_A$ and $L_A'$ such that
	\[ \int_{L_A}\hat{g}(x)\d{x} = \int_{L_A'}\hat{g}(x)\d{x} = 0 \text{ and } \int_{L_A}\hat{h}(x)\d{x} \geq 0. \]
	Note that
	\begin{equation*}
	\tag{$*$}
	\label{eqn: localization lemma intermediate 1}
		L_A'\cap (Z_0\cap K')\neq\emptyset \text{ and } L_A'\cap (Z_1\cap K')\neq\emptyset
	\end{equation*}
	since otherwise, the minimality of $\norm{a-b}$ is contradicted.\footnote{we can then restrict ourselves to $L_A$ instead of $K'$ while maintaining non-negativity of the two integrals.} As a consequence, $H$ cannot be orthogonal to the $x_1$-axis and so, it can be described as $x_2+\cdots+x_n=\ell(x_1)$ for some linear function $\ell$ (due to the structure of the $(n-2)$-dimensional affine space $A$ it contains). Due to (\ref{eqn: localization lemma intermediate 1}), this $\ell$ must also satisfy $\ell(0)\geq 0$ and $\ell(1)\geq 0$. Further, observe that we can succinctly describe $L_A\cap K'$ as
	\[ \left\{x\in\Rn : 0 \leq x_1 \leq 1, x_2,\ldots,x_n\geq 0, \text{ and } x_2+\cdots+x_n \leq \psi'(t) \right\}, \]
	where
	\[ \psi'(t) = \min\{\psi(t),\ell(t)\} \]
	is concave as well.\\

	Now, since $\psi$ is concave on $[0,1]$, it is continuous on $(0,1)$. Therefore, if there is a discontinuity, it must be at either $0$ or $1$. Based on this, we take $3$ cases and in each, construct an $A$ that yields the desired linear function.
	\begin{itemize}
		\item \textbf{Case 1.} $\psi(0)=\psi(1)=0$. Consider the affine space $A$ for $\sigma=1/2$. Due to (\ref{eqn: localization lemma intermediate 1}), $(1/2) e_1 \in L_A$ (Why?). Since $\ell(0),\ell(1)\geq 0$ and $\ell(1/2)=\tau$, as we take $\tau\to 0$, $\ell$ tends to $0$ \textit{uniformly} on $[0,1]$. Making $\tau$ sufficiently small, we see that $\psi'$ satisfies the required condition and becomes linear on a length tending to $1$ (it becomes equal to $\ell$ for a larger portion as $\tau$ becomes smaller and smaller).

		\item \textbf{Case 2.} $\psi(0)=0$ and $\psi(1)>0$ (the reversed case is nearly identical). Consider the affine space $A$ for $\tau=\psi(1)\sigma$. (\ref{eqn: localization lemma intermediate 1}) implies that $0\leq\ell(1)\leq\psi(1)$. However, in this case, $\psi'$ must be linear on $[\sigma,1]$, so we can obtain linearity on a length tending to $1$.

		\item \textbf{Case 3.} $\psi(0),\psi(1)>0$.%\footnote{While slightly irrelevant, it might be worth noting that we could try applying the same strategy as in Case 2; however, an issue arises because instead of $[\sigma,1]$, we only get the smaller of the two intervals $[0,\sigma]$ and $[\sigma,1]$.}
		Consider a convex(!) function $\eta:[0,1]\to\R^{\geq 0}$ such that
		\begin{itemize}
			\item $0\leq\eta(0)\leq\psi(0)$ and $0\leq\eta(1)\leq\psi(1)$,
			\item the convex body $K_\eta$ defined by
			\[ K_\eta = \{ x\in\Rn : 0\leq x_1\leq 1, x_2,\ldots,x_n\geq 0, \text{ and } \eta(x_1)\leq x_2+\cdots+x_n\leq\psi(x_1) \} \]
			satisfies
			\[ \int_{K_\eta} \hat{g}(x)\d{x} = 0 \text{ and } \int_{K_\eta} \hat{h}(x)\d{x}\geq 0, \text{ and } \]
			\item $\displaystyle\int_0^1 \eta(x)\d{x}$ is maximal.
		\end{itemize}
		Such an $\eta$ must exist since the zero function satisfies the first two conditions.\\
		If $\psi(0)=\eta(0)$ or $\psi(1)=\eta(1)$, then we are done, since $\psi-\eta$ satisfies the original conditions (non-negative integrals), and it is already covered in the first two cases.\\
		Otherwise, we have $\eta(0)<\psi(0)$ and $\eta(1)<\psi(1)$. Let $(\sigma,\tau)$ be the intersection point of the segments joining $(0,\eta(0))$, $(1,\psi(1))$ and $(0,\psi(0))$, $(1,\eta(1))$ and $A$ be the corresponding affine space. Let $H$ be a hyperplane containing $A$ dividing space into two half-spaces $M$ and $M'$ such that
		\[ \int_{K_\eta \cap M} \hat{g}(x)\d{x} = \int_{K_\eta \cap M'} \hat{g}(x)\d{x} = 0.  \]
		Also, choose $M$ to be the half-space that contains $(1/2) e_1$ (the one that ``faces down'').\\
		It is clear from contruction that any $(n-1)$-dimensional hyperplane containing $A$, $H$ in particular, either intersects both $Z_0\cap K_\eta$ and $Z_1\cap K_\eta$ or neither. The latter cannot happen due to (\ref{eqn: localization lemma intermediate 1}) (or rather, the corresponding expression for $K_\eta$ rather than $K'$).\\
		Now,
		\[ \int_{M'} \hat{h}(x)\d{x} < 0. \]
		Indeed, otherwise, the linear function $\ell$ corresponding to $H$ contradicts the maximality of $\eta$. However, this implies that
		\[ \int_{M\cap K'} \hat{g}(x)\d{x} = \int_{K'} \hat{g}(x)\d{x} - \int_{M'}\hat{g}(x)\d{x} = 0 \]
		and
		\[ \int_{M\cap K'} \hat{h}(x)\d{x} = \int_{K'} \hat{h}(x)\d{x} - \int_{M'}\hat{h}(x)\d{x} > 0. \]
		Finally, $M\cap K'$ is a truncated cone, thus completing the proof.\qedhere
	\end{itemize}
\end{proof}

The above result has some interesting corollaries, namely \Cref{improvement of conductance isoperimetric inequality,inequality integral setminus ball}, which are of interest to us. The following is an improvement of \Cref{conductance isoperimetric inequality}.

\begin{ftheo}
\label{improvement of conductance isoperimetric inequality}
	Let $K\subseteq\Rn$ be a convex body, $0<t<1$, and $K_1,K_2$ be two measurable sets in $K$ such that for any $a,b\in K$, the distance between $K_1\cap[a,b]$ and $K_2\cap[a,b]$ is at least $t\norm{a-b}$. Then for any log-concave function $F$ with support $K$,
	\[ \mu_F(M\setminus(K_1\cup K_2)) \geq \frac{2t}{1-t} \min\{\mu_F(K_1),\mu_F(K_2)\}. \]
\end{ftheo}

The above bound is tight when $K$ is a cylinder and $F$ is identically $1$. Note that $t \geq d(K_1, K_2)/\diam(K)$ (which explains why this is an improvement of \Cref{conductance isoperimetric inequality}).

\begin{proof}
	Suppose otherwise. Assume that $K_1$ and $K_2$ are open (otherwise, we can delete the boundary and enlarge them slightly to open sets while preserving both the assumptions and the inequality). Denote $K_3 = M\setminus(K_1\cup K_2)$. Define
	\[
		g(x) = 
		\begin{cases}
			F(x), & x\in K_1, \\
			-\frac{1-t}{2t}F(x), & x\in K_3, \\
			0, & \text{otherwise},
		\end{cases}
		\text{ and }
		h(x) = 
		\begin{cases}
			F(x), & x\in K_2, \\
			-\frac{1-t}{2t}F(x), & x\in K_3, \\
			0, & \text{otherwise}.
		\end{cases}
	\]
	By our assumption, the integrals of $g$ and $h$ over $\Rn$ are positive. We can then apply \Cref{localization lemma} to get a linear functional $\ell$ and $a,b\in\Rn$ such that
	\[ \int_0^1 \ell(u)^{n-1}g(ua+(1-u)b) \d{u} > 0 \text{ and } \int_0^1 \ell(u)^{n-1}h(ua+(1-u)b) \d{u} > 0. \]
	For each $1\leq i\leq 3$, define
	\[ H_i = \{u\in[0,1] : ua+(1-u)b \in K_i\} \text{ and } G(u) = \ell(u)^{n-1}F(ua+(1-u)b). \]
	Substituting $g$ and $h$ in the above equation, we get
	\begin{equation}
	\label{eqn: cond iso ineq imp contradictory eqn}
		\int_{H_3} G(u)\d{u} < \frac{2t}{1-t} \min\left\{\int_{H_1} G(u)\d{u}, \int_{H_2} G(u)\d{u}\right\}.
	\end{equation}
	Intuitively, the worst case to prove appears to be when $H_3$ is a single interval -- as we shall see later in the proof, we can reduce it to this case even in general. We show the following claim, which just asserts that \Cref{eqn: cond iso ineq imp contradictory eqn} is incorrect (for the single-interval case).
	% Observe that $G$ is log-concave.
	Define $\mu_G$ to be measure on $[0,1]$ with density $G$.\\

	\textbf{Claim.} For $0\leq s < s+t\leq 1$,
	\begin{equation}
	\label{eqn: 4.15}
		\mu_G([s,s+t]) \geq \frac{2t}{1-t} \min\{\mu_G([0,s]),\mu_G([s+t,1])\}.	
	\end{equation}

	Before we move to the proof, note that \Cref{conductance isoperimetric inequality}, which has a $t$ in place of $2t/(1-t)$ above, is trivial now. Indeed, since $G$ is unimodal\footnote{there is some $m\in[0,1]$ such that $G$ is monotonically increasing for $x\leq m$ and monotonically decreasing for $x\geq m$.},
	\[ \mu_G([s,s+t]) \geq t \inf_{x\in[s,s+t]} G(x) \geq t \max\{s, 1-s-t\}\inf_{x\in[s,s+t]}G(x) \geq t \min\{\mu_G([0,s]),\mu_G([s+t,1])\}. \]
	First, note that since $G$ is the product of two log-concave functions, it is log-concave as well. Choose constants $c,c_0$ such that $G(s)=c_0 e^{cs}$ and $G(s+t)=c_0 e^{c(s+t)}$. By the log-concavity of $G$, $G(u)\geq c_0 e^{cu}$ for $s\leq u\leq s+t$ and $G(u)\leq c_0 e^{cu}$ elsewhere. As a consequence, it suffices to show \Cref{eqn: 4.15} for the case where $G(u) = c_0 e^{cu}$. Further, we may assume that $c_0=1$. We wish to show that
	\[ e^{c(s+t)}-e^{cs} \geq \frac{2t}{1-t} \min\left\{e^{cs}-1, e^c - e^{c(s+t)}\right\}. \]
	The worst case is when $e^{cs}-1 = e^{c} - e^{c(s+t)}$ (Why?). That is,
	\[ e^{cs} = \frac{1+e^c}{1+e^{ct}}. \]
	We then aim to show that
	\[ (e^{ct}-1)\frac{1+e^c}{1+e^{ct}} \geq \frac{2t}{1-t} \left(\frac{1+e^c}{1+e^{ct}} - 1\right). \]
	Let $x=e^{ct}>1$ and $\lambda=1/t>1$. The equation then becomes
	\[ (x-1)(1+x^\lambda) \geq \frac{2}{\lambda-1}(x^\lambda - x), \]
	that is,
	\[ (\lambda+1)(x-x^\lambda) + (\lambda-1)(x^{\lambda+1}-1) \geq 0. \]
	Letting the expression on the left be $h(x)$, we get $h(1)=h'(1)=0$ and $h''(x)=(\lambda^2-1)\lambda(x^{\lambda-1}-x^{\lambda-2}) \geq 0$, thus proving the claim for all $x\geq 1$.\\

	Now, how do we reduce the general case problem to this? For each maximal interval $I=(a,b)$ contained in $H_3$ of length at least $t$, colour $[0,a]$ red if $\mu_G([0,a])<\mu_G([b,1])$ and $[b,1]$ red otherwise. The above claim implies that each interval introduces a red set of $\mu_G$-measure at most $\frac{1-t}{2t}\mu_G((a,b))$. The entirety of the red set then has measure at most $\frac{1-t}{2t}\mu_G(H_3)$. It then suffices to show that either $H_1$ or $H_2$ is completely red.\\
	Suppose otherwise. Denote by $U$ the region that is uncoloured. We then have that $U$ intersects both $H_1$ and $H_2$. By construction, $U$ is an open interval. Since the distance between $H_1$ and $H_2$ is at least $t$, $U\cap H_3$ must contain a subinterval (of length at least $t$). This is a contradiction because there is an adjacent red interval (that intersects $U$). 
\end{proof}

Setting $F$ to be identically $1$ on $K$ in the above result, we get the following.

\begin{corollary}
	Let $K\subseteq\Rn$ be a convex body, $0<t<1$, and $K_1,K_2$ be two measurable sets in $K$ such that for any $a,b\in K$, the distance between $K_1\cap[a,b]$ and $K_2\cap[a,b]$ is at least $t\norm{a-b}$. Then
	\[ \vol(M\setminus(K_1\cup K_2)) \geq \frac{2t}{1-t} \min\{\vol(K_1),\vol(K_2)\}. \]
	In particular, if $K$ has diameter $d$ and the distance between $K_1$ and $K_2$ is at least $1$,
	\[ \vol(M\setminus(K_1\cup K_2)) \geq \frac{2}{d-1} \min\{\vol(K_1),\vol(K_2)\}. \]
\end{corollary}

\begin{theorem}
	\label{inequality integral setminus ball}
	Let $F$ be a log-concave function on $\Rn$ and $\theta\leq 1$ such that
	\[ \int_{\Rn \setminus B_2^n} F(x)\d{x} = \theta \int_{\Rn} F(x)\d{x}. \]
	Then for every $u\geq 1$,
	\[ \int_{\Rn \setminus uB_2^n} F(x)\d{x} \leq \theta^{(u+1)/2} \int_{\Rn} F(x)\d{x}. \]
\end{theorem}
\begin{proof}
	Suppose otherwise. Then exactly as we did in the proof of \Cref{improvement of conductance isoperimetric inequality}, there exist $a,b\in\Rn$ and a linear function $\ell:[0,1]\to\Rp$ such that on setting
	\begin{align*}
		G(t) &= \ell(t)^{n-1}F((1-t)a+tb),\\
		H_1 &= \{t\in[0,1]: (1-t)a+tb\in B_2^n\},\\
		H_2 &= \{t\in[0,1]: (1-t)a+tb\in uB_2^n\setminus B_2^n\},\text{ and }\\
		H_3 &= [0,1]\setminus(H_1\cup H_2),
	\end{align*}
	we have
	\begin{equation}
	\label{eqn: 4.16}
		\int_{H_2\cup H_3} G(t)\d{t} \leq \theta\int_0^1 G(t)\d{t}
	\end{equation}
	and
	\begin{equation}
	\label{eqn: 4.17}
		\int_{H_3} G(t)\d{t} > \theta^{(u+1)/2}\int_0^1 G(t)\d{t}
	\end{equation}
	As seen before, $G$ is log-concave. Further, note that $H_1$ is an interval and $H_2,H_3$ consist of $1$ or $2$ intervals.\\

	If $0\not\in H_1$, we can choose a point $s\in H_1$ such that
	\[ \frac{\int_{[0,s]\cap H_1} G(t)\d{t}}{\int_{[s,1]\cap H_1} G(t)\d{t}} = \frac{\int_{[0,s]} G(t)\d{t}}{\int_{[s,1]} G(t)\d{t}}. \]
	We can then replace $[a,b]$ with one of $[a,(1-s)a+sb]$ and $[(1-s)a+sb,b]$ (that is, replace $[0,1]$ with $[0,s]$ or $[s,1]$).\footnote{Multiplying by the appropriate factors leaves \Cref{eqn: 4.16} unchanged in the restricted segment, and one of the two intervals will also satisfy the restriction of \Cref{eqn: 4.17}.} Therefore, we may assume wlog that $0\in H_1$, that is, $a\in B_2^n$.\\
	Let $H_1=[0,\alpha]$, $H_2=(\alpha,\beta]$, and $H_3=(\beta,1]$. It is easily shown that $\beta \geq (\frac{u+1}{2})\alpha$.\\

	Now, choose constants $c$ and $c_0$ such that
	\[ \int_0^\alpha G(t)\d{t} = \int_0^\alpha c_0e^{-ct}\d{t} \text{ and } \int_\beta^1 G(t)\d{t} = \int_\alpha^\infty c_0e^{-ct}\d{t}. \]
	Note that the log-concavity (unimodality in particular) of $G$ implies that for all $\alpha\leq t\leq\beta$, $G(t)\geq c_0e^{-ct}$. Therefore,
	\[ \int_0^1 G(t)\d{t} \geq \int_0^\infty c_0e^{-ct}\d{t}. \]
	However, \Cref{eqn: 4.16} then implies that
	\begin{align*}
		\theta &\geq \frac{\int_\alpha^1 G(t)\d{t}}{\int_0^1 G(t)\d{t}} \\
			&= 1 - \frac{\int_0^\alpha G(t)\d{t}}{\int_0^1 G(t)\d{t}} \\
			&\geq 1 - \frac{\int_0^\alpha c_0e^{-ct}\d{t}}{\int_0^\infty c_0e^{-ct}\d{t}} \\
			&= \frac{\int_\alpha^\infty c_0e^{-ct}\d{t}}{\int_0^\infty c_0e^{-ct}\d{t}} = e^{-c\alpha}.
	\end{align*}
	Using the above, we now have
	\begin{align*}
		\theta^{(u+1)/2} &\geq e^{-c\alpha(u+1)/2} \\
			&\geq e^{-c\beta} \\
			&= \frac{\int_\beta^\infty c_0e^{ct}\d{t}}{\int_0^\infty c_0e^{ct}\d{t}} \geq \frac{\int_\beta^1 G(t)\d{t}}{\int_0^1 G(t)\d{t}},
	\end{align*}
	thus resulting in a contradiction to \Cref{eqn: 4.17} and proving the claim.
\end{proof}

% Setting $F$ to be identically $1$ in the above result, we get the following.

\begin{corollary}
	\label{inequality integral setminus ball restricted to K}
	Let $K$ be a convex body in $\Rn$ and $\theta=\vol(K\setminus B_2^n)/\vol(K)$. Then $\vol(K\setminus uB_2^n)\leq\theta^{(u+1)/2}\vol(K)$.	
\end{corollary}
% *** HOW???

Before we conclude this section, we give two more lemmas that will be useful in the future.

\begin{lemma}
	\label{lemma 4.38}
	Let $K$ be a convex body and $\theta=\vol(K\setminus B_2^n)/\vol(K)$. Then $K\subseteq \frac{2n}{1-\theta}B_2^n$.
\end{lemma}
\begin{proof}
	Let $x\in K$ be the point farthest from the origin and $R=\norm{x}$. It suffices to show the result for $\conv{(K\cap B_2^n)\cup\{x\}}$ (Why?). Assume $K$ to be of this form.\\
	Note that $K$ is contained in
	\[ \left\{x+\left(\frac{R+1}{R-1}\right) v : x+v\in K\setminus B_2^n\right\}. \]
	Then,
	\[ \vol(K) \leq \left(\frac{R+1}{R-1}\right)^n \vol(K\setminus B_2^n) \leq \theta\left(\frac{R+1}{R-1}\right)^n \vol(K). \]
	Therefore,
	\[ R \leq \frac{2}{1-\theta^{1/n}} \leq \frac{2n}{1-\theta}.\qedhere \]
\end{proof}

\begin{lemma}
	\label{lemma 4.39}
	Let $0\leq t\leq 1$. If $\vol(K\setminus (x+K)) \leq 1/2 \vol(K)$, then $\vol(K\setminus (tx+K)) \leq (2t/3) \vol(K)$.
\end{lemma}
\begin{proof}
	Define $\psi:[0,1]\to\R$ by
	\[ \psi(u) = \frac{\vol(K\cap (ux+K))}{\vol(K)}. \]
	\Cref{cor: 4.30} (or rather, the line right after it) implies that $\psi$ is log-concave. We have $\psi(0)=1$ and $\psi(1)\geq 1/2$. Therefore, $\psi(t) \geq 2^{-t}$ so
	\[ \vol(K\setminus (tx+K)) \leq (1 - 2^{-t})\vol(K) \leq \frac{2t}{3}\vol(K).\qedhere \]
\end{proof}


\subsection{An \texorpdfstring{$\mathcal{O}^*(n^7)$}{O(n7)} Algorithm using Ball-Step}

As in most volume estimation algorithms, the basic idea remains nearly the same, the changes being only in the walk. The algorithm used here involves the ``ball-step'' mentioned earlier. It is worth noting instead of the ``ball'' in ball-step, one could use any other symmetric convex body $G$. An obvious choice is the cube, which is quite convenient to draw points uniformly randomly from from a programming perspective. Our analysis shall be done in this general case.

\subsubsection{Ball-Step and Bounding Conductance}

The basic walk $\mathcal{M}$ is as follows: when we are at $v_k$, we let $v_{k+1}=v_k$ with probability $1/2$. Otherwise, we generate a random vector $u$ from the uniform distribution on $G$. If $v_k+u\in K$, set $v_{k+1}=v_k+u$ and otherwise, set $v_{k+1}=v_k$. This is termed as the \textit{lazy random walk in $K$ with $G$-steps}.\\
In this particular algorithm, we further filter it to obtain a Metropolis chain (recall that we defined this in \Cref{def: metropolis chain}). That is, for any measurable $A$ such that $x\not\in A$,
\[ P_x(A) = \frac{1}{2\vol(G)} \int_{(x+G)\cap A} \min\left\{1,\frac{F(y)}{F(x)}\right\}\d{y} = \frac{1}{2\vol(G)} \int_{(x+G)\cap A} \min\left\{\frac{1}{F(y)},\frac{1}{F(x)}\right\}\d{\mu}_F(y) \]
and
\[ P_x(x) = \frac{1}{2} + \frac{1}{2\vol(G)} \int_{x+G} \max\left\{0,1-\frac{F(y)}{F(x)}\right\}\d{y}. \]
% The stationary distribution of the above chain is that 

Recall from the discussion after \Cref{def: conductance} that we denote by $H_t$ (for $0\leq t\leq 1/2$) the set of points such that
\[ \int_{x+G} \min\{F(x),F(y)\}\d{y} < tF(x)\vol(G), \]
the set of points where the probability of moving out is less than $t$.
We also saw that the $(Q_F(H_t)/2)$-conductance of the chain is at most $2t$.\\

The main result of this section, which is an improvement of a previous result, says that if the local conductance is large and the ``diameter'' (not the usual Euclidean diameter) of $K$ is small, then the overall conductance is large.

\begin{ftheo}
	\label{ftheo to bound conductance}
	Let $0\leq t\leq 1/2$, $0<\theta<1$, and $s=Q_F(H_t)$. Assume that for all $x,y\in K$,
	\begin{equation}
		\label{theo 4.40 eqn main}
		 \vol(G\cap (\theta(x-y)+G)) \leq \frac{1}{2}\vol(G).
	\end{equation}
	% *** \geq given, shouldn't it be \leq
	Then the $(7s/t)$-conductance of $\mathcal{M}(K,G)/F$ is at least $t^2\theta/18$.
\end{ftheo}

Our idea of diameter here is $1/\theta$ (where $\theta$ is the minimal value satisfying the required in the above theorem). It is seen that $1/\theta$ is the usual diameter under the norm whose unit ball is $\{x : \vol(G\cap(x+G))\geq\vol(G)/2\}$. If $G$ is the Euclidean ball, then $1/\theta$ grows as $\sqrt{n}d$, where $d$ is the usual Euclidean diameter of $K$.

\begin{proof}
	% Assume that $\overline{F}=1/2$.
	Alternatively, the above result states that if we split $K$ into measurable sets $S_1$ and $S_2$,
	\[ \int_{S_1} \int_{S_2\cap (x+G)} \min\{F(x),F(y)\}\d{y}\d{x} \geq \frac{t^2\theta\vol(G)}{18} \min\left\{Q_F(S_1)-\frac{7s}{t}, Q_F(S_2)-\frac{7s}{t}\right\}. \]
	% *** SHOULDN'T THERE BE AN EXTRA 2\overline{F} ON THE RIGHT??? 
	We may assume that $Q_F(S_1)$ and $Q_F(S_2)$ are both greater than $7s/t$.
	% The expression on the left can easily be bounded above by $\vol(G)\min\{\mu_F(S_1),\mu_F(S_2)\}$.\\
	For $i=1,2$, define
	\begin{align*}
		S_i' &= \left\{x\in S_i : \mu_F((x+G)\cap S_{3-i}) \geq \frac{t}{3}\vol(G)F(x) \right\}, \\
		S_i'' &= (S_i \setminus S_i') \setminus H_t,\text{ and} \\
		S_3 &= S_1'\cup S_2'\cup H_t.
	\end{align*}
	Note that $S_1''$, $S_2''$, and $S_3$ are disjoint (and cover the entirety of $K$). Each $S_i'$ represents the ``good'' points in $S_i$ -- the probability of transitioning to $S_{3-i}$ is decently high, or rather, sufficiently higher than the probability of even being at that point in the first place.\\
	The only problematic points are those in the $S_i''$.
	% $S_1'$ represents the points in $S_1$ where the probability of leaving to $S_2$ is (some factor) higher than the probability of even being at that point in the first place.\\

	\textbf{Claim.} If $x_1\in S_1''$ and $x_2\in S_2''$, then $\vol((x_1+G)\setminus(x_2+G))\geq\frac{t}{3}\vol(G)$.\\
	Suppose otherwise. Observe that the expression under consideration is symmetric in $x_1$ and $x_2$ so we may assume that $F(x_1)\leq F(x_2)$. Define $G_i=x_i+G$ for $i=1,2$. Since $S_i''$ and $H_t$ are disjoint,
	\[ \int_{G_i} \min\{F(x_i), F(y)\}\d{y} \geq t\vol(G)F(x_i). \]
	We also have (by the contradiction assumption) that
	\[ \int_{G_2\setminus G_1} \min\{F(x_2),F(y)\}\d{y} < \frac{t}{3}\vol(G)F(x_2). \]
	Therefore,
	\[ \int_{G_2\cap G_1} \min\{F(x_2),F(y)\}\d{y} \geq \frac{2t}{3} \vol(G)F(x_2). \]
	Since $x_2\not\in S_2'$,
	\[ \int_{G_2\cap G_1\cap S_1} \min\{F(x_2),F(y)\}\d{y} \leq \int_{G_2\cap S_1} \min\{F(x_2),F(y)\}\d{y} < \frac{t}{3}\vol(G)F(x_2). \]
	Because $S_1\cup S_2 = K$,
	\[ \int_{G_2\cap G_1\cap S_2} \min\{F(x_2),F(y)\}\d{y} > \frac{t}{3}\vol(G)F(x_2). \]
	Multiplying by $F(x_1)/F(x_2)$ on either side,
	\[ \int_{G_2\cap G_1\cap S_2} \min\{F(x_1),F(y)\}\d{y} > \frac{t}{3}\vol(G)F(x_1) \]
	and so,
	\[ \int_{G_1\cap S_2} \min\{F(x_1),F(y)\}\d{y} > \frac{t}{3}\vol(G)F(x_1). \]
	However, this means that $x_1\in S_2'$, proving the claim.\\

	Observe that the above claim is equivalent to saying that if $x_1\in S_1''$ and $x_2\in S_2''$, $\vol(G\cap ((x_2-x_1) + G)) > \frac{t}{3}\vol(G)$. Therefore, \Cref{lemma 4.39} implies that
	\[ \vol\left(G\cap \left(\frac{2}{t}(x_2-x_1) + G\right)\right) > \frac{1}{2}\vol(G). \]
	If we have some $a,b\in K$ such that $S_1''\cap[a,b]$ and $S_2''\cap[a,b]$ are non-empty and the distance between them is $\rho\norm{b-a}$, then the claim above together with \Cref{theo 4.40 eqn main} imply that $\rho>t\theta/2$. Using \Cref{improvement of conductance isoperimetric inequality}, %(or rather, \Cref{conductance isoperimetric inequality})
	\[ \mu_F(S_3) \geq \frac{t\theta}{1-t\theta/2} \min\{\mu_F(S_1''), \mu_F(S_2'')\}. \]
	Further note that
	\[ \mu_F(S_i'') \geq \mu_F(S_i) - \mu_F(S_i') - \mu_F(H_t) \geq \mu_F(S_i) - \mu_F(S_3) - s. \]
	That is,
	\[ \left(1+\frac{t\theta}{2}\right) \mu_F(S_3) \geq \frac{t\theta}{1-t\theta/2} \min\{\mu_F(S_1)-s, \mu_F(S_2)-s\}, \]
	so
	\[ \mu_F(S_3) \geq t\theta \min\{\mu_F(S_1)-s, \mu_F(S_2)-s\}. \]
	We also have
	\begin{align*}
		\int_{S_1} \int_{S_2\cap (x+G)} \min\{F(x), F(y)\} \d{y}\d{x} &\geq \int_{S_1'} \int_{S_2\cap (x+G)} \min\{F(x),F(y)\} \d{y}\d{x} \\
			&\geq \int_{S_1'} \frac{t}{3} \vol(G) F(x) \d{x} = \frac{t}{3}\vol(G)\mu_F(S_1'). 
	\end{align*}
	A similar inequality holds with $\mu_F(S_2')$ instead of $\mu_F(S_1')$ as well (invoking time-reversibility). Finally,
	\begin{align*}
		\int_{S_1} \int_{S_2\cap (x+G)} \min\{F(x),F(y)\}\d{y}\d{x} &\geq \frac{t}{6}\vol(G)\mu_F(S_1'\cup S_2') \\
			&= \frac{t}{6}\vol(G)\mu_F(S_3\setminus H_t) \\
			&\geq \frac{t}{6}\vol(G)(\mu_F(S_3) - s) \\
			&\geq \frac{t}{6}\vol(G)\left(t\theta\min\{\mu_F(S_1)-s,\mu_F(S_2)-s\} - s\right) \\
			&= \frac{t^2\theta}{6}  \min\left\{\mu_F(S_1)-s\left(1+\frac{1}{t\theta}\right), \mu_F(S_2)-s\left(1+\frac{1}{t\theta}\right)\right\} \\
			&\geq \frac{t^2\theta}{6}  \min\left\{\mu_F(S_1)-\frac{7s}{t}, \mu_F(S_2)-\frac{7s}{t}\right\}.
	\end{align*}
	% *** how does the last step come lol, you get anything but that (unless I'm an idiot)
	% *** paper has typo in the second/third-to-last expression
	% *** everything bad - last line in particular
\end{proof}

\begin{corollary}
	\label{cor to bound conductance}
	If the local conductance of $\mathcal{M}(K,G)$ is at least $t$ at each point, its conductance is at least $t^2\theta/18$.
\end{corollary}

\subsubsection{The Walk}
\label{the walk}

A simple albeit important problem is that of deciding how to sample from the body $G$ we are considering. As mentioned at the beginning of this section, a cube is quite convenient from a programmer's perspective. The Euclidean ball is not too problematic either. Letting $\xi_1,\ldots,\xi_n$ be iid standard normal distributions and $\eta$ be uniformly distributed in $[0,1]$, we see that
\[ v_0 = \left( \eta^{1/n} \frac{\xi_1}{\sqrt{\sum_i \xi_i^2}}, \ldots, \eta^{1/n} \frac{\xi_n}{\sqrt{\sum_i \xi_i^2}} \right) \]
is uniformly distributed in $B_2^n$.\\

We modify the $G$-walk described above into a suitable Metropolis chain, with the primary function being quite similar to that we used in the $\mathcal{O}^*(n^8)$ algorithm described in a previous section. Define
\[ \phi_K(x) = \min\{t\geq 0 : x\in tK\} \]
and $F_K(x)=e^{-\phi_K(x)}$. Clearly, $0<F_K \leq 1$. We often refer to these functions as just $\phi$ and $F$ if it is clear from context what convex body we are talking about (this is almost always $K$). First of all, we can use \Cref{find sphere volume,eqn: volume in terms of radial distance} to get
\[ \int_{\Rn} F = n\vol(K)\int_0^\infty t^{n-1}e^{-t}\d{t}. \]

That is,
\begin{equation}
	\vol(K) = \frac{1}{n!} \int_{\Rn} F.
\end{equation}

Let $h:\Rp\to\R$ be any function such that $\displaystyle\int_0^\infty h(t)t^{n-1}\d{t}$ exists. Then another useful corollary of \Cref{find sphere volume,eqn: volume in terms of radial distance} is that
\begin{equation}
	\label{eqn: 4.20}
	\int_{\Rn} h(\phi(x)) = n\vol(K)\int_0^\infty h(t)t^{n-1}\d{t}.
\end{equation}


Now, define
\[ \lambda(s) = \frac{1}{s}\left(\frac{1}{(n-1)!}\int_0^s e^{-t}t^{n-1}\d{t}\right)^{1/n}. \]
The reason for choosing such a convoluted function is made clear by the following lemma.
\begin{lemma}
	\label{final thing uniformly distrib}
	Let $v$ be randomly distributed in $\R^n$ with density $e^{-\phi(v)}/(n-1)!$. Then
	\[ H(v) = \lambda(\phi(v))v \]
	is uniformly distributed on $K$.
\end{lemma}
\begin{proof}
	Set $h(t)=e^{-t}/(n-1)!$ and fix some $0\leq s\leq 1$. Observe that the set $sK$ is mapped by $H$ to $s\lambda(s)K$.\footnote{if $x\in s\partial K$, then $H(x)=\lambda(s)x$.} Obviously, the probability of $s\lambda(s)K$ under the uniform distribution on $K$ is $(s\lambda(s))^n$. The probability of $sK$ under the given density $h$ is
	\[ \int_{sK} \frac{e^{-\phi(x)}}{(n-1)!} \d{x} = \int_0^s h(t) t^{n-1} \d{t}. \]
	By the definition of $\lambda$, these two quantities are equal. Since we can similarly restrict ourselves to any fixed cone similarly, the assertion holds and $H(v)$ is uniform.
\end{proof}


While we used usual (deterministic) sandwiching in our earlier algorithm, it turns out that we can get an algorithm that gives a better sandwiching ratio without being too costly in terms of time by introducing randomness.\\

The sandwiching we take is such that more than $2/3$ of the volume of $B_2^n$ is in $K$ and for a $1\leq m\leq n^{3/2}$, more than $2/3$ of the volume of $K$ is contained in the convex body $mB_2^n$.\\
There exists a randomized algorithm to obtain a $m=n$ in the above (which we describe later), but for now, we shall analyze it for a general $m$.\\

Set $q=2mn\log(4/\varepsilon)$ and $t=10^{10}nq^2(n\log n + \log(2/\varepsilon))$.\\
In the algorithm, start with the uniform distribution on $B_2^n$ and do a lazy random walk in $qB_2^n$ up till time $t$ with $B_2^n$-steps, filtered by $F$ (as defined earlier in the section). Compute $w=H(v_t)$. We claim that $w$ is close to being drawn from the uniform distribution. This is stated more rigorously in the following, which is the main result of this section.

\begin{ftheo}
	\label{LovS algorithm works!}
	For any (Lebesgue) measurable set $A\subseteq K$, the random point $w$ described above satisfies
	\[ \left| \Pr\left[w\in A\right] - \frac{\vol(A)}{\vol(K)} \right| < \varepsilon. \]
	Moreover, it requires $\mathcal{O}(n^3m^2\log^2(1/\varepsilon)(n\log n + \log(1/\varepsilon)))$ membership tests and $\mathcal{O}(n^4 m^2 \log(1/\varepsilon) (n\log n + \log(1/\varepsilon)))$ arithmetic operations involving numbers of $\mathcal{O}(\log n)$ bits.
\end{ftheo}

The remainder of this section is dedicated to proving the above.

\begin{lemma}
	\label{better bound on spherical caps}
	Let $H$ be a halfspace in $\Rn$ and $B$ a unit ball whose center is at distance $t$ from $H$. Then
	\begin{enumerate}[(i)]
		\item if $t\leq 1/\sqrt{n}$,
		\[ \vol(B\cap H) > \left(\frac{1}{2} - \frac{t\sqrt{n}}{2}\right)\vol(B_2^n). \]
		\item if $t>1/\sqrt{n}$,
		\[ \frac{1}{10t\sqrt{n}}(1-t^2)^{(n+1)/2}\vol(B_2^n) < \vol(H\cap B) < \frac{1}{t\sqrt{n}}(1-t^2)^{(n+1)/2}\vol(B_2^n). \]
	\end{enumerate}
\end{lemma}

Observe that the above is just a (better) bound on the volume of a spherical cap, which we rudimentarily bounded back in \Cref{spherical cap lower bound,spherical cap upper bound}. We omit the proof of the above.\\

The above with the sandwiching we are taking implies that $(1/3n^{1/2}) B_2^n \subseteq K$. Also, \Cref{lemma 4.38} implies that $K\subseteq 3nmB_2^n$. Therefore, for any $x\in K$,
\begin{equation}
	\label{eqn: 4.21}
	\frac{\norm{x}}{3nm} \leq \phi(x) \leq 3\sqrt{n}\norm{x}.
\end{equation}
The first lemma justifies why we can restrict ourselves to $qB_2^n$.

\begin{lemma}
	\label{lem 4.45}
	Given the above,
	\[ \left(1-\frac{\varepsilon}{4}\right)\vol(K) < \frac{1}{n!} \int_{qB_2^n} F(x)\d{x}. \]
\end{lemma}
\begin{proof}
	Let $\overline{K}=K\cap m B_2^n$. Note that for any $x$, $F_{\overline{K}}(x) \leq F(x)$. We then have
	\begin{align*}
		\int_{2nmB_2^n} F(x)\d{x} &\geq \int_{2n\overline{K}} F(x)\d{x} & (2n\overline{K}\subseteq 2nmB_2^n) \\
			&\geq \int_{2n\overline{K}} \overline{F}(x)\d{x} \\
			&= n\vol(\overline{K}) \int_0^{2n} e^{-t}t^{n-1}\d{t} & \left(\text{using \Cref{eqn: 4.20} on $h(t) = \begin{cases} e^{-t}, & 0\leq t\leq 2n, \\ 0, & \text{otherwise.} \end{cases}$}\right) \\
			&> \frac{39}{40} n! \vol(\overline{K}) \\
			&\geq \frac{13}{20} n!\vol(K) & (\text{at least $2/3$ of the volume of $K$ is in $mB_2^n$})
	\end{align*}
	for sufficiently large $n$.

	Alternatively,
	\[ \int_{\Rn \setminus 2nmB_2^n} F(x)\d{x} < \frac{7}{20} n! \vol(K) = \frac{7}{20} \int_{\Rn} F(x)\d{x}. \]
	We can then use \Cref{inequality integral setminus ball} to get
	\[ \int_{\Rn\setminus 2nm\log(4/\varepsilon)B_2^n} F(x)\d{x} < \left(\frac{7}{20}\right)^{\log(4/\varepsilon)} \int_{\Rn} F(x)\d{x} \leq \frac{\varepsilon}{4} n! \vol(K) \]
	for sufficiently small $\varepsilon$, thus proving the required.
\end{proof}

The second lemma bounds the conductance of the chain.

\begin{lemma}
	\label{lem 4.46}
	If $p\geq 10\sqrt{n}$, then the conductance of $\mathcal{M}(pB_2^n, B_2^n)/F_K$ is at least $1/(20000p\sqrt{n})$.
\end{lemma}

\begin{proof}
	The first step is bounding the local conductance, much like we did for the earlier algorithm. Let $x\in pB_2^n$ and $y$ be chosen randomly from $x+B_2^n$. By the condition that $p\geq 10\sqrt{n}$, $y\not\in pB_2^n$ with probability at most $5/9$. Since $2/3$ of the volume of $B_2^n$ is in $K$, $y\not\in K$ with probability at most $1/3$. Therefore, $y\in pB_2^n\cap K$ with probability at least $1/9$. Fix such a $y$. Then
	\[ y\in x+K\subseteq \phi(x)K + K = (\phi(x)+1)K, \]
	so $\phi(y)\leq\phi(x)+1$. Therefore, $F_K(y)\geq F_K(x)/e$. This bounds the local conductance below by $1/9e > 1/25$. In the context of \Cref{cor to bound conductance}, the minimum value of $\theta$ is bounded below by $2/3pn^{1/2}$ (using \Cref{better bound on spherical caps}).\footnote{try bounding the distance $d$ such that $\vol(B_2^n \cap (de_1 + B_2^n)) \geq \vol(B_2^n)/2$ and going from there.} \Cref{cor to bound conductance} then yields the result.
\end{proof}

Let us now move on to the proof of the main theorem.

For $A\subseteq\Rn$, define
\[ Q_F(A) = \frac{\int_A F(u)\d{u}}{\int_{\Rn} F(u)\d{u}} \]
and
\[ Q_F'(A) = \frac{\int_{A\cap qB_2^n} F(u)\d{u}}{\int_{qB_2^n} F(u)\d{u}. } \]

For measurable $A\subseteq K$,
\[ \Pr[w\in A] = \Pr[v_t \in H^{-1}(A)] = Q_t(H^{-1}(A)). \]
By \Cref{final thing uniformly distrib},
\[ Q_F(H^{-1}(A)) = \frac{\vol(A)}{\vol(K)}. \]
Clearly, it suffices to show that for any measurable $U\subseteq\Rn$,
\[ |Q_F(U) - Q_t(U)| < \varepsilon. \]
Using \Cref{lem 4.45},
\begin{align*}
	Q_F(U) - Q_F'(U) &= \frac{\mu_F(U)}{\mu_F(\Rn)} - \frac{\mu_F(U\cap q B_2^n)}{\mu_F(q B_2^n)}  \\
		&\leq \frac{\mu_F(U)}{\mu_F(\Rn)} - \frac{\mu_F(U\cap q B_2^n)}{\mu_F(\Rn)} \\
		&= \frac{\mu_F(U\setminus qB_2^n)}{\mu_F(\Rn)} \\
		&\leq \frac{\mu_F(\Rn\setminus qB_2^n)}{\mu_F(\Rn)} \leq \frac{\varepsilon}{4} \\
	Q_F'(U) - Q_F(U) &= \frac{\mu_F(U\cap q B_2^n)}{\mu_F(q B_2^n)} - \frac{\mu_F(U)}{\mu_F(\Rn)} \\
		&\leq \frac{\mu_F(U\cap q B_2^n)}{\mu_F(qB_2^n)} - \frac{\mu_F(U\cap q B_2^n)}{\mu_F(\Rn)} \\
		&= \frac{\mu_F(U\cap q B_2^n)\mu_F(\Rn\setminus qB_2^n)}{\mu_F(qB_2^n)\mu_F(\Rn)} \\
		&\leq \frac{\mu_F(\Rn\setminus qB_2^n)}{\mu_F(\Rn)} \leq \frac{\varepsilon}{4}
\end{align*}
Therefore, it suffices to bound $|Q_F'(U)-Q_t(U)|$ by $3\varepsilon/4$. To do so, we use \Cref{cor 4.23}.
% \[ |Q_t(U) - Q_F'(U)| \leq \sqrt{M}\left(1-\frac{\Phi^2}{2}\right)^t. \]
To estimate $M$ (in the context of \Cref{cor 4.23}),
\begin{align*}
	M 	&= \sup_{A} \frac{Q_0(A)}{Q_F'(A)} \\
			% &= \sup_{A} \frac{\mu_F(q B_2^n)}{\vol(B_2^n)}\cdot\frac{\vol(A\cap B_2^n)}{\mu_F(A\cap q B_2^n)} \\
			% &= \frac{\mu_F(q B_2^n)}{\vol(B_2^n)} \sup_{A\subseteq B_2^n}\frac{\vol(A)}{\mu_F(A)} \\
		&= \frac{\mu_F(q B_2^n)}{\mu_F(B_2^n)} & (\text{the supremum is attained for }A=B_2^n) \\
		&\leq \frac{n!\vol(K)}{e^{-3\sqrt{n}}\vol(B_2^n)} & (\text{\Cref{eqn: 4.21}}) \\
		&\leq n^{3n} & \left(n!\leq n^n, \frac{\vol(K)}{\vol(B_2^n)} \leq \frac{3}{2}m^n, e^{-3\sqrt{n}}\leq\frac{2}{3}n^{n/2}\right)
\end{align*}
% *** WHY IS SUPREMUM ATTAINED FOR B_2^n?
for sufficiently large $n$. By \Cref{lem 4.46},
\[ \Phi \geq \frac{1}{20000q\sqrt{n}}. \]
Therefore,
\begin{align*}
	|Q_t(U) - Q_F'(U)| &\leq n^{3n/2}\left(1-\frac{\Phi^2}{2}\right)^t \\
		&= n^{3n/2} \left(1 - \frac{25}{2t} \left(n\log(n) + \log(2/\varepsilon)\right)\right)^t \\
		&\leq n^{3n/2} \exp\left(-\frac{25}{2}\left(n\log(n) + \log(2/\varepsilon)\right)\right) \leq \frac{\varepsilon}{2}.
\end{align*}

This proves \Cref{LovS algorithm works!}.\\
It is actually possible to perform the above algorithm with a far smaller value of $t$ ($\mathcal{O}^*(n^4m^2)$) while taking $\frac{\varepsilon}{1000n}B_2^n$-steps, but the analysis is more complicated so we omit it (we would have to use $s$-conductance in the proof instead). 

\subsubsection{Better Sandwiching and Ignoring the Error Probability}
\label{pro sandwiching}

One of the basic assumptions in our algorithm was that of the sandwiching. Our body $K$ is such that
\begin{itemize}
	\item At least $2/3$ of the volume of $B_2^n$ is in $K$.
	\item At least $2/3$ of the volume of $K$ is in $mB_2^n$ for some $1\leq m\leq n^{3/2}$.
\end{itemize}

We have already seen in \Cref{sandwiching} that it is possible to deterministically obtain get $B_2^n\subseteq K\subseteq n^{3/2}B_2^n$.
% First, we sketch an alternate algorithm to obtain $m=n^{3/2}$. At each step of \Cref{lovasz pre-sandwich}, if we have $\mathcal{E}_r\supseteq K$, we check if $(1/2n)\mathcal{E}_r$ is contained in $K$.
Sticking with $n^{3/2}$ would cost us a running time of $n$ in the main part, so how do we do better?

Suppose we have $n^{-1/2}B_2^n\subseteq K\subseteq nB_2^n$. First, we randomly select $T=3\log n$ points in $B_2^n$ independently and then check if they are in $K$. If one of them is not, we have a point in $B_2^n\setminus K$ to perform the ellipsoid step. Otherwise, we shall conclude that at least $2/3$ of the volume of $B_2^n$ is in $K$. Indeed, if not, the probability that we do not find a point in $B\setminus K$ is less than $(2/3)^{3\log n} \leq 1/(100n^2\log n)$. The procedure itself lasts at most $25n^2\log n$ steps, so the probability that it halts with $K$ containing less than $2/3$ of $B_2^n$ (which it has) is less than $1/4$.

This gives a randomized algorithm that achieves the required with error probability less than $1/4$.\\
It is worth noting that here, we have the second condition for $m=n$ in a stronger sense with at least $2/3$ of the volume of $B_2^n$ in $K$ and $K\subseteq m B_2^n$.\\

We now discuss another tactic to obtain the first condition in a strong sense and the second condition in a weaker sense using the polar.

Suppose we have $B_2^n\subseteq K\subseteq n^{3/2}B_2^n$. If $K$ contains some point $x$ with $\norm{x}>6n$, then we first expand the body by a factor of $(1+1/n)$ in all directions orthogonal to $x$, shrink it by a factor of $3$ along $x$, and then translate it by $(-2/3)(x/\norm{x})$. This affine transformation of $K$ still contains $B_2^n$ (Why?). We can repeat this until we get $B_2^n\subseteq K\subseteq 6nB_2^n$ (or rather, some affine transformation of $K$).\\

However, there is still one problem -- how do we find a valid $x$ or determine that no such $x$ exists? That is, we wish to determine if $K\subseteq 6nB_2^n$ and if not, find a point in $6B_2^n\setminus K$. We can do this by generating $\log_{3/2} (10n\log n)$ independent random points in $K$(!) and test if they belong to $6nB_2^n$. If one of them does, we have a point to carry out the ellipsoid step. If all of them belong to $6nB_2^n$, then we shall conclude that at least $2/3$ of the volume is in $6nB_2^n$. Indeed, if not, the probability of not finding a point in $6nB_2^n\setminus K$ is less than $(2/3)^{\log_{3/2} (10n\log n)} \leq 1/(10n\log n)$.

Generating points in $K$ is quite expensive however, especially because the whole reason we are sandwiching is to make generating points easy.\\
If we use the sampling algorithm discussed in \Cref{LovS algorithm works!}, we require $\mathcal{O}(n^4 m^2 \log n)$ membership test and $\mathcal{O}(n^5 m^2 \log n)$ arithmetic operations.\\
With a neat little trick however, we may in fact take $m=\mathcal{O}(n)$ here. Let $K'=K\cap 18n B_2^n$. If $\vol(K\setminus 6nB_2^n)=\theta\vol(K)$ ($\theta>1/3$), then by \Cref{inequality integral setminus ball restricted to K}, $\vol(K\setminus 18nB_2^n)\leq\theta^2\vol(K)$. Therefore,
\begin{align*}
	\vol(K'\setminus 6nB_2^n) &= \vol(K\cap 18nB_2^n) - \vol(K\cap 6nB_2^n) \\
		&\geq (1-\theta^2)\vol(K) - (1-\theta)\vol(K) \\
		&= (\theta-\theta^2)\vol(K) \\
		&= \theta\vol(K\cap 6nB_2^n) = \theta\vol(K'\cap 6nB_2^n) \\
	(1+\theta)\vol(K'\setminus 6nB_2^n) &\geq \theta\vol(K') \\
	\vol(K'\setminus 6nB_2^n) &\geq \frac{\theta}{1+\theta} \vol(K') \geq \frac{1}{4} \vol(K').
\end{align*}

We need to generate $\mathcal{O}(n\log^2 n)$ random points overall, which makes the cost of this phase $\mathcal{O}(n^7\log^3 n)$, which is at most a $\log$ factor more than that of the main part.\\

The reader might be wondering why we even introduced this alternate way to obtain a sandwiching; it does not seem to be any better than what we did earlier with the strong satisfaction of the second condition. In special cases however, this method turns out to be extremely powerful.\\
If $K$ is a polytope with polynomially many (in $n$) facets, the two conditions can be attained with $m=\mathcal{O}(\sqrt{n}\log n)$. If $K$ is a symmetric polytope with polynomially many facets, then we can in fact achieve $m=\mathcal{O}(\log n)$!\\

We mentioned in \Cref{the walk} that while it seems like we sacrifice precision and get a bad error probability $\delta$, this is not really the case. This is due to a neat statistical trick from \cite{JerrumValiantVazirani1986} which claims that it suffices to solve the problem for $\delta=1/3$. Fix some $\varepsilon>0$ and suppose we have some algorithm that calculates a $\zeta$ such that $\zeta\in[(1-\varepsilon)\vol(K),(1+\varepsilon)\vol(K)]$ with probability at least $2/3$. Let $s=10\log(1/\delta)$ and repeat the algorithm $2s+1$ times to obtain $\zeta_1,\zeta_2,\ldots,\zeta_{2s+1}$. Arranging the $(\zeta_i)$ in increasing order, let $\zeta=\zeta_{s+1}$ (the median).

We claim that $\Pr[\zeta\in[(1-\varepsilon)\vol(K),(1+\varepsilon)\vol(K)]] \geq 1-\delta$.\\
If $\zeta$ is not in the interval, at least $(s+1)$ of the $(2s+1)$ $(\zeta_i)$ are outside the interval. We can then use Chernoff's inequality, which yields that out of $2s+1$ independent events, each having a probability at most $1/3$ of occurring, the probability that more than half occur is at most $e^{-s/10}\leq\delta$.

\subsubsection{Bringing Everything Together and a Final Analysis}

We have now built the various parts of the algorithm, and all that remains is to put the pieces together.

Let $K$ be a convex body with $(2/3)\vol(B_2^n) \leq \vol(K\cap B_2^n)$ and $(2/3)\vol(K) \leq \vol(K\cap mB_2^n)$. We have described how to attain this (in a randomized manner) in \Cref{pro sandwiching}.\\

Although we have described the primary algorithm in detail before, we restate it here because it is slightly different.\\
Set $q=4mn\log(1/\varepsilon)$, $k=4n\log n$, and $t=10^{11}nkq^2\varepsilon^{-2}$.\\
For each $i$, define
\[ F_i(x) = \min\{F(x), \exp(-\norm{x}n^{1/2}2^{-i/n})\} \]
where $F$ is defined as in the beginning of \Cref{the walk}.\\
% *** Why is there an n^{1/2}? Aren't we just taking the minimum where K_i is K \cap 2^{i/n} B_2^n?
% To compute $\vol(K)$, we (attempt to) integrate $F$ --  as we have seen,
% \[ \vol(K) = \frac{1}{n!} \int_{\Rn} F(x)\d{x}. \]
Now, for each $i$, perform a lazy walk taking $3t$ steps in $qB_2^n$ with $B_2^n$-steps, filtered by $F_i$. Let these steps be $v_{i,1},\ldots,v_{i,3t}$. The stationary distribution of this walk $Q_{F_i}$ is such that $Q_{F_i}(A) = \mu_F(A)/\mu_F(qB_2^n)$. Here, the first $t$ steps are to get close to the stationary distribution, the next $t$ are what we use for the algorithm, and the last $t$ are to ensure independence from the next phase. \\
The $i$th walk begins where the $(i-1)$th ends. For each $i$, compute
\[ \Lambda_i = \frac{1}{t} \sum_{j=t+1}^{2t} \frac{F_{i-1}(v_{i,j})}{F_i(v_{i,j})}. \]
Note that
\[ \Lambda_i \approx \int_{qB_2^n} \frac{F_{i-1}(x)}{F_i(x)} \d{Q}_{F_i}(x) = \frac{\int_{qB_2^n} F_{i-1}(x) \d{x}}{\int_{qB_2^n} F_i(x) \d{x}} \approx \frac{\vol(K_{i-1})}{\vol(K_i)}, \]
where $K_i = 2^{i/n} B_2^n \cap K$. Finally, we return our estimate of $\vol(K)$ as
\[ \evol(K) = (\Lambda_1\Lambda_2\cdots\Lambda_k)^{-1} \vol(B_2^n). \]

Let us now analyze this beast.

\begin{ftheo}
	If $K$ is a convex body such that $\vol(K\cap B_2^n) \geq (2/3) \vol(B_2^n)$ and $\vol(K\cap mB_2^n) \geq (2/3) \vol(K)$ and $\zeta$ is the estimate returned by the above algorithm, then
	\[ \Pr[(1-\varepsilon)\zeta \leq \vol(K) \leq (1+\varepsilon)\zeta] \geq 3/4. \]
	Further, the algorithm uses
	\[ \mathcal{O}\left(n^3m^2\varepsilon^{-2} (n+\log(1/\varepsilon))^2 (\log m + \log\log(1/\varepsilon))^2 \log^2(1/\varepsilon) \right) \]
	membership oracle calls.
\end{ftheo}

\begin{corollary}
	Let $K$ be a convex body given by a well-guaranteed membership oracle. Then it is possible to compute a $\zeta$ such that
	\[ \Pr[(1-\varepsilon)\zeta \leq \vol(K) \leq (1+\varepsilon)\zeta] \geq 1-\delta \]
	in
	\[ \mathcal{O}\left(n^7 \varepsilon^{-2} \log^2 (n) \log^2(1/\varepsilon) \log(1/\delta) \right) \]
	oracle calls. Further, if $K$ is a polytope with polynomially many (in $n$) facets, we only require $\mathcal{O}\left(n^6 \varepsilon^{-2} \log^4 (n) \log^2(1/\varepsilon) \log(1/\delta) \right)$	oracle calls and if $K$ is a centrally symmetric polytope with polynomially many facets, we only require $\mathcal{O}\left(n^5 \varepsilon^{-2} \log^4 (n) \log^2(1/\varepsilon) \log(1/\delta) \right)$ oracle calls.
\end{corollary}

The rest of this section is dedicated to proving the above, which we have already done most of the work for over the past 3 subsections. For each $i$, let $W_i = \int_{qB_2^n} F_i$. By \Cref{lem 4.46}, 
\[ \left(1-\frac{\varepsilon}{4}\right) \vol(K) \leq \frac{1}{n!}W_i \leq \vol(K). \]

First, we claim that for any $i$,
\[ \frac{1}{2} \leq \frac{W_{i-1}}{W_i} \leq 1. \]
Indeed, the latter inequality is trivial and for the former,
\begin{align*}
	W_i &= \int_{qB_2^n} F_i(x)\d{x} \\
		&\leq \int_{2^{i/n}qB_2^n} F_i(x)\d{x} \\
		&= \frac{1}{2} \int_{qB_2^n} F_i(2^{i/n}x)\d{x} \\
		&\leq \frac{1}{2} \int_{qB_2^n} F_{i-1}(x)\d{x} = \frac{1}{2} W_{i-1}.
\end{align*}

We also see that $W_0$ is easily computed to be $n^{-n/2}n!\vol(B_2^n)$.\\

What are the possible (probabilistic) sources of error in our algorithm?
\begin{enumerate}
	\item The distribution of the point $v_{i,t+1}$ generated is not exactly $(1/W_i)Q_{F_i}$.
	\item The points $v_{i,t+1}$ are not independent.
	\item The sum used to estimate $W_{i-1}/W_i$ has variance.
\end{enumerate}

Let us look at each of these separately, starting with the first.

Let $P_{i,p}$ represent the distribution of $v_{i,p}$. As defined in \Cref{subsec: measure theoretic markov chains}, let $h_{i,p}$ be the distance function between $P_{i,p}$ and $Q_{F_i}$. Because $Q_{F_i}$ is atom-free,
\[ h_{i,p}(x) = \sup_{\substack{A\subseteq qB_2^n \\ Q_{F_i}(A) = x}} P_{i,p}(A) - x. \]
Also let $h_{0,p}(x)=0$. Define $\Phi=\min_i\Phi_i$. By \Cref{lem 4.46}, $\Phi \geq 1/(20000q\sqrt{n})$. Also define
\[ \eta = \left(1-\frac{\Phi^2}{2}\right)^t \leq e^{-t\Phi^2/2} \] % \leq m^{-128n/\varepsilon^2} \]

\begin{lemma}
	\label{analysis lemma 1}
	For every $0\leq x\leq 1$,% and $t\leq p\leq 3t$,
	\[
		h_{i,p}(x) \leq
		\begin{cases}
			2\eta\min\{\sqrt{x},\sqrt{1-x}\}, & \text{if } t\leq p\leq 3t, \\
			4\min\{\sqrt{x},\sqrt{1-x}\} & \text{otherwise.}
			% can't we do 2\min\{\sqrt{x},\sqrt{1-x}\} ???
		\end{cases}
	\]
\end{lemma}
\begin{proof}
	We prove this with induction on $i$. The claim is trivially true for $i=0$. Let $i>0$. Then for some $A\subseteq qB_2^n$ with $Q_{F_i}(A)=x$,
	\[ h_{i,0}(x) = P_{i,0}(A) - x. \]
	Let $y=Q_{F_{i-1}}(A)$. Then
	\[ y = \frac{1}{W_{i-1}} \int_{A} F_{i-1}(u)\d{u} \leq \frac{2}{W_i}\int_{A} F_i(u)\d{u} = 2x. \]
	Therefore, using the inductive hypothesis,
	\begin{align*}
		h_{i,0}(x) &= P_{i,0}(A) - x \\
			&= P_{i-1,3t}(A) - x \\
			&= (P_{i-1,3t}(A) - y) + (y-x) \\
			&\leq h_{i-1,3t}(y) + \min\{x,1-x\} & (y-x\leq x) \\
			&\leq 2\eta\min\{\sqrt{y},\sqrt{1-y}\} + \min\{\sqrt{x},\sqrt{1-x}\} \\
			&\leq (1+2\sqrt{2}\eta) \min\{\sqrt{x},\sqrt{1-x}\} < (1+4\eta)\min\{\sqrt{x},\sqrt{1-x}\}.
	\end{align*}
	We can then use \Cref{decrease in markov distance} to conclude that for $p\geq t$,
	\begin{align*}
		h_{i,p}(x) &< (1+4\eta) \min\{\sqrt{x},\sqrt{1-x}\}\left(1-\frac{\Phi^2}{2}\right)^p \\
			&\leq (\eta+4\eta^2) \min\{\sqrt{x},\sqrt{1-x}\} \leq 2\eta \min\{\sqrt{x},\sqrt{1-x}\}.
	\end{align*}
	The case for $p<t$ follows similarly (with the last two inequalities above slightly modified).
\end{proof}

The above lemma resolves the first of the issues by bounding how far the distribution of $v_{i,t+1}$ can get from the required.\\

Next, let us look at the second issue. Let $u=v_{i,a}$ and $w=v_{j,b}$ for some $i<j$ and $t<a,b\leq 2t$. Let $f(u)=F_{i-1}(u)/F_i(u)$ and $g(w)=F_{j-1}(w)/F_j(w)$. We wish to bound the correlation between the two random variables.

\begin{lemma}
	\label{analysis lemma 2}
	Let $f$, $g$, $u$, and $w$ be defined as above. Then
	\[ \left|\expec[f(u)g(w)] - \expec[f(u)]\expec[g(w)] \right| \leq 4\eta. \]
\end{lemma}

\begin{proof}
	Let us consider another random walk $\overline{v}_{i,a},\ldots,\overline{v}_{j,b}$ defined as follows. The transition probability from a $\overline{v}$ to another is exactly the same as that for the $v$, but the distribution of the starting point $\overline{v}_{i,a}$ is slightly different. Set
	\[ c_f = \frac{1}{\int_{qB_2^n} f\d{P}_{i,a}} = \frac{W_i}{W_{i-1}}. \]
	Further, let
	\[ \overline{P}_{i,a}(A) = \Pr[\overline{v}_{i,a}] = \frac{\int_A f\d{P}_{i,a}}{\int_{qB_2^n} f\d{P}_{i,a}} = c_f\int_A f\d{P}_{i,a}. \]
	Similarly, for each $i\leq k\leq j$ and $0\leq r\leq 2t$, let $\overline{P}_{k,r}$ be the distribution of $\overline{v}_{k,r}$. Then
	\begin{align*}
		\expec[f(u)g(w)] &= \int_{qB_2^n} \expec[f(u)g(w)\mid u=x] \d{P}_{i,a}(x) \\
			&= \int_{qB_2^n} \expec[g(w)\mid u=x] f(x)\d{P}_{i,a}(x) \\
			&= \int_{qB_2^n} \expec[g(w)\mid u=x] \d{\overline{P}}_{i,a}(x) \int_{qB_2^n} f(x)\d{P}_{i,a}(x) \\
			&= \expec[g(\overline{v}_{j,b})]\expec[f(u)].
	\end{align*}
	Therefore,
	\begin{align*}
		\left| \expec[f(u)g(w)] - \expec[f(u)]\expec[g(w)] \right| &= \expec[f(u)] \left| \expec[g(\overline{v}_{j,b}) - g(v_{j,b})] \right| \\
			&= \expec[f(u)] \left| \int_{qB_2^n} g(y) \left(\d{\overline{P}}_{j,b} - \d{P}_{j,b}\right) \right| \\
			&\leq \sup_{A\in\mathcal{A}} |\overline{P}_{j,b}(A) - P_{j,b}(A)| & \left(\frac{1}{2}\leq \expec[f(u)] \leq 1 \text{ and \Cref{eqn: hk and one norm}} \right) \\
			&\leq \sup_{A\in\mathcal{A}} |\overline{P}_{j,b}(A) - Q_{F_i}(A)| + |Q_{F_i}(A) - P_{j,b}(A)|
	\end{align*}
	By \Cref{analysis lemma 1}, the second quantity is at most $2\eta$. % *** HOW???
	For each $i\leq k\leq j$ and $0\leq r\leq 2t$, let $\overline{h}_{k,r}$ be the distance between $\overline{P}_{k,r}$ and $Q_{F_k}$. To bound the first quantity, let $A\subseteq qB_2^n$ with $Q_{F_i}(A)=z$. Let $y=\int_A f\d{Q}_{F_i}\leq z$. Then
	\begin{align*}
		\overline{P}_{i,a}(A) - Q_{F_i}(A) &= \int_A c_f f \d{P}_{i,a} - z \\
			&= \int_A c_f f \left(\d{P}_{i,a} - \d{Q}_{F_i}\right) + \int_A c_f f \d{Q}_{F_i} - z \\
			&\leq c_f h_{i,a}(y) + c_f y - z \\
			&\leq 2 h_{i,a}(y) + 2 y - z. 
	\end{align*}
	By \Cref{analysis lemma 1} and because $1\leq c_f\leq 2$,
	\begin{align*}
		\overline{h}_{i,a}(z) &\leq 4\eta\min\{\sqrt{y},\sqrt{1-y}\} + 2y - z \\
			&\leq 4\eta\sqrt{z} + z \leq 5\sqrt{z}.
	\end{align*}
	We also trivially have
	\[ \overline{h}_{i,a}(z) \leq 1-z \leq 5\sqrt{1-z}. \]
	Applying the proof of \Cref{analysis lemma 1} to the alternate chain,
	\[ \overline{h}_{j,b}(z) \leq 2\eta\min\{\sqrt{z},\sqrt{1-z}\} \leq 2\eta, \]
	completing the proof.
	% *** Result says 6\eta (but 4\eta is used later). doesn't the ``proof of \Cref{analysis lemma 1}'' work for any similar chain? Paper says only constant 4 (in the sqrt x sqrt 1-x) (then proceeds to use it for 5 as written above), 
\end{proof}

The third issue has already been taken care of in \Cref{bounding variance}.\\


For each $i$, let $1+\beta_i = \Lambda_i / \expec[\Lambda_i]$. We shall then show that
\begin{equation}
	\Pr\left[1-\frac{\varepsilon}{2} \leq \prod_{i=1}^k (1+\beta_i) \leq 1+\frac{\varepsilon}{2}\right] > \frac{3}{4}.
\end{equation}
Obviously, $\expec[\beta_i]=0$ for any $i$. We split the proof into two cases.\\
\begin{enumerate}
	\item $\sum_{i=1}^k \beta_i^2 > \varepsilon/8$. By Markov's inequality,
	\[ \Pr\left[\sum_{i=1}^k \beta_i^2 > \frac{\varepsilon}{8}\right] \leq \frac{\expec\left[\sum_{i=1}^k \beta_i^2\right]}{\varepsilon/8}. \]
	Let $F=F_{i-1}/F_i$. Applying \Cref{bounding variance},
	\[ \Var[t\Lambda_i] \leq \frac{4t}{\Phi^2} \leq \frac{4t}{\Phi^2}. \]
	Therefore,
	\[ \expec[\beta_i^2] = \frac{\Var[\Lambda_i]}{\expec[\Lambda_i]^2} \leq \frac{4t/\Phi^2}{1/4} \leq \frac{\varepsilon^2}{8k} \]
	and
	\[ \Pr\left[\sum_{i=1}^k \beta_i^2 > \frac{\varepsilon}{8}\right] \leq \frac{1}{8}. \]

	\item $\sum_{i=1}^k \beta_i^2 \leq \varepsilon/8$. We may assume that each $|\beta_i|\leq\frac{1}{2}$.\\
	Suppose that $\left| \prod_i\left(1+\beta_i\right) - 1 \right| > \varepsilon/2$. Then,
	\[ \log\left(1-\frac{\varepsilon}{2}\right) > \sum_i \log(1+\beta_i) \text{ or } \sum_i \log(1+\beta_i) > \log\left(1+\frac{\varepsilon}{2}\right) \]
	so
	\[ \left| \sum_{i=1}^k \log(1+\beta_i) \right| > \frac{\varepsilon}{4}. \]
	Then note that $|\log(1+\beta_i) - \beta_i| < \beta_i^2$. Therefore,
	\begin{align*}
		\sum_i \beta_i^2 &> \sum_i \left| \log(1+\beta_i) - \beta_i \right| \\
			&\geq \sum_i \left| \log(1+\beta_i) \right| - \sum_i \left| \beta_i \right| \\
			&\geq \left| \sum_i \log(1+\beta_i) \right| - \left| \sum_i \beta_i \right| \\
		\left| \sum_i \beta_i \right| &> \left| \sum_i \log(1+\beta_i) \right| - \left| \sum_i \beta_i^2 \right| \geq \frac{\varepsilon}{8}.
	\end{align*}
	The probability of this occurring can be computed using Chebyshev's inequality. We have
	\begin{align*}
		\Var\left(\sum_i \beta_i\right) &= \expec\left[\left(\sum_i\beta_i\right)^2\right] \\
			 &= \sum_i \expec[\beta_i^2] + 2 \sum_{1\leq i<j\leq n} \expec[\beta_i\beta_j] \\
			 &\leq \frac{\varepsilon^2}{8} + 2 \sum_{1\leq i<j\leq n} \expec[\beta_i\beta_j]. \\
		\expec[\beta_i\beta_j] &= \frac{\expec[\Lambda_i\Lambda_j]}{\expec[\Lambda_i]\expec[\Lambda_j]} - 1.
	\end{align*}
	Using \Cref{analysis lemma 2},
	\begin{align*}
		\expec[\Lambda_i\Lambda_j] &= \frac{1}{t^2} \sum_{a=t+1}^{2t} \sum_{b=t+1}^{2t} \expec\left[f(v_{i,a})g(v_{j,b})\right] \\
			&\leq \frac{1}{t^2} \sum_{a=t+1}^{2t} \sum_{b=t+1}^{2t} \expec[f(v_{i,a})]\expec[g(v_{j,b})] + 4\eta \\
			&= \expec[\Lambda_i]\expec[\Lambda_j] + 4\eta.
	\end{align*}
	Because $\expec[\Lambda_i] \geq \frac{1}{2}$, $\expec[\beta_i\beta_j] \leq 16\eta < \varepsilon^2/8k^2$. Therefore,
	\[ \Var\left(\sum_i \beta_i\right) \leq \frac{\varepsilon^2}{4}. \]
	Using Chebyshev's inequality,
	\[ \Pr\left[\left|\sum_i \beta_i\right| > \frac{\varepsilon}{8}\right] \leq \frac{64\Var\left[\sum_i\beta_i\right]}{\varepsilon^2} \leq  \]
\end{enumerate}
% *** yeah, I have no idea

This proves the required.

How large is the running time? We make $kt$ ``moves'', where each move constitutes
\begin{itemize}
	\item An update of $n$ coordinates.
	\item One test of membership in $qB_2^n$.
	\item The evaluation of $\phi(x)$.
	\item Some constant number of arithmetic operations.
\end{itemize}

We can evaluate $\phi(x)$ using binary search with error $\varepsilon/n^2$ in $\mathcal{O}(\log n + \log(1/\varepsilon))$ membership tests. Thus, the algorithm makes $\mathcal{O}(kt(\log n + \log(1/\varepsilon)))$ membership tests.\\
It is worth noting that we have assumed that we can compute membership in an affine image of $K$ in a single step. This is indeed true for sensible encodings such as linear or algebraic inequalities, but it may become important if it takes $n^2$ or more operations.