\section{The It\^{o} Integral and Stochastic Differential Equations}

\subsection{The It\^{o} Integral: Motivations and Definitions}

\subsubsection*{Motivations}

For a moment, let us look at a more physical notion -- that of white noise. Given a discrete time signal $(a_n)$, we model the received signal by $x_n = a_n + \xi_n$, where the $\xi_n$ are iid Gaussian random variables with zero mean. This is discrete time white noise, often known as Additive White Gaussian Noise (AWGN).\\
How would this extend to continuous time? If we associate a standard normal $\xi_t$ for each $t\in\R^{\geq 0}$ such that $\expec[\xi_s\xi_t]=0$ for $s\neq t$, then if we let $\Xi_\varepsilon = \frac{1}{\varepsilon} \int_0^\varepsilon \xi_s\d{s}$, then it is seen that $\expec[\Xi_\varepsilon] = \Var[\Xi_\varepsilon] = 0$. This doesn't make any sense, because it would mean that observing the received signal for an arbitrarily small time from $0$ would allow us to completely determine what the transmitted signal at time $0$ was. This isn't ``noise'' then, so how do we fix it?\\
In AWGN, the corrupting noise in any unit time interval is a zero mean Gaussian (with say, unit variance), so maybe we would want that the average white noise in unit time $\Xi_1$ is standard normal. We also want that $\xi_t$ and $\xi_s$ are independent for $t\neq s$. This just means that $\int \xi_s \d{s} = W_t$! However, we have seen that $W_t$ is almost surely non-differentiable, so $\xi_t$ is not really a function. That is, a mathematical model for white noise does not exist, at least within this current theory.\footnote{Certain theories of generalized stochastic processes do lead to a mathematical model for white noise, but we shall not study these.}\\
However, fortunately, it turns out that most of the things we want to do work out if we just stick to working with the Wiener process instead of the white noise itself.\\
Indeed, in the signal analogy, if $(a_t)$ was transmitted, then the received signal would be $x_t = a_t + \xi_t$. This is not meaningful, but integrating on either side, we get
\[ X_t = \int_0^t (a_s + \xi_s)\d{s} = \int_0^t a_s\d{s} + W_t.  \]
Instead of attempting to estimate $a_t$ from $x_t$, we could instead try to solve this problem with $X_t$, fixing our problem.\\

\subsubsection{An Elementary Definition}

Henceforth, in this section, fix a filtered probability space $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in[0,\infty)},\Pbb)$ and a $\mathcal{F}_t$-Wiener process $W_t$. The stochastic integrals are defined with respect to $W_t$.

Let us now get to a more mathematical formulation of the above handwavy argument. Similar to how we define a general measure-theoretic integral, we begin by defining the It\^{o} integral for a suitable class of ``simple'' functions. Finally, we aim to define it for the set of stochastic processes that are $\mathcal{F}_t$ adapted. We take the relevant limits in $\mathcal{L}^2$.\\

Let $X_t^n$ be a $\mathcal{F}_{t_i}$-adaptable random variable in $\mathcal{L}^2$ that is constant for $t_i \leq t < t_{i+1}$, where $t_i$ for $i=0,\ldots,N+1$ is a finite set of \textit{non-random} jump times, with the convention that $t_0 = 0$ and $t_{N+1} = T$. For this \textit{simple} integrand, we define the stochastic integral
\[ I(X^n) = \int_0^T X_t^n \d{W_t} = \sum_{i=0}^N X_{t_i}^n (W_{t_{i+1}} - W_{t_i}). \]
We aim to extend this definition to a more general class of function. To do so, define the following \textit{It\^{o} Isometry}:
\begin{equation}
	\label{eqn: ito isometry}
	\expec\left[\left(\int_0^T X_t^n \d{W}_t\right)^2\right] = \sum_{i=0}^N \expec[(X_{t_i}^n)^2] (t_{i+1} - t_i) = \expec\left[\int_0^T (X_t^n)^2 \d{t} \right]
\end{equation}
Observe that the adaptability is required for the independence of $X_{t_i}^n$ and $W_{t_{i+1}} - W_{t_i}$ and the $\mathcal{L}^2$ condition is required for $\expec[(X_{t_i}^n)^2]$ to be defined.\\
$I(X^n_\cdot)$ is a random variable in $\mathcal{L}^2$. It is easier to analyze the above equation if we just think of $X_n$ as a measurable map from $[0,T]\times\Omega\to\R$ for now. Let $\mu_T$ be the Lebesgue measure on $[0,T]$. Denote by $\norm{\cdot}_{2,\mu}$ the $\mathcal{L}^2$-norm with respect to the measure $\mu$. Then, \eqref{eqn: ito isometry} just says
\begin{equation}
	\label{eqn: ito isometry 2}
	\norm{I(X_n)}_{2,\Pbb} = \norm{X_n}_{2,\mu_T\times\Pbb}.
\end{equation}
This is precisely why the equation is an isometry -- the map $I : \mathcal{L}^2(\Pbb) \to \mathcal{L}^2(\mu_T\times\Pbb)$ preserves $\mathcal{L}^2$-distance (at least when applied to adapted integrands)!\\

\begin{lemma}
	Let $X_\cdot \in \mathcal{L}^2(\mu_T\times\Pbb)$ and suppose there exists a sequence of $\mathcal{F}_t$-adapted simple processes $X_\cdot^n \in \mathcal{L}^2(\mu_T\times\Pbb)$ such that
	\begin{equation}
		\label{eqn: extending ito beyond simple}
		\norm{X_\cdot^n - X_\cdot}_{2,\mu_T\times\Pbb}^2 = \expec\left[\int_0^T (X_t^n - X_t)^2 \d{t} \right] \xrightarrow[]{n\to\infty} 0.
	\end{equation}
	Then $I(X_\cdot)$ can be defined as the limit in $\mathcal{L}^2(\Pbb)$ of the simple integrands $I(X^n_\cdot)$ and further, this definition does not depend on the choice of simple approximations of $X_\cdot^n$.
\end{lemma}
\begin{proof}
	As $m,n\to\infty$,
	\[ \norm{X_\cdot^m - X_\cdot^n}_{2,\mu_T\times\Pbb} \leq \norm{X_\cdot^m - X_\cdot}_{2,\mu_T\times\Pbb} + \norm{X_\cdot - X_\cdot^n}_{2,\mu_T\times\Pbb} \to 0. \]
	By \eqref{eqn: ito isometry 2}, $\norm{I(X_m) - I(X_n)}_{2,\Pbb}\to 0$ as $m,n\to\infty$ as well. That is, $I(X_\cdot^n)$ is a Cauchy sequence in $\mathcal{L}^2(\Pbb)$. Let $I(X_\cdot)$ be the limit of this sequence.\\
	Now, let $Y_\cdot^n$ be another $\mathcal{F}_t$-adapted simple process that satisfies \eqref{eqn: extending ito beyond simple} and let $I(X_\cdot^n)$ converge to $I(Y_\cdot)$ in $\mathcal{L}^2(\Pbb)$. Then,
	\[ \norm{I(Y_\cdot) - I(X_\cdot)}_{2,\Pbb} \leq \norm{I(Y_\cdot) - I(Y_\cdot^n)}_{2,\Pbb} + \norm{I(Y_\cdot^n) - I(X_\cdot^n)}_{2,\Pbb} + \norm{I(X_\cdot^n) - I(X_\cdot)}_{2,\Pbb}. \]
	The first and last terms converge to $0$ by definition and the above argument, and the second argument can be shown to do the same using \eqref{eqn: ito isometry 2}.\\
	Therefore, $I(Y_\cdot) = I(X_\cdot)$ almost surely, proving the claim.
\end{proof}

Now, the question is: what functions $X_\cdot\in\mathcal{L}^2(\mu_T\times\Pbb)$ can be approximated as a sequence of $\mathcal{F}_t$-adapted simple processes (as in \eqref{eqn: extending ito beyond simple})?\\
It turns out that this is the case for any $\mathcal{F}_t$-adapted process.

\begin{lemma}
	Let $X_\cdot\in\mathcal{L}^2(\mu_T\times\Pbb)$ be $\mathcal{F}_t$-adapted. Then, there is a sequence of $\mathcal{F}_t$-adapted simple processes $X_\cdot^n\in\mathcal{L}^2(\mu_T\times\Pbb)$
\end{lemma}

In the case where $X_\cdot$ is bounded and has continuous sample paths, the simple functions defined by $X_t^n = X_{k2^{-n}}$, where $k2^{-n}T \leq t < (k+1)2^{-n}T$ gets the job done -- this is not too difficult to show using the Dominated Convergence Theorem.\\
While it is not very complicated, we omit the rest of the proof of the above.

\begin{definition}[Elementary It\^{o} Integral]
	Let $X_t$ be any $\mathcal{F}_t$-adapted process in $\mathcal{L}^2(\mu_T\times\Pbb)$. Then the It\^{o} integral $I(X_\cdot)$, defined as the limit in $\mathcal{L}^2(\Pbb)$ of simple integrals $I(X_\cdot^n)$ exists and is unique (is independent of the choice of the $X_\cdot^n$).
\end{definition}

For example, show that $\int_0^T W_t\d{W}_t = W_T^2 - T$.

\subsubsection{Towards More Generality}

Now, we aim to extend the It\^{o} integral even further. Indeed, while the set of $\mathcal{F}_t$-adapted processes in $\mathcal{L}^2(\mu_T\times\Pbb)$ may seem very open, it is still quite restrictive. Further, we would like to extend this to processes on $[0,\infty)$.\\
To do so, we first define the It\^{o} integral as a stochastic process on $[0,T]$ with continuous sample paths, then extend it to a process on $[0,\infty)$ using a neat trick called \textit{localization}. We then extend the integral to a wider class of integrands.\\

Let $X^n_t$ be a $\mathcal{F}_t$-adapted simple process in $\mathcal{L}^2(\mu_T\times\Pbb)$ with jump times $(t_i)_{i=0}^{N+1}$. For any $t\leq T$, define the simple integral
\[ I_t(X^n_\cdot) = \int_0^t X_s^n \d{W}_s = \int_0^T \indic_{s\leq t} X_s^n \d{W}_s = \sum_{i=0}^N X_{t_i}^n (W_{\min\{t_{i+1}, t\}} - W_{\min\{t_i,t\}}. \]

First of all, $I_t(X^n_\cdot)$ is a $\mathcal{F}_t$-martingale. This is not too difficult to show, show that for any $i$ and $r<t$,
\[ \expec[X_{t_i}^n(W_{\min\{t_{i+1},t\}} - W_{t_{i},t}) \mid \mathcal{F}_r] = X_{t_i}^n (W_{\min\{t_{i+1},r\}} - W_{t_{i},r}). \]
Indeed, this makes sense since the discrete time $I_{t_i}(X^n_\cdot)$ is a martingale transform, and the Wiener process is a martingale.

\begin{lemma}
	Let $X_t$ be a $\mathcal{F}_t$-adapted process in $\mathcal{L}^2(\mu_T\times\Pbb)$. Then the It\^{o} integral $I_t(X_\cdot)$, $t\in[0,T]$, can be chosen to have continuous sample paths.
\end{lemma}

Now, how do extend the It\^{o} integral to $[0,\infty)$ as a stochastic process? A straightforward idea is to require that the integrand is in $\mathcal{L}^2(\mu\times\Pbb)$ and $\mathcal{F}_t$-adapted, where $\mu$ is the Lebesgue measure on $[0,\infty)$.\\
However, localization, which is not a very deep idea, stipulates that we don't need this. To define it on $[0,\infty)$, it suffices to define it on every $[0,T]$, that is, $X_\cdot \in \bigcap_{T < \infty} \mathcal{L}^2(\mu_T\times\Pbb)$. The integrand does not need to be square integrable, it just needs to be \textit{locally} square integrable, that is, square integrable when restricted to any $[0,T]$.\\
However, we do need to ensure that this definition is consistent. That is, we need to check that $I_t(X_\cdot)$ does not depend on which $T>t$ we choose.\\

This is easily shown however, by taking two times $s<T$ and observing that a sequence $(X_t^n)$ we choose for $T$ would also work for $s$ because $\mathcal{L}^2(\mu_T\times\Pbb)\subseteq\mathcal{L}^2(\mu_s\times\Pbb)$.

\subsubsection{Extending the It\^{o} Integral}

Before we get to an actual extension, let us work with stopping times for a bit.

\begin{lemma}
	\label{ito: stopping time lemma 1}
	Let $X_t$ be a $\mathcal{F}_t$-adapted process in $\bigcap_{T<\infty} \mathcal{L}^2(\mu_T\times\Pbb)$ and $\tau$ a $\mathcal{F}_t$-stopping time. Then $I_{\min\{t,\tau\}}(X_\cdot) = I_t(X_\cdot \indic_{\cdot < \tau})$.
\end{lemma}

This is not too difficult to show using our standard technique of proving the claim for simple processes, then extending it.\\

The condition we are working with right now is that
\[ \expec\left[\int_0^T X_t^2 \d{t} \right] < \infty \]
for all $T<\infty$. Suppose instead that we are in a situation wherein
\[ \expec\left[\int_0^{\tau_n} X_t^2 \d{t} \right] < \infty \]
for some sequence $\tau_n\uparrow\infty$ of $\mathcal{F}_t$-stopping times. Then $(\tau_n)$ is said to be a \textit{localizing sequence} for $X_t$.\\
While $X_t$ itself need not be in $\mathcal{L}^2(\mu_T\times\Pbb)$ for any $T$, we \textit{do} have that $X_t\indic_{t<\tau_n}$ is in $\bigcap_{T<\infty}\mathcal{L}^2(\mu_T\times\Pbb)$.\\
In view of the above lemma then, it makes sense to define $I_t(X_\cdot)$ as $I_t(X_\cdot\indic_{\cdot<\tau_n})$ for some $n$ sufficiently large so that $t\leq\tau_n$.\\
As before, there are two issues we must deal with:
\begin{itemize}
	\item Does it matter which (sufficiently large) $n$ we take?
	\item Does it matter which localizing sequence we choose?
\end{itemize}

The first issue is easily taken care of since for $m>n$, $I_t(X_\cdot \indic_{\cdot < \tau_n}) = I_t(X_\cdot \indic_{\cdot < \tau_m})$. Indeed, since $t<\tau_n\leq\tau_m$, we have that $\indic_{\cdot<\tau_m}\indic_{\cdot<\tau_n} = \indic_{\cdot<\tau_n}$. Then since $I_t(X_\cdot\indic_{t<\tau_n}) = I_{\min\{t,\tau_n\}}(X_\cdot\indic_{t<\tau_m})$, the proof is straightforward.\\

For the second issue, let $(\tau_n)$ and $(\tau_n')$ be two localizing sequences. Then letting $\sigma_n = \min\{\tau_n,\tau_n'\}$ and applying the above method, we get that the integral corresponding to this stopping time is equal to each of the integrals corresponding to $\tau_n$ and $\tau_n'$, so it does not matter which sequence we choose.\\

Now, this still isn't very handy. However, there is a very natural set of processes that do have localizing sequences, namely that which consists of functions that are $\mathcal{F}_t$-adapted and satisfy
\[ \int_0^T X_t^2\d{t} < \infty \text{ almost surely for all } T<\infty. \]
A localizing sequence is then
\[ \tau_n = \inf\left\{t\leq n : \int_0^t X_s^2\d{s} \geq n \right\}. \]
The condition on the integrand implies that $\tau_n\uparrow\infty$ and for all $n\in\N$,
\[ \int_0^{\tau_n} X_t^2\d{t} \leq n \]
almost surely. We may now state the final definition\footnote{Technically, there \textit{is} a more general definition that allows integration with respect to a general martingale and not just the Wiener process, but we omit it here.} of the It\^{o} integral.

\begin{fdef}[It\^{o} Integral]
	Let $X_t$ be a $\mathcal{F}_t$-adapted stochastic process with
	\[ \Pr\left[\int_0^T X_t^2\d{t} < \infty\right] = 1 \text{ for all } T<\infty. \]
	Then the It\^{o} integral
	\[ I_t(X_\cdot) = \int_0^t X_s\d{W}_s \]
	is uniquely defined, by localization and choice of continuous modification, as a $\mathcal{F}_t$-adapted stochastic process on $[0,\infty)$ with continuous sample paths.
\end{fdef}

\subsubsection{Some Properties}

In this section, we give some properties of the It\^{o} integral.

\begin{lemma}[Linearity]
	Let $X_t$ and $Y_t$ be It\^{o} integrable processes, and $\alpha,\beta\in\R$. Then $I_t(\alpha X_\cdot + \beta Y_\cdot) = \alpha I_t(X_\cdot) + \beta I_t(Y_\cdot)$.
\end{lemma}
\begin{proof}
	For the case where $X_t$ and $Y_t$ are simple, it follows by definition. If $(\sigma_n)$ and $(\tau_n)$ are localizing sequences for each of the processes, then $\min\{\sigma_n,\tau_n\}$ is a localizing sequence for both, which allows us to extend to the general case by localizing on this sequence.
\end{proof}

Next, we give a slight generalization (to the wider class of processes) of \Cref{ito: stopping time lemma 1}.

\begin{lemma}
	Let $X_t$ be It\^{o} integrable and $\tau$ a $\mathcal{F}_t$-stopping time. Then
	\[ \int_0^{\min\{t,\tau\}} X_s\d{W}_s = \int_0^t X_s \indic_{s<\tau}\d{W}_s. \]
\end{lemma}
\begin{proof}
	If $(\sigma_n)$ is a localizing sequence for $X_t$, then we see that it is also a localizing sequence for $X_t\indic_{t<\tau}$. For $t<\sigma_n$, $I_{\min\{t,\tau\}}(X_\cdot \indic_{t < \sigma_n}) = I_{\min\{t,\tau\}}(X_\cdot \indic_{t < \tau} \indic_{t < \sigma_n})$. The result follows by localizing on $\sigma_n$.
\end{proof}

Now, let us extend the It\^{o} isometry to a more general integrand.

\begin{lemma}
	Let $X_\cdot \in \bigcap_{T < \infty} \mathcal{L}^2(\mu_T\times\Pbb)$. Then for any $T<\infty$,
	\[ \expec\left[\int_0^T X_t\d{W}_t\right] = 0 \text{ and } \expec\left[\left(\int_0^T X_t\d{W}_t\right)^2\right] = \expec\left[\int_0^T X_t^2\d{t}\right]. \]
	Further, $X_\cdot$ is a $\mathcal{F}_t$-martingale.
\end{lemma}
\begin{proof}
	The results clearly hold for simple integrands. In general, note that if $Y_n \to Y$ in $\mathcal{L}^2$, then $\expec[Y_n] \to \expec[Y]$ and $\expec[Y_n^2] \to \expec[Y_n]$ (because convergence in $\mathcal{L}^2$ implies convergence in law), and $\expec[Y_n \mid \mathcal{F}] \to \expec[Y \mid \mathcal{F}]$ in $\mathcal{L}^2$ (Why?).
\end{proof}

Unfortunately, the above need not hold for a $X_\cdot$ in the more general class of processes. In fact, in the general case, $X_\cdot$ need not even be in $\mathcal{L}^1(\Pbb)$, so the expectation need not be defined.

\begin{corollary}
	If $X_\cdot^n \to X_\cdot$ in $\mathcal{L}^2(\mu_T\times\Pbb)$, then $I_t(X^n_\cdot) \to I_t(X_\cdot)$ in $\mathcal{L}^2(\Pbb)$. If the convergence is sufficiently fast, then $I_t(X_\cdot^n) \to I_t(X_\cdot)$ almost surely.
\end{corollary}

We can instead get a condition more general than $X_\cdot$ just being a $\mathcal{F}_t$-martingale.

\begin{definition}
	A $\mathcal{F}_t$-measurable process $X_t$ is called a $\mathcal{F}_t$-local martingale if there exists a sequence of $\mathcal{F}_t$-stopping times $\tau_n\uparrow\infty$ such that $X_{\min\{t,\tau_n\}}$ is a martingale for each $n$. Such a $(\tau_n)$ is called a \textit{reducing sequence} for $X_t$.
\end{definition}

Any It\^{o} integral $I_t(X_\cdot)$ is a local martingale. Indeed, any localizing sequence is a reducing sequence.

\subsubsection{It\^{o} Calculus}

The definition of the It\^{o} integral is quite hard to work with, so in this section, we give some tools to make it much handier.\\
We work in a more general multi-dimensional framework. Suppose we have a filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_t)_{t\in[0,\infty)},\Pbb)$, on which an $m$-dimensional $\mathcal{F}_t$-Wiener process $W_t = (W_t^1,\ldots,W_t^m)$ is defined.\\
We look at $\mathcal{F}_t$-adapted processes of the form
\[ X_t^i = X_0^i + \int_0^t F_s^i \d{s} + \sum_{j=1}^m \int_0^t G_s^{ij} \d{W}_s^j, \]
where $F_s^i$ and $G_s^{ij}$ are $\mathcal{F}_t$-progressively measurable processes that satisfy
\[ \int_0^t |F_s^i| \d{s} < \infty \text{ and } \int_0^t (G_s^{ij})^2\d{s} < \infty \text{ almost surely} \]
for all $t<\infty$ and $i,j$.

\begin{fdef}[It\^{o} Process]
	A process $X_t = (X_t^1,\ldots,X_t^n)$ satisfying the above is called an \textit{$n$-dimensional It\^{o} process}. It is also denoted as
	\begin{equation}
		\label{eqn: ito process}
		X_t = X_0 + \int_0^t F_s\d{s} + \int_0^t G_s\d{W}_s.
	\end{equation}
\end{fdef}

The main result of this section is the following.

\begin{ftheo}[It\^{o}'s Lemma]
	Let $u:[0,\infty)\times\Rn\to\R$ be a function such that $u(t,x)$ is $C^1$ with respect to $t$ and $C^2$ with respect to $x$. Then $u(t,X_t)$ is an It\^{o} process:
	\begin{multline}
		\label{eqn: ito rule}
		u(t,X_t) = u(0,X_0) + \sum_{\substack{1\leq i\leq n \\ 1\leq k\leq m}} \int_0^t u_i(s,X_s)G_s^{ik}\d{W}_s^k \\ + \int_0^t \left(u'(s,X_s) + \sum_{1\leq i\leq n} u_i(s,X_s)F_s^i + \frac{1}{2} \sum_{\substack{1\leq i,j\leq n \\ 1\leq k\leq m}} u_{ij}(s,X_s)G_s^{ik}G_s^{jk}\right)\d{s},
	\end{multline}
	where $u'(t,x) = \partial u(t,x)/\partial t$ and $u_i(t,x) = \partial u(t,x) / \partial x_i$.
\end{ftheo}

The above might seem quite unwieldy and arbitrary, but it is actually just the analogue of the chain rule for the It\^{o} integral. Before moving on, let us rewrite this a little to make it more compact.\\
An It\^{o} process is often written as
\[ \d{X}_t = F_t \d{t} + G_t\d{W}_t. \]
This is just suggestive notation for the integrals involved in \eqref{eqn: ito process}. Sticking with this notation, \eqref{eqn: ito rule} says
\begin{equation}
	\label{eqn: ito rule compact}
	\d{u}(t,X_t) = u'(t,X_t)\d{t} + \partial u(t,X_t)\d{X}_t + \frac{1}{2} \Tr\left[\partial^2 u(t,X_t) \d{X}_t(\d{X}_t)^*\right],
\end{equation}
where $\partial u(t,x)$ is the row vector with $u_i(t,x)$, $\partial^2 u(t,x)$ is the matrix with entries $u_{ij}(t,x)$, and $\d{X}^i_t\d{X}^j_t$ can be manipulated as
\[ (\d{W}^i_t)^2 = \d{t} \text{ and } (\d{t})^2 = \d{W}^i_t\d{t} = \d{W}^i_t\d{W}^j_t = 0 \text{ if } i\neq j. \]
(Check that the two equations are equivalent!)
\begin{itemize}
	\item If we set the $G^{ij}$ to $0$, the third term vanishes and we recover the usual (deterministic) chain rule from calculus.
	\item The third extra term is essentially a \textit{second} order approximation. To explain it in an extremely handwavy fashion, we have $(\d{t})^2 = 0$ in deterministic functions, which is why that term never crops up. Here however, the It\^{o} integral \textit{does} contribute. The reason for this could perhaps be attributed to the fact that $\expec[(W_b-W_a)^2] = b-a$, so $\d{W}_t$ can be thought of something like $\sqrt{\d{t}}$. As a result, the squared term involved in the second order term does contribute non-trivially.\\
	In the one-dimensional case, \eqref{eqn: ito rule compact} reads
	\[ \d{u}(t,X_t) = \frac{\partial u}{\partial t} (t,X_t)\d{t} + \frac{\partial u}{\partial x} (t,X_t)\d{X_t} + \frac{\partial^2 u}{\partial x^2}(t,X_t) (\d{X}_t)^2, \]
	which corresponds exactly to the handwavy argument given above.
\end{itemize}

Suppose that $X^1_t$ and $X^2_t$ are two one-dimensional It\^{o} processes and consider $u(t,x_1,x_2)=x_1x_2$. Then $u\in C^2$, so It\^{o}'s rule (in differential form) implies that
\[ \d{X_t^1 X_t^2} = X_t^1 \d{X}_t^2 + X_t^2 \d{X}_t^1 + \d{X}_t^1\d{X}_t^2. \]
Therefore, the class of It\^{o} processes is closed under multiplication and forms an algebra.

\subsection{Stochastic Differential Equations}

Stochastic differential expressions are usually encountered written in the form
\[ \d{X}_t = b(t,X_t)\d{t} + \sigma(t,X_t)\d{W}_t \text{ and } X_0 = x. \]
As before, this is just suggestive notation for the It\^{o} process
\begin{equation*}
	X_t = x + \int_0^t b(s,X_s) \d{s} + \int_0^t \sigma(s,X_s)\d{W}_s.
\end{equation*}

For example, if $W_t$ is a $m$-dimensional Wiener process, $A$ is a $n\times n$ matrix, and $B$ is a $m\times n$ matrix, then the $n$-dimensional equation
\[ \d{X}_t = AX_t\d{t} + B\d{W}_t \text{ and } X_0 = x \]
is called a \textit{linear stochastic differential equation}. It (always) has a unique solution
\[ X_t = e^{At}x + \int_0^t e^{A(t-s)}B\d{W}_s. \]

Call that a function $f:\R^n\to\R^m$ is \textit{Lipschitz uniformly on $s$} if $\norm{g(s,x)-g(s,y)} \leq K\norm{x-y}$ for some constant $K<\infty$ that does not depend on $s$.\\
For now, let us restrict ourselves to some bounded time $[0,T]$. Consider a filtered probability space $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in[0,T]},\Pbb)$ on which a $m$-dimensional Wiener process $W_t$ is defined. Choose $X_0$ to be a $\mathcal{F}_0$-measurable $n$-dimensional random variable, We seek a solution to
\begin{equation}
	\label{eqn: stochastic diff eqn}
	X_t = X_0 + \int_0^t b(s,X_s)\d{s} + \int_0^t \sigma(s,X_s)\d{W}_s.
\end{equation}
We now show existence of a solution to a large class of differential equations.

\begin{ftheo}
	Suppose that
	\begin{enumerate}
		\item $X_0 \in \mathcal{L}^2(\Pbb)$,
		\item $b,\sigma$ are Lipschitz continuous uniformly on $[0,T]$, and
		\item $\norm{b(t,0)}$ and $\norm{\sigma(t,0)}$ are bounded on $t\in[0,T]$.
	\end{enumerate}
	There exists a solution $X_t$ to the associated stochastic differential equation \eqref{eqn: stochastic diff eqn}. Moreover, for this solution, $X_t$, $b(t,X_t)$, and $\sigma(t,X_t)$ are in $\mathcal{L}^2(\mu_T\times\Pbb)$ and the solution is unique $\Pbb$-almost surely.
\end{ftheo}
\begin{proof}
	For $\mathcal{F}_t$-adapted $Y_\cdot\in\mathcal{L}^2(\mu_T\times\Pbb)$, consider the map
	\[ (\mathfrak{P}(Y_\cdot))_t = X_0 + \int_0^t b(s,Y_s)\d{s} + \int_0^t \sigma(s,Y_s)\d{W}_s. \]
	Our aim is to find a $\mathcal{F}_t$-adapted process $X_\cdot\in\mathcal{L}^2(\mu_T\times\Pbb)$ such that $\mathfrak{P}(X_\cdot) = X_\cdot$. We carry out the proof of existence in three parts.
	\begin{itemize}
		\item First, we claim that $\mathfrak{P}$ maps to a $\mathcal{F}_t$-adapted process in $\mathcal{L}^2(\mu_T\times\Pbb)$.\\
			Note that
			\begin{equation*}
				\tag{$*$}
				\label{eqn: picard iteration bounded growth}
				\norm{b}(t,x) \leq \norm{b(t,x)-b(t,0)} + \norm{b(t,0)} \leq K\norm{x} + K' \leq C(1+\norm{x}),
			\end{equation*}
			where $K$, $K'$, and $C$ are suitably chosen constants. Doing so similarly for $\sigma$, we may assume that $C$ is large enough such that both the above and $\norm{\sigma(t,x)} \leq C(1+\norm{x})$ hold. To show that $\mathfrak{P}$ maps to a process in $\mathcal{L}^2$, we shall show that each of the terms on the right side of the inequality
			\[ \norm{(\mathfrak{P}(Y_\cdot))_t}^2_{2,\mu_T\times\Pbb} \leq \norm{X_0}^2_{2,\mu_T\times\Pbb} + \norm{\int_0^t b(s,Y_s)\d{s}}^2_{2,\mu_T\times\Pbb} + \norm{\int_0^t \sigma(s,Y_s)\d{W}_s}^2_{2,\mu_T\times\Pbb} \]
			is finite.
			\begin{itemize}
				\item The first term is
				\[ \norm{X_0}^2_{2,\mu_T\times\Pbb} = T^{1/2} \norm{X_0}^2_{2,\Pbb} < \infty \]
				by the first assumption.
				\item The second term is
				\[ \norm{\int_0^t b(s,Y_s)\d{s}}^2_{2,\mu_T\times\Pbb} \leq T^2 \norm{b(s,Y_s)\d{s}}^2_{2,\mu_T\times\Pbb} \leq T^2C^2 \norm{1+\norm{Y_\cdot}}^2_{2,\mu_T\times\Pbb} < \infty, \]
				where the first inequality follows by using Jensen's inequality to get $(t^{-1}\int_0^t a_s\d{s})^2 \leq t^{-1}\int_0^t a_s^2\d{s}$, the second inequality uses \eqref{eqn: picard iteration bounded growth}, and the third inequality follows from the fact that $Y_\cdot$ is in $\mathcal{L}^2(\mu_T\times\Pbb)$.
				\item The third term is
				\[ \norm{\int_0^t \sigma(s,Y_s)\d{W}_s}^2_{2,\mu_T\times\Pbb} \leq T \norm{\sigma(\cdot,Y_\cdot)}^2_{2,\mu_T\times\Pbb} \leq TC^2\norm{1+\norm{Y_\cdot}}^2_{2,\mu_T\times\Pbb} < \infty, \]
				where the first inequality follows on using the It\^{o} isometry and the rest is as in the previous step.
			\end{itemize}
			This proves that the mapping is to an element of $\mathcal{L}^2(\mu_T\times\Pbb)$. It is clear that $\mathfrak{P}(Y_\cdot)$ is $\mathcal{F}_t$-adapted, proving the claim.
		\item Second, we claim that $\mathfrak{P}$ is continuous.\\
			That is, we want to show that if $\norm{Y^n_\cdot - Y_\cdot}_{2,\mu_T\times\Pbb}\to 0$, then $\norm{\mathfrak{P}(Y^n_\cdot) - \mathfrak{P}(Y_\cdot)}_{2,\mu_T\times\Pbb}\to 0$. As in the first part, we get
			\[ \norm{\mathfrak{P}(Y^n_\cdot) - \mathfrak{P}(Y_\cdot)}_{2,\mu_T\times\Pbb} \leq T \norm{b(\cdot,Y^n_\cdot) - b(\cdot,Y_\cdot)}_{2,\mu_T\times\Pbb} + \sqrt{T} \norm{\sigma(\cdot,Y^n_\cdot) - \sigma(\cdot,Y_\cdot)}_{2,\mu_T\times\Pbb} \leq K(T+\sqrt{T})\norm{Y^n - Y}_{2,\mu_T\times\Pbb}, \]
			where the second inequality follows for a suitably large $K$ from the uniformly Lipschitz condition. The required follows.
		\item Finally, we show the actual existence using a method known as \textit{Picard iteration}.\\
			Let $Y^0_t$ be an arbitrary $\mathcal{F}_t$-adapted process in $\mathcal{L}^2(\mu_T\times\Pbb)$ and for each $n\geq 0$, let $Y^{n+1}_\cdot = \mathfrak{P}(Y^n_\cdot)$. If we show that the $(Y^n_\cdot)$ converge, then we are done, since the (unique $\Pbb$-almost surely) limit would be a fixed point of $\mathfrak{P}$ (Why?).\\
			To do so, it suffices to show that the sequence is Cauchy. Exactly as in the proof of the above claim (note the measures involved in each of the norms!),
			\[ \norm{(\mathfrak{P}(Y_\cdot))_t - (\mathfrak{P}(Z_\cdot))_t}_{2,\Pbb} \leq \sqrt{t}\norm{b(\cdot,Y_\cdot)-b(\cdot,Z_\cdot)}_{2,\mu_t\times\Pbb} + \norm{\sigma(\cdot,Y_\cdot)-\sigma(\cdot,Z_\cdot)}_{2,\mu_t\times\Pbb} \leq L\norm{Y_\cdot - Z_\cdot}_{2,\Pbb}, \]
			where $L = K(\sqrt{T}+1)$. In general,
			\begin{align*}
				\norm{(\mathfrak{P}^n(X_\cdot))_t - (\mathfrak{P}^n(Z_\cdot))_t}^2_{2,\mu_T\times\Pbb} &= \int_0^T \norm{(\mathfrak{P}^n(X_\cdot))_t - (\mathfrak{P}(Z_\cdot))_t}^2_{2,\Pbb} \\
					&\leq L^{2n} \int_0^T \int_0^{t_1} \int_0^{t_2} \cdots \int_0^{t_{n-1}} \norm{Z_\cdot-Y_\cdot}^2_{2,\mu_{t_n}\times\Pbb}\d{t}_n\d{t}_{n-1}\cdots\d{t}_1 \\
					&\leq \frac{L^{2n}T^n}{n!}\norm{Z_\cdot-Y_\cdot}^2_{2,\mu_T\times\Pbb}.
			\end{align*}
			In particular,
			\[ \sum_{n=0}^\infty \norm{\mathfrak{P}^{n+1}(Y^0_\cdot) - \mathfrak{P}^n(Y^0_\cdot)}_{2,\mu_T\times\Pbb} \leq \norm{\mathfrak{P}(Y^0_\cdot) - Y^0_\cdot}_{2,\mu_T\times\Pbb} \sum_{n=0}^\infty \sqrt{\frac{L^{2n}T^n}{n!}} < \infty, \]
			completing this part of the proof.
	\end{itemize}
	Next, suppose that $X_\cdot$ is the above obtained solution and $Y_\cdot$ is another solution. We prove uniqueness in two steps.
	\begin{itemize}
		\item If $Y_\cdot\in\mathcal{L}^2(\mu_T\times\Pbb)$, then for any $n$,
		\[ \norm{X_\cdot - Y_\cdot}^2_{2,\mu_T\times\Pbb} = \norm{\mathfrak{P}^n(X_\cdot) - \mathfrak{P}^n(Y_\cdot)}^2_{2,\mu_T\times\Pbb} \leq \sqrt{\frac{L^{2n}T^n}{n!}} \norm{X_\cdot - Y_\cdot}^2_{\mu_T\times\Pbb}. \]
		Letting $n\to\infty$, we see that the expression on the left must be $0$, so $X_\cdot$ and $Y_\cdot$ are equal $\mu_T\times\Pbb$-almost surely.
		\item To complete the proof, we show that any solution $Y_t$ with $Y_0 = X_0$ must be in $\mathcal{L}^2(\mu_T\times\Pbb)$.
	\end{itemize}
\end{proof}