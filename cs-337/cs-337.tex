\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{cs-337}


\begin{document}

\thispagestyle{empty}

\titleBC
\tableofcontents
\clearpage

\setcounter{section}{-1}

\input{notation}
% \input{introduction}

\section{Introduction}

	\subsection{What and Why?}

		Machine learning involves learning from past experiences to perform a job better.\\
		Broadly, the goal of the field is to make a computer emulate human pattern recognition, which is second nature to us.
		What do each of these words mean in the context of computer science? ``Learning'' and ``better'' usually depend heavily on the context surrounding our goals, and past experiences usually refers to a data set of some form. Broadly, there are two types of learning: \emph{supervised} and \emph{unsupervised}.\\

		In the former, we have access to some \emph{training data}, usually consisting of a set of pairs of input and output. The supervised learning algorithm then analyzes this data to produce an inferred function, which can then be used for determining what unknown inputs (not in the training data) must map to. This requires the algorithm to go from a smaller set of training data to a much broader set of inputs in a way that ``makes sense''. For example, given the heights and weights of a thousand people, we might be asked to determine the weight of a person of a person of some new height.\\
		In unsupervised learning on the other hand, we do not have access to any prior information, so we must classify them in some sensible manner. For example, given a set of fruits, we may wish to classify them into various groups. If we divide on the basis of colour (alone), we might get two groups -- apples and cherries in one and oranges and peaches in the other. If we divide on the basis of both colour and size, we might further divide the first of these groups into two. The computer is forced to create some compact internal representation of the features of these fruits, and from this we might even be able to generate \emph{new} fruits.\\

		How does all this work? It might be simpler to grasp if we dive headlong into an example.

		Suppose we are given a data set $\mathcal{D} = \{\langle x_1, y_1 \rangle,\ldots,\langle x_k, y_k \rangle\}$. We wish to determine a function $f^*$ such that $f^*(x)$ is the best ``predictor'' of $y$ with respect to $\mathcal{D}$.\\
		To quantify how good a prediction is, we introduce an \emph{error function} $\varepsilon(f,\mathcal{D})$ that reflects the discrepancy of the function with respect to $\mathcal{D}$. We also assume that our $f^*$ is taken from some base class of functions, say $\mathcal{F}$. We then set
		\[ f^* = \argmin_{f \in \mathcal{F}} \varepsilon(f,\mathcal{D}). \]

		Typically, we present our function in terms of some ``basis functions'' $(\phi_i)_{i=1}^n$ each from the set of inputs to $\R$. More compactly, we have a single function $\phi$ from the set of inputs to $\Rn$ that shows all the relevant things we can glean about any input. Suppose our set of outputs is in $\R^k$. We then learn a function $f : \R^n\to\R^k$ and given a new input $x'$, we predict the corresponding output as $y' = f(\phi(x'))$.\\
		Due to this, it is often helpful to think of the input as an element of $\Rn$, where $n$ is the number of basis functions.\\

		\begin{remark}
			It is important to note that even though our original input $x$ may be in $\R$, we could have more than just $1$ basis function. For instance, we could choose the basis functions as $\{ (x\mapsto x^i) : 1 \le i \le 100\}$. This is especially important in the coming topic of linear regression, where linearity in the basis functions should not be mixed up with linearity in the original input.
		\end{remark}
		

	\subsection{Linear Regression}

		We first discuss perhaps the simplest example of machine learning. In this, the class $\mathcal{F}$ of functions we consider is merely the class of all linear functions over the basis functions. That is,
		\[ \mathcal{F} = \{ (x \mapsto w^\top \phi(x) ) : w \in \R^n \}. \]

		Observe that if one of our basis functions is $1$ (or some constant), this is equivalent to the set of all \emph{affine} functions (linear functions plus a constant).


		\subsubsection{The method of least squares}

			\label{subsubsec: the method of least squares}

			Suppose our data set is $\mathcal{D} = \{\langle x_i,y_i\rangle : 1 \le i \le m\}$, where each $y_i$ is in $\R$. In the method of least squares, our choice of error function is given by
			\[ \varepsilon(f,\mathcal{D}) = \sum_{i=1}^m (f(x_i) - y_i)^2. \]
			This is an extremely common error function for various reasons -- it is convex, non-negative, and well-behaved.

			In the case of linear regression, where any function $f$ is uniquely determined by a $w\in \Rn$, we can express the error in terms of this vector as
			\[ \varepsilon(w) = \sum_{i=1}^m (w^\top \phi(x_i) - y_i)^2. \]

			Now, set
			\[
				\Phi =
				\begin{pmatrix}
					\phi_1(x_1) & \cdots & \phi_n(x_1) \\
					\vdots & \ddots & \vdots \\
					\phi_1(x_m) & \cdots & \phi_n(x_m)
				\end{pmatrix}
				\text{ and }
				y =
				\begin{pmatrix}
					y_1 \\
					\vdots \\
					y_m
				\end{pmatrix}.
			\]
			So,
			\[ \varepsilon(w) = \norm{\Phi w - y}_2^2. \]

			The minimum value of the above is just the distance from $y$ to $\mathcal{C}(\Phi)$, the column space of $\Phi$! Let $\hat{w} = \argmin_{w} \varepsilon(w)$.\\
			In particular, if $y\in\mathcal{C}(\Phi)$, it is possible to get the cost to $0$.\\
			The least square solution is then the distance from $y$ to the projection $\hat{y}$ of $y$ on $\mathcal{C}(\Phi)$. How do we find $\hat{y}$ and $\hat{w}$?\\
			The line joining $y$ and $\hat{y}$ is orthogonal to $\mathcal{C}(\Phi)$. As a result, $(\hat{y} - y)^\top \Phi = 0$. Therefore,
			\begin{align*}
				\hat{y}^\top \Phi &= y^\top \Phi \\
				(\Phi \hat{w})^\top \Phi &= y^\top \Phi \\
				\hat{w} &= (\Phi^\top \Phi)^{-1} \Phi^\top y.
			\end{align*}
			This is well-defined only if $(\Phi^\top \Phi)$ is invertible ($\Phi$ has full column rank).

		\begin{remark}[Overfitting]
			One might think that more basis functions means a better approximation. Indeed, this would mean that we have more ``freedom'', which should allow us to get a better function. However, this is not the case. While adding more basis functions allows us to emulate the \emph{training} data better, we might mimic it too closely and lose sight of the overall behaviour, which results in bad performance when it comes to the general \emph{testing} data.\\
			This is very clearly seen in the following example where the output is slightly noisy linear data, and we have taken two cases: one wherein the basis function are just $\phi(x) = (1,x)$ (fitting a degree $1$ polynomial), and the second where $\phi(x) = (1,x,\ldots,x^{10})$ (fitting a degree $10$ polynomial).

			\begin{figure}[H]
				\centering
				\caption{Overfitted data}
				\copyrightbox[r]{\includegraphics[width = 0.4\linewidth]{overfitting.png}}{\textcopyright \href{https://commons.wikimedia.org/wiki/File:Overfitted_Data.png}{Overfitted Data} by Ghiles, licensed under \href{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0}, via Wikimedia Commons}
			\end{figure}
		\end{remark}

	\subsection{Probabilistic Interpretations}

		Let us jump away from regression for a moment and look at what happens if instead of trying to learn a function with inputs and outputs, we are instead trying to determine the parameters of some random variable given some datapoints drawn from it.

		To do this, suppose that any observation $z$ is drawn from the random variable $Z$ with probability density/mass function $g_\theta$ (depending on the parameter $\theta$, which we are trying to learn). For example, if $Z$ is a gaussian with mean $\mu$ and variance $\sigma^2$, we might have $\theta = (\mu,\sigma^2)$ and
		\[ g_\theta(z) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left( -\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2} \right). \]
	
		% Now, let us work w

		% Using an error function is not the only way to arrive at a good estimate of $w$. In fact, we need not even arrive at a single estimate of $w$, it might be better to finally arrive at some \emph{distribution} that represents how $w$ might behave depending on the information we have.\\

		\subsubsection{Maximum likelihood estimation}

			Suppose we are given a dataset $\mathcal{D} = \{z_1,\ldots,z_m\}$.\\
			Also suppose that we have discrete outputs for the moment ($g_w$ is a probability \emph{mass} function). Then, define the \emph{likelihood} function
			\[ \mathcal{L}(\mathcal{D} \mid \theta) = \Pr[ \mathcal{D} \mid \theta ]. \]
			Making the mild assumption that the various datapoints in $\mathcal{D}$ are drawn iid, this can be rewritten as
			\[ \mathcal{L}(\mathcal{D} \mid \theta) = \prod_{i=1}^m g_\theta(z_i). \]
			Even in the case where we have a continuous output (so a probability density function) instead, this expression remains the same.\\
			As might be expected, in \emph{maximum likelihood estimation} (MLE), we choose that $w$ which maximizes the likelihood. Often, we work with the \emph{log}-likelihood $\ell$ as well, which is just defined by $\ell(\mathcal{D} \mid w) = \log \mathcal{L}(\mathcal{D} \mid w)$.

		\subsubsection{Bayesian estimation}

			In all that we have done so far, we have arrived at a single estimate for $w$ or $\theta$. However, this may not necessarily be the best option -- might it not be better to arrive at a probability distribution for $w$ which represents how likely various values are?\\
			Bayesian estimation is one simple way of doing this.\\

			We make the assumption initially that $\theta$ is drawn from some \emph{prior} probability distribution $p$. Given the dataset $\mathcal{D} = \{z_1,\ldots,z_m\}$ (and the function $g_\theta$ from earlier), we try to improve on this prior to get to a \emph{posterior} probability distribution that better reflects how $\theta$ behaves, now that we have knowledge about the dataset. To do this, we use Bayes' law:

			\begin{align}
				\Pr[\theta\mid\mathcal{D}] &= \frac{\Pr[\mathcal{D}\mid\theta] \Pr[\theta]}{\Pr[\mathcal{D}]} \nonumber \\
					&= \frac{\Pr[\mathcal{D}\mid\theta] p(\theta)}{\int \Pr[\mathcal{D}\mid\alpha] p(\alpha) \dif \alpha}. \label{eqn: 1.1}
			\end{align}

			Note the contrast between this and MLE; in the the former we use $\Pr[\theta\mid\mathcal{D}]$, whereas in the latter we use $\Pr[\mathcal{D}\mid\theta]$.\\
			We can then use the posterior to return some estimate of $\theta$.\\
			In \emph{maximum a posteriori estimation} (MAP estimation), we return the mode of the posterior density.\\
			In \emph{Bayesian estimation}, we return the mean of the posterior density.\\
			In the pure Bayesian estimate, we do not return any point estimate for $\theta$ at all, and we instead use Bayes' rule to calculate the output given an input as
			\[ \Pr[x \mid \mathcal{D}] = \int p(x\mid\theta) p(\theta\mid\mathcal{D}) \dif \theta \]

			Now, the integral in the denominator of \eqref{eqn: 1.1} can be quite daunting. To deal with this, we introduce the idea of a \emph{conjugate prior}. Such a prior is of the form that the base distribution of both the prior and the posterior are the same. This then enables us to skip the calculation of the denominator, since it merely leads to a constant to normalize the distribution (its integral must be $1$), and we know this constant from the structure of the distribution anyway. This can be better understood by an example. Suppose that the datapoints are drawn from a Bernoulli distribution with parameter $\theta$. We then have
			\[ \Pr[\mathcal{D} \mid \theta] = \theta^r (1-\theta)^{m-r}, \]
			where $r$ is the number of $1$s in $\mathcal{D}$. If we choose our prior to be a \href{https://en.wikipedia.org/wiki/Beta_distribution}{Beta distribution} with parameters $\alpha$ and $\beta$, then the numerator of \eqref{eqn: 1.1} is
			\[ \theta^r (1-\theta)^{m-r} \cdot \theta^{\alpha-1} (1-\theta)^{\beta-1} \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}. \]
			This is proportional to $\theta^{\alpha+r-1}(1-\theta)^{m-r+\beta-1}$. Since the denominator only gives a constant multiplicative factor, the posterior must just be $\Beta(\alpha+r,\beta+m-r)$.\\

			Some more examples are that the \href{https://en.wikipedia.org/wiki/Dirichlet_distribution}{Dirichlet distribution} is the conjugate prior to the multivariate Bernoulli and the Gaussian is the conjugate prior to the Gaussian.

			% $y = w^\top \phi(x) + \varepsilon
			% \varepsilon \sim \mathcal{N}(0,\sigma^2)
			% w \sim \mathcal{N}(\mu_0,\Sigma_0)
			% \Sigma_m^{-1} = \Sigma_0^{-1} + \frac{1}{\sigma^2} \Phi^\top \Phi
			% \Sigma_m^{-1} \mu_m = \Sigma_0^{-1} \mu_0 + \Phi^\top y/\sigma^2$


			Bayesian estimation takes care of overfitting

		\subsubsection{Relating probabilistic interpretations and regression}

			Let us jump back to regression. While we typically assume that our input/output pairs are initially taken from some basic (fixed) function $f$, this is typically not the case. Indeed, we usually have some amount of error when taking the observations itself. This error is commonly taken to be a Gaussian with $0$ mean and some (small) variance $\sigma^2$.\\
			The output as a whole is then just a random variable, with the parameters involving our base parameter $\theta$ \emph{and} the input.\\
			
			For example, let our training data set be $\mathcal{D} = \{\langle x_1,y_1\rangle,\ldots,\langle x_m,y_m\rangle\}$ where the $x_i$ are in $\Rn$ and the $y_i$ in $\R$. Let us perform normal linear regression, but also add a Gaussian error of known variance $\sigma^2$. That is, if the input is $x$, we predict the distribution of the corresponding output $y$ to be $\mathcal{N}(w^\top x, \sigma^2)$.\\
			Supposing we use MLE, we wish to determine
			\[ \hat{w} = \argmax_{w} \frac{1}{(\sqrt{2\pi}\sigma)^m} \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^m (y_i - w^\top x_i)^2 \right). \]
			It is not too difficult to see that this is in fact the same solution as that given by the discussion in \hyperref[subsubsec: the method of least squares]{the method of least squares}.

	\subsection{Regularisation}

		As mentioned in a remark towards the end of \Cref{subsubsec: the method of least squares}, overfitting tends to make the error over the test data too large, due to large fluctuations in the model we learn (to emulate the training data better). However, this also leads to large coefficients of the learnt vector, and thus large norm. Regularisation attempts to fix this, by further adding in the constraint that $\norm{w}_2$ is not too large.

		\subsubsection{Regularised regression}

			This leads to the \emph{penalized regularised least squares regression problem}, where the cost function is replaced with
			\[ \norm{\Phi w - y}_2^2 + \lambda\Omega(w). \]
			We are said to be performing
			\begin{itemize}
				\item \emph{ridge regression} if $\Omega(w) = \norm{w}_2^2$,
				\item \emph{lasso\footnote{this is an abbreviation for \emph{l}east \emph{a}bsolute \emph{s}hrinkage and \emph{s}election \emph{o}perator} regression} if $\Omega(w) = \norm{w}_1$, and
				\item \emph{support-based penalty regression} if $\Omega(w) = \norm{w}_0$.
			\end{itemize}
			The closed-form solutions to the first two can be computed by setting the gradient to $0$. In particular, the closed-form solution for ridge regression is
			\[ \hat{w} = (\Phi^\top\Phi + \lambda I)^{-1} \Phi^\top y. \]
			Also, this turns out to be the same as performing Bayes/MAP estimation with a multivariate Gaussian prior of mean $0$ and variance $(1/\lambda)I$.\\

			Alternatively, we have the \emph{constrained regularised least squares regression problem}, where the cost function is the normal $\norm{\Phi w - y}_2^2$, but we have the added constraint that $\Omega(w) \le \theta$ for some fixed $\theta$.\\
			We again have ridge, lasso, or support-based penalty regression depending on $\Omega$.\\
			It in fact turns out that the above two constraints are equivalent -- for any penalized formulation with a $\lambda$, there is a corresponding constrained formulation problem with some $\theta$ (that depends on both $\lambda$ and the data), and vice-versa.\\

			Why is lasso regression interesting? Observe that the level curves of $\norm{w}_1$ (namely cross-polytopes) have many corners on the axes, and as a result, it is likely that many components of $w$ are close to $0$ (as opposed to ridge regression where the level curves are rotationally invariant). That is, the learnt vector is likely very sparse, which is desirable.\\
			The closed form solution is the MAP estimate of linear regression subject to the prior being the \href{https://en.wikipedia.org/wiki/Laplace_distribution}{Laplace distribution} with parameters $0$ and $\theta$. Note that the Laplace distribution is extremely similar to the Gaussian, except that we use the $L^1$ norm in the exponential instead of the $L^2$. The intuition that lasso regression leads to many parameters being close to $0$ is strengthened by the sparsity of the Laplace distribution (the density at $0$ is high).\\
			However, there is no known closed form solution for the solution to lasso regression. Instead, it is solved iteratively with normal gradient descent. We have
			\begin{align*}
				\hat{w} &= \argmin_w \norm{\Phi w - y}_2^2 + \lambda\norm{w}_1 \\
					&= \argmin_w E_{\text{LS}}(w) + \lambda\norm{w}_1.
			\end{align*}
			At the $k$th step, set
			\begin{align*}
				w_{\text{LS}}^{k+1} &= w_{\text{Lasso}}^{k} - \eta\nabla E_\text{LS}(w_\text{Lasso}^{k}) \\
				(w_\text{Lasso}^{k+1})_{i} &=
				\begin{cases}
					(w_\text{LS}^{k+1})_i - \lambda\eta, & (w_\text{LS}^{k+1})_i > \lambda\eta, \\
					(w_\text{LS}^{k+1})_i + \lambda\eta, & (w_\text{LS}^{k+1})_i < -\lambda\eta, \\
					0, & \text{otherwise.}
				\end{cases}
			\end{align*}
			We then estimate $\hat{w}$ as $w_\text{Lasso}^r$ for a suitably large $r$.

		\subsubsection{Understanding regularisation}

			Ridge regression might seem slightly silly from the perspective of just training error. Indeed, the the (minimized) error for the ridge cost function is always greater than the (minimized) error for normal least squares.\\

			Suppose we are performing regression with a true function $g$ and let $f$ be our predicted/fitted function. We sample a bunch of points from $g$ to get a dataset, with possibly some noise that causes small perturbations.\\
			There are three primary sources of test error:
			\begin{itemize}
				\item \emph{Bias}: This occurs due to the difference between the nature of the true function and fitted function. It is just equal to $\expec[f(x)] - g(x)$.
				\item \emph{Variance}: This occurs due to the choice of our dataset. It is equal to $\Var(f(\hat{x}))$.
				\item \emph{Noise}: This occurs due to the implicit noise that is present when we take measurements from the true function. For example, there may be an additive Gaussian noise with mean $0$ and variance $\sigma^2$. The noise is then just equal to the variance $\sigma^2$ of this component.
			\end{itemize}

			Now, suppose the noise is some random variable $\varepsilon$ with mean $0$ and variance $\sigma^2$. Then, given an input $x$, we observe the output $y$ as being drawn from $g(x)+\varepsilon$.\\
			Suppose we are given a dataset $\mathcal{D} = \{\langle x_i,y_i\rangle : 1 \le i\le m\}$, and using it we predict the function $f$.\\
			The question we would like to answer is: what is the expected test error $\expec[(f(\hat{x}) - (g(\hat{x}) + \varepsilon))^2]$ for a new input $\hat{x}$? The expectation is taken over both the sampled dataset and the distribution of $\varepsilon$.\\
			Let $\hat{y} = g(\hat{x}) + \varepsilon$. We then have
			\begin{align*}
				\expec[(f(\hat{x}) - \hat{y})^2] &= \expec[f(\hat{x})^2] + \expec[\hat{y}^2] - 2 \expec[f(\hat{x})]\expec[\hat{y}] & \text{(assuming the dataset and $\varepsilon$ are independent)} \\
					&= \expec[f(\hat{x})]^2 + \Var(f(\hat{x})) + \expec[\hat{y}^2] - 2 \expec[\hat{f}(x)] g(\hat{x}) \\
					&= \expec[f(\hat{x})]^2 + \Var(f(\hat{x})) + \expec[g(\hat{x})^2 + \varepsilon^2] - 2 \expec[\hat{f}(x)] g(\hat{x}) \\
					&= \expec[f(\hat{x})]^2  + g(\hat{x})^2 - 2 \expec[\hat{f}(x)] g(\hat{x}) + \Var(f(\hat{x})) + \sigma^2 \\
					&= \underbrace{(\expec[f(\hat{x})] - g(\hat{x}))^2}_{\textstyle\text{Bias$^2$}} + \underbrace{\Var(f(\hat{x}))}_{\textstyle\text{Variance}} + \underbrace{\sigma^2 \mathstrut}_{\textstyle\text{Noise}}.
			\end{align*}

			Given the bias-variance tradeoff, how do we choose the best predictor? How do we set the model's parameters?\\
			In \emph{bootstrap sampling}, we repeatedly sample observations from a dataset with replacement. Given dataset $\mathcal{D}_b$, we repeatedly choose a $V_b$, train on $\mathcal{D}_b \setminus V_b$, then test on $V_b$.\\
			So, the training set is that we use to actually train the algorithm. $V_b$ is the \emph{validation} or \emph{development} set, and is used for model selection and tuning hyperparameters. The test/evaluation set is that we use for testing finally.

\section{Classification}

	Suppose we wish to intelligently classify emails into two categories depending on whether they are spam or not. How do we learn a method to classify an email as spam?

	It is not similar to regression where the function maps to $\R$, but we instead map to a discrete set $\{c_1,c_2,\ldots,c_k\}$.\\
	The simple hack of mapping $c_i$ to $i$, say, and then performing usual regression. However, the ordering of the $c_i$ influences it significantly then, which is very undesirable, since the ordering doesn't mean anything.\\
	A better idea would be to take $c_i$ as
	\[ y_i = e_i \in \R^k, \]
	to encode the idea that the various components are `independent'.\\
	We present these ideas over the next few sections.

	\subsection{Perceptron Classification}

		\subsubsection{The basic idea}

			Rather than mapping directly to $\mathcal{C} = \{c_1,\ldots,c_k\}$, we could map to some intermediate space which in turn maps to $\mathcal{C}$ (or a sequence of spaces and functions, the last of which is $\mathcal{C}$).\\

			First, consider the binary classification problem, where $\mathcal{C} = \{-1,1\}$. Let $\mathcal{D} = \{ \langle x_i , y_i \rangle : 1 \le i \le m \}$, where each $y_i \in \{-1,1\}$. The ordering does not matter then (both orderings are equivalent), so we can learn a linear classifier. Suppose $f(x) = w^\top \phi(x)$.\\
			Given a new input $x$, we predict the new output as $1$ if $f(x) \ge 0$ and $-1$ if $f(x) < 0$. So, we want
			\[ y\cdot w^\top \phi(x) \ge 0. \]

			We learn $w$ as follows. Starting with $w^{(0)} = 0$, we iterate over the dataset. If a point is correctly classified, we leave $w^{(r+1)} = w^{(r)}$. Otherwise, we marginally correct the weights by performing
			\[ w^{(r+1)} = w^{(r)} + \eta y_{r+1}\phi(x_{r+1}), \]
			for some (typically small) learning rate $\eta$.\\
			Observe that the quantity $w^\top \phi(x)$ is the distance of $x_r$ from the hyperplane $\{x : w^\top\phi(x) = 0\}$.\\
			So,
			\[ y_{r+1}(w^{(r+1)})^\top \phi(x_r) = y_{r+1} (w^{(r)})^\top\phi(x_{r+1}) + \eta\norm{\phi(x_{r+1})}^2. \]
			This makes the quantity $y_{r+1} w^\top \phi(x_r)$ less negative. We repeat the above process until $y_iw^\top\phi(x_i) \ge 0$ for all $i$ (or until we terminate). \\
			This \emph{perceptron} algorithm does not attempt to find the best separating hyperplane, it merely tries to find any separating hyperplane -- it need not provide any `breathing room' for the inputs. This is addressed by support vector machines, which we shall read later.\\
			One issue with perceptron classification is that it does not acknowledge how much the estimate differs from the expected $\pm 1$ -- we only care that $yw^\top\phi(x) \ge 0$, not how positive it is.

		\subsubsection{Convergence}

			Now, suppose there exists some $u$ such that for all $i$, $y_i u^\top \phi(x_i) \ge 0$. It turns out that the perceptron update rule will then converge in a finite number of iterations. That is, perceptrons can model linearly separable functions.
			\begin{proof}
				Since we can normalize $u$ suppose that $\norm{u} = 1$. We can also normalize the dataset itself, so assume $\norm{\phi(x_i)} \le 1$ for all $i$. \\
				Define the \emph{margin} $\gamma = \min_i |u^\top x_i|$.\\
				Further make the assumption that $\eta = 1$ (the general case follows similarly.) \\
				We claim that in at most $t = 1/\gamma^2$ updates, we will have $y_i {w^{(t)}}^\top x_i > 0$ for all $i$.\\
				If we update $w^{(k)}$ to $w^{(k+1)} = w^{(k)} + y\phi(x)$ on reading the input $\langle x,y\rangle$,
				\[ {w^{(k+1)}}^\top u =  (w^{(k)}+y\phi(x))^\top u \ge {w^{(k)}}^\top u + \gamma. \]
				and
				\[ \norm{w^{(k+1)}}^2 = \norm{w^{(k)} + y\phi(x)}^2 < \norm{w^{(k)}}^2 + 1. \]
				As a result, after $k$ updates,
				\[ {w^{(k)}}^\top u \ge k\gamma \text{ and } \norm{w^{(k)}}^2 \le k \]
				so
				\[ \sqrt{k} \ge \underbrace{\norm{w^{(k)}} \ge u^\top w^{(k)}}_{\text{Using Cauchy-Schwarz}} \ge k\gamma. \]
				This gives $k \le 1/\gamma^2$, completing the proof.
			\end{proof}

		\subsubsection{Viewing the algorithm with a loss function}

			Consider the loss function
			\[ L_P(w^\top(\phi(x)),y) = \max\{ 0 , -yw^\top\phi(x) \}. \]
			We can then apply the \emph{stochastic gradient descent} algorithm. Let $f_w(x) = w^\top\phi(x)$ and choose some example $\langle x_t,y_t\rangle$. We then compute the gradient of the loss of $f_w(x_t)$ with respect to $w$, then update the weight vector using it as
			\[ w \gets w - \nabla_w L_P(f_w(x_t),y). \]
			Perceptron update is then just stochastic gradient descent on this ``hinge loss'' function.\\
			Similarly, we could use different loss functions.\\

			Now, what $w$ should we use at the time of testing? Using the final $w$ is not always a good idea due to overfitting and the stochastic nature of the algorithm.\\
			So, we usually use an intermediate $w$. The \emph{voted perceptron} algorithm counts votes for weight vectors tallying how many examples they have correctly classified. The \emph{averaged perceptron} algorithm uses the averaged weight vector (not necessarily the average of all weight vectors, but a suitable subset).\\

			Typically, rather than doing stochastic gradient descent datapoint by datapoint, we evaluate it in batches datapoint. That is, we do
			\[ w \gets w - \frac{1}{|\mathcal{B}|} \sum_{(x,y) \in \mathcal{B}} \nabla_w L_P(f_w(x),y) \]
			for a suitable batch $B$, which is intermediate between normal gradient descent and stochastic gradient descent. Observe that if $B$ is the entire dataset, we get ordinary gradient descent.

	\subsection{Logistic Regression}

		Recall that the function we use in perceptron classification is just a step function. Is it possible to do better, and possibly take care of smoothness issues? That is, instead of $\operatorname{sign}(w^\top\phi(x))$, can we use some more general $\Omega(w^\top \phi(x))$? In logistic regression, we use the \textit{sigmoid function} defined by
		\[ \sigma(t) = \frac{1}{1+e^{-t}}. \]
		Rather than looking at whether the value of the function is positive or negative, we instead see here whether it is greater than or less than $1/2$. Now, let us return to the binary classification problem, taking $\mathcal{C} = \{0,1\}$ instead of $\{-1,1\}$. Since the sigmoid maps into $(0,1)$, this also allows us a probabilistic interpretation as
		\[ \Pr[y=1 \mid x] = \frac{1}{1+e^{-w^\top \phi(x)}} \text{ and } \Pr[y=0\mid x] = \frac{e^{-w^\top \phi(x)}}{1+e^{-w^\top \phi(x)}}. \]
		That is, we can view it as a Bernoulli model with parameter $\sigma(w^\top\phi(x))$.\\
		We can then do maximum likelihood estimation for the parameter of the Bernoulli (or minimize the error), and thus arrive at an estimate for $w$! We can also do something along the lines of Bayesian or MAP estimation.\\
		To be more explicit, the maximum likelihood estimate is is
		\begin{align*}
			\hat{w} &= \argmax_w \mathcal{L}(\mathcal{D};w) \\
				&= \argmax_w \Pr\left[ y_1,\ldots,y_m \mid w,x_1,\ldots,x_m \right] \\
				&= \argmax_w \prod_{i=1}^m \Pr\left[ y_i \mid w, x_i \right] \\
				&= \argmax_w \prod_{i=1}^m \sigma(w^\top\phi(x_i))^{y_i} (1 - \sigma(w^\top\phi(x_i)))^{1-y_i} \\
				&= \argmax_w \sum_{i=1}^m y_i \log(w^\top \phi(x_i)) + (1 - y_i) \log(1-\sigma(w^\top\phi(x_i))) \\
				&= \argmin_w \underbrace{- \sum_{i=1}^m y_i \log(w^\top \phi(x_i)) + (1 - y_i) \log(1-\sigma(w^\top\phi(x_i)))}_{-\log(\mathcal{L}(\mathcal{D};w))}.
		\end{align*}
		The average negative log-likelihood ($- 1/m \log(\mathcal{L}(\mathcal{D};w))$) is often referred to as the \emph{cross-entropy} loss function $E(w)$, and represents a ``distance'' between the data distribution (the $y_i$) and the model distribution (the $\sigma$). Cross-entropy is the average number of bits required to identify an event drawn from the dataset, if a coding scheme is used that is optimized for a modelled probability distribution $\Pr[y\mid\mathcal{M}]$ (as opposed to the `true' distribution $\Pr[y\mid\mathcal{D}]$).\\
		Alternatively, we have
		\[ E(w) = - \frac{1}{m} \sum_{i=1}^m y_i w^\top \phi(x_i) - \log(1 + e^{w^\top\phi(x_i)}). \]
		The first component here resembles the loss in perceptron classification.\\
		We do not have a simple closed-form solution for the cross-entropy loss function, so we apply gradient descent. The update rule looks like
		\[ w^{(k+1)} = w^{(k)} + \eta \left(\frac{1}{m} \sum_{i=1}^m (y_i - \sigma({w^{(k)}}^\top\phi(x_i))) \phi(x_i) \right). \]
		The stochastic version of this is
		\[ w^{(k+1)} = w^{(k)} + \eta (y_i - \sigma({w^{(k)}}^\top\phi(x_i))) \phi(x_i). \]
		We shall return to something similar when we study generalized linear models later in the course, of which perceptron, linear regression, and logistic regression are special cases.\\
		The \emph{regularized} (logistic) cross-entropy loss function is
		\[ E_r(w) = -\frac{1}{m} \sum_{i=1}^m y_i \log(w^\top \phi(x_i)) + (1 - y_i) \log(1-\sigma(w^\top\phi(x_i))) + \frac{\lambda}{2m}\norm{w}_2^2, \]
		which attempts to avoid overfitting. The probabilistic interpretation of this is that this is the MAP estimate given a Gaussian prior on $w$ with mean $0$ and variance $1/\lambda$. In this case, the gradient descent update rule is
		\[ w^{(k+1)} = w^{(k)} + \eta \left(\frac{1}{m} \sum_{i=1}^m (y_i - \sigma({w^{(k)}}^\top\phi(x_i))) \phi(x_i) - \lambda w^{(k)} \right). \]

		Logistic regression is easily extended to even the multi-class scenario (say $k$ classes $1,\ldots,k$), by keeping a different weight vector $w_{c}$ for each $1\le c\le k-1$, then letting
		\[ \Pr[y=c\mid x] = \frac{e^{-w_c^\top \phi(x)}}{1 + \sum_{i=1}^{k-1} e^{-w_i^\top \phi(x)}} \]
		for $1\le c\le k-1$ and
		\[ \Pr[y=k\mid x] = \frac{1}{1 + \sum_{i=1}^{k-1} e^{-w_i^\top \phi(x)}}. \]
		We can have parameters for all $k$ classes, but this is not necessary since the $k$th class occurs iff none of the other $k-1$ classes occurs.

% \bibliographystyle{alpha}
% \bibliography{references}


\end{document}